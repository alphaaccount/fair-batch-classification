{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "8LPCEO_adult_multiple.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tIEkDw7fexa"
      },
      "source": [
        "# Import libraries necessary for this project\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "import seaborn as sns\n",
        "sns.set(style=\"darkgrid\")\n",
        "from time import time\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Import 'GridSearchCV', 'make_scorer', and any other necessary libraries\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import fbeta_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "# Import the three supervised learning models from sklearn\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA    \n",
        "\n",
        "# Pretty display for notebooks\n",
        "%matplotlib inline\n",
        "from random import shuffle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "b4A2tdgUfeye",
        "outputId": "695efa11-8a77-42c2-c7ef-6f67ee22bd2c"
      },
      "source": [
        "# without accuracy\n",
        "import time\n",
        "# import pulp as p \n",
        "# from random import *\n",
        "\n",
        "# Add column names to data set\n",
        "columns = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', \n",
        "           'relationship', 'race','sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income']\n",
        "\n",
        "# Read in train data\n",
        "adult_train = pd.read_csv('data/adult_actual/adult_train_data.csv', header=None, names=columns, skipinitialspace=True)\n",
        "\n",
        "# Drop the fnlwgt column which is useless for later analysis\n",
        "adult_train = adult_train.drop('fnlwgt', axis=1)\n",
        "\n",
        "# Read in test data\n",
        "adult_test = pd.read_csv('data/adult_actual/adult_test_data.csv', header=None, skiprows=1, names=columns, skipinitialspace=True)\n",
        "\n",
        "# Drop the fnlwgt column which is useless for later analysis\n",
        "adult_test = adult_test.drop('fnlwgt', axis=1)\n",
        "\n",
        "# Remove '.' in income column\n",
        "adult_test['income'] = adult_test['income'].apply(lambda x: '>50k' if x=='>50k.'  else '<=50k')\n",
        "\n",
        "\n",
        "# Convert '?' to NaNs and remove the entries with NaN value\n",
        "# Check missing value code and convert to NaNs\n",
        "object_col = adult_train.select_dtypes(include=object).columns.tolist()\n",
        "for col in object_col:\n",
        "    adult_train.loc[adult_train[col]=='?', col] = np.nan\n",
        "    adult_test.loc[adult_test[col]=='?', col] = np.nan\n",
        "\n",
        "# Perform an mssing assessment in each column of the dataset.\n",
        "col_missing_pct = adult_train.isna().sum()/adult_train.shape[0]\n",
        "col_missing_pct.sort_values(ascending=False)\n",
        "\n",
        "# Remove data entries with missing value\n",
        "adult_train = adult_train.dropna(axis=0, how='any')\n",
        "adult_test = adult_test.dropna(axis=0, how='any')\n",
        "\n",
        "# Show the results of the split\n",
        "# print(\"After removing the missing value:\")\n",
        "# print(\"Training set has {} samples.\".format(adult_train.shape[0]))\n",
        "# print(\"Testing set has {} samples.\".format(adult_test.shape[0]))\n",
        "for col in object_col:\n",
        "    print(adult_train[col].value_counts(dropna=False)/adult_train.shape[0],'\\n')\n",
        "# print(adult_train.head())\n",
        "# print(adult_test.head())    \n",
        "\n",
        "adult_train.reset_index(drop=True, inplace=True)\n",
        "adult_test.reset_index(drop=True, inplace=True)\n",
        "p=adult_train.shape[0]\n",
        "q=adult_test.shape[0]\n",
        "# reducing dimensionality of some very sparse features\n",
        "for i in range(0,p):\n",
        "    if adult_train.loc[i,'native-country'] not in [\"united-states\"] :\n",
        "               adult_train.loc[i,\"native-country\"] = \"non-united-stated\"        \n",
        "    if adult_train.loc[i,\"education\"] in [\"Preschool\", \"1st-4th\", \"5th-6th\", \"7th-8th\"]:\n",
        "               adult_train.loc[i,\"education\"] = \"prim-middle-school\"\n",
        "    elif adult_train.loc[i,\"education\"] in [\"9th\", \"10th\", \"11th\", \"12th\"]:\n",
        "               adult_train.loc[i,\"education\"] = \"high-school\"   \n",
        "    if adult_train.loc[i,'income'] in [\">50k\"] :\n",
        "               adult_train.loc[i,\"income\"] = 1 \n",
        "    else: \n",
        "               adult_train.loc[i,\"income\"] = 0         \n",
        "#reducing dimensionality of some very sparse features\n",
        "for i in range(0,q):                \n",
        "    if adult_test.loc[i,'native-country'] not in [\"united-states\"]:\n",
        "               adult_test.loc[i,'native-country'] = \"non-united-stated\"\n",
        "    if adult_test.loc[i,'education'] in [\"Preschool\", \"1st-4th\", \"5th-6th\", \"7th-8th\"]:\n",
        "               adult_test.loc[i,'education'] = \"prim-middle-school\"\n",
        "    elif adult_test.loc[i,'education'] in [\"9th\", \"10th\", \"11th\", \"12th\"]:\n",
        "               adult_test.loc[i,'education'] = \"high-school\"   \n",
        "    if adult_test.loc[i,'income'] in [\">50k\",\">50k.\"] :\n",
        "               adult_test.loc[i,\"income\"] = 1 \n",
        "    else: \n",
        "               adult_test.loc[i,\"income\"] = 0            \n",
        "# print(adult_train.head())\n",
        "# print(adult_test.head())\n",
        "DATA=pd.concat([adult_train,adult_test],ignore_index=True)\n",
        "# print(DATA.tail())\n",
        "m=DATA.shape[1]\n",
        "\n",
        "dat=DATA.iloc[:,0:m-1]\n",
        "\n",
        "\n",
        "# Initialize a scaler, then apply it to the features\n",
        "scaler = MinMaxScaler() # default=(0, 1)\n",
        "num_col = dat.dtypes[dat.dtypes != 'object'].index\n",
        "features_log_minmax_transform = pd.DataFrame(data = dat)\n",
        "features_log_minmax_transform[num_col] = scaler.fit_transform(features_log_minmax_transform[num_col])\n",
        "\n",
        "display(features_log_minmax_transform.head())\n",
        "\n",
        "# sens=DATA[['sex','race']]\n",
        "\n",
        "Data_c = pd.get_dummies(features_log_minmax_transform, columns=['sex','race','workclass','education','marital-status','occupation','relationship','native-country'], prefix =['s','r','work','edu','ms','occ','rls','nc'])\n",
        "r=DATA.iloc[:,m-1]\n",
        "print(Data_c)\n",
        "print(DATA['income'].value_counts())\n",
        "\n",
        "\n",
        "DATA=pd.concat([adult_train,adult_test],ignore_index=True)\n",
        "# print(DATA.tail())\n",
        "m=DATA.shape[1]\n",
        "\n",
        "dat=DATA.iloc[:,0:m-1]\n",
        "\n",
        "\n",
        "# Initialize a scaler, then apply it to the features\n",
        "scaler = MinMaxScaler() # default=(0, 1)\n",
        "num_col = dat.dtypes[dat.dtypes != 'object'].index\n",
        "features_log_minmax_transform = pd.DataFrame(data = dat)\n",
        "features_log_minmax_transform[num_col] = scaler.fit_transform(features_log_minmax_transform[num_col])\n",
        "\n",
        "display(features_log_minmax_transform.head())\n",
        "\n",
        "# sens=DATA[['sex','race']]\n",
        "\n",
        "Data_c = pd.get_dummies(features_log_minmax_transform, columns=['sex','race','workclass','education','marital-status','occupation','relationship','native-country'], prefix =['s','r','work','edu','ms','occ','rls','nc'])\n",
        "r=DATA.iloc[:,m-1]\n",
        "print(Data_c)\n",
        "print(DATA['income'].value_counts())\n",
        "#marital\n",
        "#U=80, M=24928, S=11568, D=4612\n",
        "# m_3, m_0, m_1, m_2\n",
        "#age\n",
        "#>60 and <25= a_1\n",
        "#>=25and <=60 =a_2\n",
        "# print(data.head())\n",
        "# print(data.shape[0],data.shape[1])\n",
        "\n",
        "#sensitive columns name 0='age',2='marital'\n",
        "\n",
        "\n",
        "#X_test,Y_test_pred,Y_test,e = adult_svm(Data_c , r)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#marital\n",
        "#U=80, M=24928, S=11568, D=4612\n",
        "# m_3, m_0, m_1, m_2\n",
        "#age\n",
        "#>60 and <25= a_1\n",
        "#>=25and <=60 =a_2\n",
        "# print(data.head())\n",
        "# print(data.shape[0],data.shape[1])\n",
        "\n",
        "#sensitive columns name 0='age',2='marital'\n",
        "\n",
        "\n",
        "#X_test,Y_test_pred,Y_test,e = adult_svm(Data_c , r)\n",
        "\n",
        "#X_test.reset_index(drop=True, inplace=True)\n",
        "# Y_test_pred.reset_index()\n",
        "#Y_test.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# print(X_test)\n",
        "# print(Y_test_pred)\n",
        "# print(Y_test)\n",
        "#sens=X_test[['s_male', 's_female'  ,'r_white', 'r_black', 'r_asian-pac-islander','r_amer-indian-eskimo','r_other']]\n",
        "#sensitive = sens.T\n",
        "#accu_all,DP_all,acceptance_rate,alpha_weight = main(sensitive, Y_test, Y_test_pred,e)\n",
        "\n",
        "\n",
        "# "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "private             0.738877\n",
            "self-emp-not-inc    0.082853\n",
            "local-gov           0.068530\n",
            "state-gov           0.042404\n",
            "self-emp-inc        0.035608\n",
            "federal-gov         0.031265\n",
            "without-pay         0.000464\n",
            "Name: workclass, dtype: float64 \n",
            "\n",
            "hs-grad         0.326238\n",
            "some-college    0.221404\n",
            "bachelors       0.167230\n",
            "masters         0.053942\n",
            "assoc-voc       0.043333\n",
            "11th            0.034746\n",
            "assoc-acdm      0.033420\n",
            "10th            0.027187\n",
            "7th-8th         0.018467\n",
            "prof-school     0.017970\n",
            "9th             0.015085\n",
            "12th            0.012499\n",
            "doctorate       0.012433\n",
            "5th-6th         0.009548\n",
            "1st-4th         0.005006\n",
            "preschool       0.001492\n",
            "Name: education, dtype: float64 \n",
            "\n",
            "married-civ-spouse       0.466315\n",
            "never-married            0.322459\n",
            "divorced                 0.139712\n",
            "separated                0.031132\n",
            "widowed                  0.027419\n",
            "married-spouse-absent    0.012267\n",
            "married-af-spouse        0.000696\n",
            "Name: marital-status, dtype: float64 \n",
            "\n",
            "prof-specialty       0.133877\n",
            "craft-repair         0.133612\n",
            "exec-managerial      0.132352\n",
            "adm-clerical         0.123367\n",
            "sales                0.118825\n",
            "other-service        0.106492\n",
            "machine-op-inspct    0.065181\n",
            "transport-moving     0.052119\n",
            "handlers-cleaners    0.044758\n",
            "farming-fishing      0.032790\n",
            "tech-support         0.030237\n",
            "protective-serv      0.021351\n",
            "priv-house-serv      0.004741\n",
            "armed-forces         0.000298\n",
            "Name: occupation, dtype: float64 \n",
            "\n",
            "husband           0.413202\n",
            "not-in-family     0.256150\n",
            "own-child         0.148067\n",
            "unmarried         0.106492\n",
            "wife              0.046615\n",
            "other-relative    0.029474\n",
            "Name: relationship, dtype: float64 \n",
            "\n",
            "white                 0.859790\n",
            "black                 0.093396\n",
            "asian-pac-islander    0.029673\n",
            "amer-indian-eskimo    0.009482\n",
            "other                 0.007659\n",
            "Name: race, dtype: float64 \n",
            "\n",
            "male      0.675685\n",
            "female    0.324315\n",
            "Name: sex, dtype: float64 \n",
            "\n",
            "united-states                 0.911876\n",
            "mexico                        0.020224\n",
            "philippines                   0.006233\n",
            "germany                       0.004244\n",
            "puerto-rico                   0.003614\n",
            "canada                        0.003548\n",
            "el-salvador                   0.003315\n",
            "india                         0.003315\n",
            "cuba                          0.003050\n",
            "england                       0.002851\n",
            "jamaica                       0.002652\n",
            "south                         0.002354\n",
            "china                         0.002254\n",
            "italy                         0.002254\n",
            "dominican-republic            0.002221\n",
            "vietnam                       0.002122\n",
            "guatemala                     0.002089\n",
            "japan                         0.001956\n",
            "columbia                      0.001857\n",
            "poland                        0.001857\n",
            "taiwan                        0.001392\n",
            "haiti                         0.001392\n",
            "iran                          0.001392\n",
            "portugal                      0.001127\n",
            "nicaragua                     0.001094\n",
            "peru                          0.000995\n",
            "greece                        0.000961\n",
            "ecuador                       0.000895\n",
            "france                        0.000895\n",
            "ireland                       0.000796\n",
            "hong                          0.000630\n",
            "cambodia                      0.000597\n",
            "trinadad&tobago               0.000597\n",
            "thailand                      0.000564\n",
            "laos                          0.000564\n",
            "yugoslavia                    0.000530\n",
            "outlying-us(guam-usvi-etc)    0.000464\n",
            "hungary                       0.000431\n",
            "honduras                      0.000398\n",
            "scotland                      0.000365\n",
            "holand-netherlands            0.000033\n",
            "Name: native-country, dtype: float64 \n",
            "\n",
            "<=50k    0.751078\n",
            ">50k     0.248922\n",
            "Name: income, dtype: float64 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:1763: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  isetter(loc, value)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-e525360f-918e-4966-b5b0-059d9dce1798\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>workclass</th>\n",
              "      <th>education</th>\n",
              "      <th>education-num</th>\n",
              "      <th>marital-status</th>\n",
              "      <th>occupation</th>\n",
              "      <th>relationship</th>\n",
              "      <th>race</th>\n",
              "      <th>sex</th>\n",
              "      <th>capital-gain</th>\n",
              "      <th>capital-loss</th>\n",
              "      <th>hours-per-week</th>\n",
              "      <th>native-country</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.301370</td>\n",
              "      <td>state-gov</td>\n",
              "      <td>bachelors</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>never-married</td>\n",
              "      <td>adm-clerical</td>\n",
              "      <td>not-in-family</td>\n",
              "      <td>white</td>\n",
              "      <td>male</td>\n",
              "      <td>0.02174</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.397959</td>\n",
              "      <td>united-states</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.452055</td>\n",
              "      <td>self-emp-not-inc</td>\n",
              "      <td>bachelors</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>married-civ-spouse</td>\n",
              "      <td>exec-managerial</td>\n",
              "      <td>husband</td>\n",
              "      <td>white</td>\n",
              "      <td>male</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.122449</td>\n",
              "      <td>united-states</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.287671</td>\n",
              "      <td>private</td>\n",
              "      <td>hs-grad</td>\n",
              "      <td>0.533333</td>\n",
              "      <td>divorced</td>\n",
              "      <td>handlers-cleaners</td>\n",
              "      <td>not-in-family</td>\n",
              "      <td>white</td>\n",
              "      <td>male</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.397959</td>\n",
              "      <td>united-states</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.493151</td>\n",
              "      <td>private</td>\n",
              "      <td>high-school</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>married-civ-spouse</td>\n",
              "      <td>handlers-cleaners</td>\n",
              "      <td>husband</td>\n",
              "      <td>black</td>\n",
              "      <td>male</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.397959</td>\n",
              "      <td>united-states</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.150685</td>\n",
              "      <td>private</td>\n",
              "      <td>bachelors</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>married-civ-spouse</td>\n",
              "      <td>prof-specialty</td>\n",
              "      <td>wife</td>\n",
              "      <td>black</td>\n",
              "      <td>female</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.397959</td>\n",
              "      <td>non-united-stated</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e525360f-918e-4966-b5b0-059d9dce1798')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e525360f-918e-4966-b5b0-059d9dce1798 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e525360f-918e-4966-b5b0-059d9dce1798');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "        age         workclass  ... hours-per-week     native-country\n",
              "0  0.301370         state-gov  ...       0.397959      united-states\n",
              "1  0.452055  self-emp-not-inc  ...       0.122449      united-states\n",
              "2  0.287671           private  ...       0.397959      united-states\n",
              "3  0.493151           private  ...       0.397959      united-states\n",
              "4  0.150685           private  ...       0.397959  non-united-stated\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            age  education-num  ...  nc_non-united-stated  nc_united-states\n",
            "0      0.301370       0.800000  ...                     0                 1\n",
            "1      0.452055       0.800000  ...                     0                 1\n",
            "2      0.287671       0.533333  ...                     0                 1\n",
            "3      0.493151       0.400000  ...                     0                 1\n",
            "4      0.150685       0.800000  ...                     1                 0\n",
            "...         ...            ...  ...                   ...               ...\n",
            "45216  0.219178       0.800000  ...                     0                 1\n",
            "45217  0.301370       0.800000  ...                     0                 1\n",
            "45218  0.287671       0.800000  ...                     0                 1\n",
            "45219  0.369863       0.800000  ...                     0                 1\n",
            "45220  0.246575       0.800000  ...                     0                 1\n",
            "\n",
            "[45221 rows x 59 columns]\n",
            "0    34013\n",
            "1    11208\n",
            "Name: income, dtype: int64\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-3be78a6c-f12e-4d59-a3b6-2961b92d3f27\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>workclass</th>\n",
              "      <th>education</th>\n",
              "      <th>education-num</th>\n",
              "      <th>marital-status</th>\n",
              "      <th>occupation</th>\n",
              "      <th>relationship</th>\n",
              "      <th>race</th>\n",
              "      <th>sex</th>\n",
              "      <th>capital-gain</th>\n",
              "      <th>capital-loss</th>\n",
              "      <th>hours-per-week</th>\n",
              "      <th>native-country</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.301370</td>\n",
              "      <td>state-gov</td>\n",
              "      <td>bachelors</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>never-married</td>\n",
              "      <td>adm-clerical</td>\n",
              "      <td>not-in-family</td>\n",
              "      <td>white</td>\n",
              "      <td>male</td>\n",
              "      <td>0.02174</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.397959</td>\n",
              "      <td>united-states</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.452055</td>\n",
              "      <td>self-emp-not-inc</td>\n",
              "      <td>bachelors</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>married-civ-spouse</td>\n",
              "      <td>exec-managerial</td>\n",
              "      <td>husband</td>\n",
              "      <td>white</td>\n",
              "      <td>male</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.122449</td>\n",
              "      <td>united-states</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.287671</td>\n",
              "      <td>private</td>\n",
              "      <td>hs-grad</td>\n",
              "      <td>0.533333</td>\n",
              "      <td>divorced</td>\n",
              "      <td>handlers-cleaners</td>\n",
              "      <td>not-in-family</td>\n",
              "      <td>white</td>\n",
              "      <td>male</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.397959</td>\n",
              "      <td>united-states</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.493151</td>\n",
              "      <td>private</td>\n",
              "      <td>high-school</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>married-civ-spouse</td>\n",
              "      <td>handlers-cleaners</td>\n",
              "      <td>husband</td>\n",
              "      <td>black</td>\n",
              "      <td>male</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.397959</td>\n",
              "      <td>united-states</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.150685</td>\n",
              "      <td>private</td>\n",
              "      <td>bachelors</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>married-civ-spouse</td>\n",
              "      <td>prof-specialty</td>\n",
              "      <td>wife</td>\n",
              "      <td>black</td>\n",
              "      <td>female</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.397959</td>\n",
              "      <td>non-united-stated</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3be78a6c-f12e-4d59-a3b6-2961b92d3f27')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3be78a6c-f12e-4d59-a3b6-2961b92d3f27 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3be78a6c-f12e-4d59-a3b6-2961b92d3f27');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "        age         workclass  ... hours-per-week     native-country\n",
              "0  0.301370         state-gov  ...       0.397959      united-states\n",
              "1  0.452055  self-emp-not-inc  ...       0.122449      united-states\n",
              "2  0.287671           private  ...       0.397959      united-states\n",
              "3  0.493151           private  ...       0.397959      united-states\n",
              "4  0.150685           private  ...       0.397959  non-united-stated\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            age  education-num  ...  nc_non-united-stated  nc_united-states\n",
            "0      0.301370       0.800000  ...                     0                 1\n",
            "1      0.452055       0.800000  ...                     0                 1\n",
            "2      0.287671       0.533333  ...                     0                 1\n",
            "3      0.493151       0.400000  ...                     0                 1\n",
            "4      0.150685       0.800000  ...                     1                 0\n",
            "...         ...            ...  ...                   ...               ...\n",
            "45216  0.219178       0.800000  ...                     0                 1\n",
            "45217  0.301370       0.800000  ...                     0                 1\n",
            "45218  0.287671       0.800000  ...                     0                 1\n",
            "45219  0.369863       0.800000  ...                     0                 1\n",
            "45220  0.246575       0.800000  ...                     0                 1\n",
            "\n",
            "[45221 rows x 59 columns]\n",
            "0    34013\n",
            "1    11208\n",
            "Name: income, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from aif360.datasets import AdultDataset\n",
        "from scipy.optimize import minimize\n",
        "from warnings import warn\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.base import BaseEstimator, MetaEstimatorMixin, clone\n",
        "\n",
        "\n",
        "#ad = AdultDataset(instance_weights_name='fnlwgt',features_to_drop=[])\n",
        "#WARNING:root:Missing Data: 3620 rows removed from dataset.\n",
        "single_protected = ['sex','race']\n",
        "single_privileged = [['Male'],['White']]\n",
        "#ceo = CalibratedEqualizedOdds(prot_attr = None)\n",
        "y_train = adult_train.loc[:,'income']\n",
        "y_test = adult_test.loc[:,'income']\n",
        "adult_train_X = adult_train.drop('income', axis=1)\n",
        "adult_test_X = adult_test.drop('income', axis=1)\n",
        "\n",
        "print(list(set(y_train.index.names)))\n",
        "print(y_train.index.names)      \n",
        "print(len(adult_train.columns))\n",
        "print(len(adult_train_X.columns))\n",
        "\n",
        "\n",
        "X_train = adult_train_X\n",
        "X_test = adult_test_X\n",
        "X_train, X_test, y_train, y_test = train_test_split(Data_c , r, test_size= 0.3, random_state=0, shuffle=True)\n",
        "#data_preproc = make_column_transformer((OneHotEncoder(sparse=False, handle_unknown='ignore'),\n",
        "#                                        X_train.dtypes == 'category'), remainder=StandardScaler())\n",
        "\n",
        "#X_train = pd.DataFrame(data_preproc.fit_transform(X_train), index=X_train.index)\n",
        "#X_test = pd.DataFrame(data_preproc.fit_transform(X_test), index=X_test.index)\n",
        "\n",
        "y_train=y_train.astype('int')\n",
        "y_test=y_test.astype('int')\n",
        "print(y_train)\n",
        "y_train = y_train.rename_axis('race')\n",
        "print(y_train)\n",
        "print(X_train)\n",
        "l = []\n",
        "for i in range(len(y_train)):\n",
        "    if(X_train.iloc[i,9]==1):\n",
        "        l.append(0)\n",
        "    else:\n",
        "        l.append(1)\n",
        "\n",
        "y_train.index = l\n",
        "y_train = y_train.rename_axis('s_male')\n",
        "print(y_train)\n",
        "#print(p not in y_train.index.names for p in 'race')\n",
        "print(y_train.index.names)    \n",
        "\n",
        "data_preproc = make_column_transformer(\n",
        "    (OneHotEncoder(sparse=False, handle_unknown='ignore'), X_train.dtypes == 'category'),\n",
        "        remainder=StandardScaler())\n",
        "\n",
        "#X_train = pd.DataFrame(data_preproc.fit_transform(X_train), index=X_train.index)\n",
        "#X_test = pd.DataFrame(data_preproc.transform(X_test), index=X_test.index)\n",
        "\n",
        "lr = LogisticRegression()\n",
        "#pp = CalibratedEqualizedOdds(prot_attr='s_male',cost_constraint='weighted', random_state=1234567)\n",
        "#lr = LogisticRegression(C=1000.0, random_state=0)\n",
        "#ceo = PostProcessingMeta(estimator=lr,postprocessor=pp, random_state=1234567)\n",
        "#ceo.fit(X_train, y_train)\n",
        "#ceo.prot_attr_= '9'\n",
        "\n",
        "print(X_test)\n",
        "\n",
        "\n",
        "#ceo.fit(adult_train_X, y_train)\n",
        "#z = ceo.predict(adult_test)\n",
        "\n",
        "#ad_data = AdultDataset(protected_attribute_names=single_protected, privileged_classes=single_privileged, categorical_features=[],features_to_keep=['age', 'education-num'])\n",
        "#print(ad_data.df.size)\n",
        "#['education-num', 'age', 'sex'] \n",
        "#print(ad.label_names)\n",
        "#x,y = adv._classifier_model(ad_data.features_names, 3, 0.4)\n",
        "#op = OptimPreproc(rosen, {'disp': True}, unprivileged_groups=[{'sex':'Female'}], privileged_groups=[{'sex':'Male'}])\n",
        "#ad_data_new = op.fit_transform(ad_data)\n",
        "#['income-per-year']\n",
        "\n",
        "#origin_adult = BinaryLabelDataset(df = adult_test, label_names = 'income',\n",
        "#                                protected_attribute_names = ['sex', 'race'], \n",
        "#                                privileged_protected_attributes = ['Male','White'],\n",
        "#                                instance_weights_name=None,\n",
        "                               # categorical_features=['workclass', 'education','marital-status', 'occupation', 'relationship','native-country'],  \n",
        "                               # features_to_drop=['fnlwgt'],custom_preprocessing=None,\n",
        "#                                favorable_label='1', unfavorable_label='0')\n",
        "\n",
        "\n",
        "#svm_adult = BinaryLabelDataset()\n",
        "\n",
        "#ad2 = eo.fit(ad_data,ad_data_new)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jZVU1yo5xgsg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sens=X_test[['s_male', 's_female'  ,'r_white', 'r_black', 'r_asian-pac-islander','r_amer-indian-eskimo','r_other']]\n",
        "sens_train_race= X_train[['r_white', 'r_black', 'r_asian-pac-islander','r_amer-indian-eskimo','r_other']]\n",
        "sens_train = X_train[['s_male', 's_female'  ,'r_white', 'r_black', 'r_asian-pac-islander','r_amer-indian-eskimo','r_other']]\n",
        "sens_train_sex = X_train[['s_male', 's_female']]\n",
        "\n",
        "sens_test_race = X_test[['r_white', 'r_black', 'r_asian-pac-islander','r_amer-indian-eskimo','r_other']]\n",
        "sens_test_sex = X_test[['s_male', 's_female']]\n",
        "\n",
        "\n",
        "\n",
        "print(sens)\n",
        "sensitive = sens.T\n",
        "#print(sensitive_final)"
      ],
      "metadata": {
        "id": "v-6_no3iue-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install fairlearn"
      ],
      "metadata": {
        "id": "BF-68ZbAxKs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1JRV_X08sW7",
        "outputId": "cb678e79-735d-487c-c85c-0b85ba23d077"
      },
      "source": [
        "\n",
        "#Agarwal EO multiple\n",
        "\n",
        "#np.random.seed(0)  # set seed for consistent results with ExponentiatedGradient\n",
        "from fairlearn.reductions import ExponentiatedGradient, DemographicParity,EqualizedOdds\n",
        "constraint_mul =EqualizedOdds(difference_bound=0.001)\n",
        "# classifier = DecisionTreeClassifier(min_samples_leaf=10, max_depth=4)\n",
        "print(\"hello\")\n",
        "mitigator_mul = ExponentiatedGradient(rf, constraint_mul)\n",
        "\n",
        "print(\"hello\")\n",
        "\n",
        "mitigator_mul.fit(X_train, y_train, sensitive_features=sens_train)\n",
        "print(\"hello\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello\n",
            "hello\n",
            "hello\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0apZWg-9qmb",
        "outputId": "a5c81d99-e8e8-415d-9c7c-6ca9337471cf"
      },
      "source": [
        "from fairlearn.metrics import MetricFrame\n",
        "from sklearn.metrics import accuracy_score\n",
        "from fairlearn.metrics import selection_rate\n",
        "\n",
        "y_pred_mitigated_mul1 = mitigator_mul.predict(X_train)\n",
        "print(\"hello\")\n",
        "sr_mitigated_mul1 = MetricFrame(selection_rate, y_train, y_pred_mitigated_mul1, sensitive_features=sens_train_race)\n",
        "print(sr_mitigated_mul1.overall)\n",
        "print(sr_mitigated_mul1.by_group)\n",
        "sr_mitigated_mul2 = MetricFrame(selection_rate, y_train, y_pred_mitigated_mul1, sensitive_features=sens_train_sex)\n",
        "print(sr_mitigated_mul2.overall)\n",
        "print(sr_mitigated_mul2.by_group)\n",
        "\n",
        "\n",
        "y_pred_mitigated_mul2 = mitigator_mul.predict(X_test)\n",
        "print(\"hello\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "sr_mitigated_mul3 = MetricFrame(selection_rate, y_test, y_pred_mitigated_mul2, sensitive_features=sens_test_race)\n",
        "print(sr_mitigated_mul3.overall)\n",
        "print(sr_mitigated_mul3.by_group)\n",
        "sr_mitigated_mul4 = MetricFrame(selection_rate, y_test, y_pred_mitigated_mul2, sensitive_features=sens_test_sex)\n",
        "print(sr_mitigated_mul4.overall)\n",
        "print(sr_mitigated_mul4.by_group)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/fairlearn/metrics/_metric_frame.py:67: FutureWarning: You have provided 'metrics', 'y_true', 'y_pred' as positional arguments. Please pass them as keyword arguments. From version 0.10.0 passing them as positional arguments will result in an error.\n",
            "  FutureWarning)\n",
            "Found 32 subgroups. Evaluation may be slow\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello\n",
            "0.19245592974031717\n",
            "r_white  r_black  r_asian-pac-islander  r_amer-indian-eskimo  r_other\n",
            "0        0        0                     0                     0                NaN\n",
            "                                                              1           0.113445\n",
            "                                        1                     0          0.0947368\n",
            "                                                              1                NaN\n",
            "                  1                     0                     0            0.19911\n",
            "                                                              1                NaN\n",
            "                                        1                     0                NaN\n",
            "                                                              1                NaN\n",
            "         1        0                     0                     0           0.119721\n",
            "                                                              1                NaN\n",
            "                                        1                     0                NaN\n",
            "                                                              1                NaN\n",
            "                  1                     0                     0                NaN\n",
            "                                                              1                NaN\n",
            "                                        1                     0                NaN\n",
            "                                                              1                NaN\n",
            "1        0        0                     0                     0           0.201983\n",
            "                                                              1                NaN\n",
            "                                        1                     0                NaN\n",
            "                                                              1                NaN\n",
            "                  1                     0                     0                NaN\n",
            "                                                              1                NaN\n",
            "                                        1                     0                NaN\n",
            "                                                              1                NaN\n",
            "         1        0                     0                     0                NaN\n",
            "                                                              1                NaN\n",
            "                                        1                     0                NaN\n",
            "                                                              1                NaN\n",
            "                  1                     0                     0                NaN\n",
            "                                                              1                NaN\n",
            "                                        1                     0                NaN\n",
            "                                                              1                NaN\n",
            "Name: selection_rate, dtype: object\n",
            "0.19245592974031717\n",
            "s_male  s_female\n",
            "0       0                NaN\n",
            "        1           0.116356\n",
            "1       0           0.229369\n",
            "        1                NaN\n",
            "Name: selection_rate, dtype: object\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/fairlearn/metrics/_metric_frame.py:67: FutureWarning: You have provided 'metrics', 'y_true', 'y_pred' as positional arguments. Please pass them as keyword arguments. From version 0.10.0 passing them as positional arguments will result in an error.\n",
            "  FutureWarning)\n",
            "Found 32 subgroups. Evaluation may be slow\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello\n",
            "0.18662932114690056\n",
            "r_white  r_black  r_asian-pac-islander  r_amer-indian-eskimo  r_other\n",
            "0        0        0                     0                     0                NaN\n",
            "                                                              1          0.0956522\n",
            "                                        1                     0           0.146667\n",
            "                                                              1                NaN\n",
            "                  1                     0                     0           0.212871\n",
            "                                                              1                NaN\n",
            "                                        1                     0                NaN\n",
            "                                                              1                NaN\n",
            "         1        0                     0                     0            0.12377\n",
            "                                                              1                NaN\n",
            "                                        1                     0                NaN\n",
            "                                                              1                NaN\n",
            "                  1                     0                     0                NaN\n",
            "                                                              1                NaN\n",
            "                                        1                     0                NaN\n",
            "                                                              1                NaN\n",
            "1        0        0                     0                     0           0.193698\n",
            "                                                              1                NaN\n",
            "                                        1                     0                NaN\n",
            "                                                              1                NaN\n",
            "                  1                     0                     0                NaN\n",
            "                                                              1                NaN\n",
            "                                        1                     0                NaN\n",
            "                                                              1                NaN\n",
            "         1        0                     0                     0                NaN\n",
            "                                                              1                NaN\n",
            "                                        1                     0                NaN\n",
            "                                                              1                NaN\n",
            "                  1                     0                     0                NaN\n",
            "                                                              1                NaN\n",
            "                                        1                     0                NaN\n",
            "                                                              1                NaN\n",
            "Name: selection_rate, dtype: object\n",
            "0.18662932114690056\n",
            "s_male  s_female\n",
            "0       0                NaN\n",
            "        1           0.116162\n",
            "1       0           0.219954\n",
            "        1                NaN\n",
            "Name: selection_rate, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17YmwLMVA0-r",
        "outputId": "ec30f841-3ab0-469c-d767-0bb5390d221c"
      },
      "source": [
        "#flip(y_pred_mitigated_mul2)\n",
        "find_eo_stats_multiple(y_test,y_pred_mitigated_mul2)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TPR of the  0 th sensitive groups is 0.5797567954220315\n",
            "TPR of the  1 th sensitive groups is 0.5747863247863247\n",
            "TPR of the  2 th sensitive groups is 0.5862186014935505\n",
            "TPR of the  3 th sensitive groups is 0.5308641975308642\n",
            "TPR of the  4 th sensitive groups is 0.5079365079365079\n",
            "TPR of the  5 th sensitive groups is 0.5333333333333333\n",
            "TPR of the  6 th sensitive groups is 0.3333333333333333\n",
            "=====================\n",
            "FPR of the  0 th sensitive groups is 0.06313328137178488\n",
            "FPR of the  1 th sensitive groups is 0.06095679012345679\n",
            "FPR of the  2 th sensitive groups is 0.06126889601465873\n",
            "FPR of the  3 th sensitive groups is 0.06143667296786389\n",
            "FPR of the  4 th sensitive groups is 0.07913669064748201\n",
            "FPR of the  5 th sensitive groups is 0.1037037037037037\n",
            "FPR of the  6 th sensitive groups is 0.06\n",
            "=====================\n",
            "Acceptance Rate of the  0 th sensitive groups is 0.21995440234502225\n",
            "Acceptance Rate of the  1 th sensitive groups is 0.11616161616161616\n",
            "Acceptance Rate of the  2 th sensitive groups is 0.19369755095050523\n",
            "Acceptance Rate of the  3 th sensitive groups is 0.12377049180327869\n",
            "Acceptance Rate of the  4 th sensitive groups is 0.21287128712871287\n",
            "Acceptance Rate of the  5 th sensitive groups is 0.14666666666666667\n",
            "Acceptance Rate of the  6 th sensitive groups is 0.09565217391304348\n",
            "The Difference of Equalized odds of the classifier is= 0.2965889718639209\n",
            "Accuracy of the classifier 0.8514041424043636\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z94Un5T_fezp",
        "outputId": "64cd3380-1596-47ba-bf4b-463409ec2a41"
      },
      "source": [
        "from sklearn.model_selection import cross_val_score  \n",
        "from sklearn.svm import SVC\n",
        "rf = RandomForestClassifier(n_estimators=100, max_depth=None, min_samples_split=64, random_state=0)\n",
        "#svm = SVC(kernel='rbf', random_state=0, gamma=.1, C=10.0,probability=True)\n",
        "rf.fit(X_train, y_train)\n",
        "print('The accuracy of the Random_Forest classifier on training data is {:.2f}'.format(rf.score(X_train, y_train)))\n",
        "print('The accuracy of the Random_Forest classifier on test data is {:.2f}'.format(rf.score(X_test, y_test)))\n",
        "print('####Train prediction Label###############################################')\n",
        "y_train_pred=rf.predict(X_train)\n",
        "#print(y_1)\n",
        "y_test_pred=rf.predict(X_test)\n",
        "e=rf.predict_proba(X_test)\n",
        "print(e)\n",
        "print(y_test_pred)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuracy of the Random_Forest classifier on training data is 0.88\n",
            "The accuracy of the Random_Forest classifier on test data is 0.87\n",
            "####Train prediction Label###############################################\n",
            "[[0.69829171 0.30170829]\n",
            " [0.96621278 0.03378722]\n",
            " [0.85440665 0.14559335]\n",
            " ...\n",
            " [0.82390484 0.17609516]\n",
            " [0.99506751 0.00493249]\n",
            " [1.         0.        ]]\n",
            "[0 0 0 ... 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###################Perturbation of Sensitive Attributes############################\n",
        "\n",
        "import random\n",
        "n1 = len(X_train)\n",
        "sens_train_random = sens_train_sex\n",
        "for i in range(n1):\n",
        "  #print(i)\n",
        "  ran = random.random()\n",
        "  if ran >= 0.5:\n",
        "    sens_train_random.iloc[i,0] = 1  \n",
        "    sens_train_random.iloc[i,1] = 0\n",
        "  else:\n",
        "    sens_train_random.iloc[i,0] = 0  \n",
        "    sens_train_random.iloc[i,1] = 1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cS5KbKPfNdiv",
        "outputId": "bcaf698b-7ce4-42d0-eafc-1471b57864f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  iloc._setitem_with_indexer(indexer, value)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if __name__ == '__main__':\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sens_train_random = pd.concat([sens_train_random, sens_train_race], axis=1)\n",
        "print(sens_train_random)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nei5XY0ZDnc",
        "outputId": "71d61ef0-9245-4ac6-8499-11b471dbbae1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       s_male  s_female  ...  r_amer-indian-eskimo  r_other\n",
            "10300       0         1  ...                     0        0\n",
            "39301       1         0  ...                     0        0\n",
            "2666        0         1  ...                     0        0\n",
            "17989       0         1  ...                     0        0\n",
            "11495       0         1  ...                     0        0\n",
            "...       ...       ...  ...                   ...      ...\n",
            "30403       0         1  ...                     0        0\n",
            "21243       1         0  ...                     0        0\n",
            "42613       1         0  ...                     0        0\n",
            "43567       0         1  ...                     0        0\n",
            "2732        0         1  ...                     0        0\n",
            "\n",
            "[31654 rows x 7 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Agarwal EO multiple for Perturbation Sensitive Attributes\n",
        "\n",
        "#np.random.seed(0)  # set seed for consistent results with ExponentiatedGradient\n",
        "from fairlearn.reductions import ExponentiatedGradient, DemographicParity,EqualizedOdds\n",
        "constraint_mul =EqualizedOdds(difference_bound=0.001)\n",
        "# classifier = DecisionTreeClassifier(min_samples_leaf=10, max_depth=4)\n",
        "\n",
        "mitigator_mul = ExponentiatedGradient(rf, constraint_mul)\n",
        "\n",
        "\n",
        "\n",
        "mitigator_mul.fit(X_train, y_train, sensitive_features=sens_train_random)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQa5VRO_ZqLC",
        "outputId": "b79936a0-1f36-420c-fbb9-3bdc54bcbfc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello\n",
            "hello\n",
            "hello\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fairlearn.metrics import MetricFrame\n",
        "from sklearn.metrics import accuracy_score\n",
        "from fairlearn.metrics import selection_rate\n",
        "\n",
        "y_pred_mitigated_mul1 = mitigator_mul.predict(X_train)\n",
        "print(\"hello\")\n",
        "sr_mitigated_mul1 = MetricFrame(selection_rate, y_train, y_pred_mitigated_mul1, sensitive_features=sens_train_race)\n",
        "print(sr_mitigated_mul1.overall)\n",
        "print(sr_mitigated_mul1.by_group)\n",
        "sr_mitigated_mul2 = MetricFrame(selection_rate, y_train, y_pred_mitigated_mul1, sensitive_features=sens_train_random)\n",
        "print(sr_mitigated_mul2.overall)\n",
        "print(sr_mitigated_mul2.by_group)\n",
        "\n",
        "\n",
        "y_pred_mitigated_mul2 = mitigator_mul.predict(X_test)\n",
        "print(\"hello\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "sr_mitigated_mul3 = MetricFrame(selection_rate, y_test, y_pred_mitigated_mul2, sensitive_features=sens_test_race)\n",
        "print(sr_mitigated_mul3.overall)\n",
        "print(sr_mitigated_mul3.by_group)\n",
        "sr_mitigated_mul4 = MetricFrame(selection_rate, y_test, y_pred_mitigated_mul2, sensitive_features=sens_test_sex)\n",
        "print(sr_mitigated_mul4.overall)\n",
        "print(sr_mitigated_mul4.by_group)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoEAMjw6aLVh",
        "outputId": "7d242dfc-8c8d-4ec4-d5b0-89e08a3a5d33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/fairlearn/metrics/_metric_frame.py:67: FutureWarning: You have provided 'metrics', 'y_true', 'y_pred' as positional arguments. Please pass them as keyword arguments. From version 0.10.0 passing them as positional arguments will result in an error.\n",
            "  FutureWarning)\n",
            "Found 32 subgroups. Evaluation may be slow\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Found 128 subgroups. Evaluation may be slow\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.21229544449358692\n",
            "r_white  r_black  r_asian-pac-islander  r_amer-indian-eskimo  r_other\n",
            "0        0        0                     0                     0               NaN\n",
            "                                                              1          0.142857\n",
            "                                        1                     0          0.126316\n",
            "                                                              1               NaN\n",
            "                  1                     0                     0          0.231368\n",
            "                                                              1               NaN\n",
            "                                        1                     0               NaN\n",
            "                                                              1               NaN\n",
            "         1        0                     0                     0          0.134686\n",
            "                                                              1               NaN\n",
            "                                        1                     0               NaN\n",
            "                                                              1               NaN\n",
            "                  1                     0                     0               NaN\n",
            "                                                              1               NaN\n",
            "                                        1                     0               NaN\n",
            "                                                              1               NaN\n",
            "1        0        0                     0                     0          0.221745\n",
            "                                                              1               NaN\n",
            "                                        1                     0               NaN\n",
            "                                                              1               NaN\n",
            "                  1                     0                     0               NaN\n",
            "                                                              1               NaN\n",
            "                                        1                     0               NaN\n",
            "                                                              1               NaN\n",
            "         1        0                     0                     0               NaN\n",
            "                                                              1               NaN\n",
            "                                        1                     0               NaN\n",
            "                                                              1               NaN\n",
            "                  1                     0                     0               NaN\n",
            "                                                              1               NaN\n",
            "                                        1                     0               NaN\n",
            "                                                              1               NaN\n",
            "Name: selection_rate, dtype: object\n",
            "0.21229544449358692\n",
            "s_male  s_female  r_white  r_black  r_asian-pac-islander  r_amer-indian-eskimo  r_other\n",
            "0       0         0        0        0                     0                     0          NaN\n",
            "                                                                                1          NaN\n",
            "                                                          1                     0          NaN\n",
            "                                                                                1          NaN\n",
            "                                    1                     0                     0          NaN\n",
            "                                                                                          ... \n",
            "1       1         1        1        0                     1                     1          NaN\n",
            "                                    1                     0                     0          NaN\n",
            "                                                                                1          NaN\n",
            "                                                          1                     0          NaN\n",
            "                                                                                1          NaN\n",
            "Name: selection_rate, Length: 128, dtype: object\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/fairlearn/metrics/_metric_frame.py:67: FutureWarning: You have provided 'metrics', 'y_true', 'y_pred' as positional arguments. Please pass them as keyword arguments. From version 0.10.0 passing them as positional arguments will result in an error.\n",
            "  FutureWarning)\n",
            "Found 32 subgroups. Evaluation may be slow\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello\n",
            "0.2022554728385052\n",
            "r_white  r_black  r_asian-pac-islander  r_amer-indian-eskimo  r_other\n",
            "0        0        0                     0                     0                NaN\n",
            "                                                              1          0.0608696\n",
            "                                        1                     0           0.146667\n",
            "                                                              1                NaN\n",
            "                  1                     0                     0           0.240099\n",
            "                                                              1                NaN\n",
            "                                        1                     0                NaN\n",
            "                                                              1                NaN\n",
            "         1        0                     0                     0           0.131967\n",
            "                                                              1                NaN\n",
            "                                        1                     0                NaN\n",
            "                                                              1                NaN\n",
            "                  1                     0                     0                NaN\n",
            "                                                              1                NaN\n",
            "                                        1                     0                NaN\n",
            "                                                              1                NaN\n",
            "1        0        0                     0                     0           0.210396\n",
            "                                                              1                NaN\n",
            "                                        1                     0                NaN\n",
            "                                                              1                NaN\n",
            "                  1                     0                     0                NaN\n",
            "                                                              1                NaN\n",
            "                                        1                     0                NaN\n",
            "                                                              1                NaN\n",
            "         1        0                     0                     0                NaN\n",
            "                                                              1                NaN\n",
            "                                        1                     0                NaN\n",
            "                                                              1                NaN\n",
            "                  1                     0                     0                NaN\n",
            "                                                              1                NaN\n",
            "                                        1                     0                NaN\n",
            "                                                              1                NaN\n",
            "Name: selection_rate, dtype: object\n",
            "0.2022554728385052\n",
            "s_male  s_female\n",
            "0       0                 NaN\n",
            "        1           0.0798898\n",
            "1       0            0.260124\n",
            "        1                 NaN\n",
            "Name: selection_rate, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "find_eo_stats_multiple(y_test,y_pred_mitigated_mul2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1POntv8atV9",
        "outputId": "182aa7a5-a41f-49b4-e0b1-2b88774190e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TPR of the  0 th sensitive groups is 0.6484263233190272\n",
            "TPR of the  1 th sensitive groups is 0.5405982905982906\n",
            "TPR of the  2 th sensitive groups is 0.6391717583163612\n",
            "TPR of the  3 th sensitive groups is 0.5802469135802469\n",
            "TPR of the  4 th sensitive groups is 0.6031746031746031\n",
            "TPR of the  5 th sensitive groups is 0.5333333333333333\n",
            "TPR of the  6 th sensitive groups is 0.3333333333333333\n",
            "=====================\n",
            "FPR of the  0 th sensitive groups is 0.09088074824629774\n",
            "FPR of the  1 th sensitive groups is 0.024434156378600823\n",
            "FPR of the  2 th sensitive groups is 0.0657352267521759\n",
            "FPR of the  3 th sensitive groups is 0.06332703213610585\n",
            "FPR of the  4 th sensitive groups is 0.07553956834532374\n",
            "FPR of the  5 th sensitive groups is 0.1037037037037037\n",
            "FPR of the  6 th sensitive groups is 0.02\n",
            "=====================\n",
            "Acceptance Rate of the  0 th sensitive groups is 0.26012376506351104\n",
            "Acceptance Rate of the  1 th sensitive groups is 0.07988980716253444\n",
            "Acceptance Rate of the  2 th sensitive groups is 0.21039561568761775\n",
            "Acceptance Rate of the  3 th sensitive groups is 0.1319672131147541\n",
            "Acceptance Rate of the  4 th sensitive groups is 0.2400990099009901\n",
            "Acceptance Rate of the  5 th sensitive groups is 0.14666666666666667\n",
            "Acceptance Rate of the  6 th sensitive groups is 0.06086956521739131\n",
            "The Difference of Equalized odds of the classifier is= 0.3987966936893975\n",
            "Accuracy of the classifier 0.8617232991818383\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14RjNrNUfe0A"
      },
      "source": [
        "#----------------------------------FPR, TPR computation--------------------------------------------\n",
        "#print(y_test.iloc[10])\n",
        "from random import *\n",
        "\n",
        "def find_eo_stats_multiple(y,y_pred):\n",
        "    m = 7\n",
        "    sens_stats = np.zeros((4,m), dtype = int)\n",
        "    sens_acc = np.zeros(m, dtype = float)\n",
        "    sizes = np.zeros(m, dtype = int)\n",
        "    #first row positives, second row negatives, third row true positive, fourth row false positive\n",
        "    \n",
        "    \n",
        "    for i in range(m):\n",
        "        for j in range(len(y)):\n",
        "            if(sensitive.iloc[i,j] == 1):\n",
        "                sizes[i] = sizes[i]+1\n",
        "                if(y_pred[j]==1):\n",
        "                    sens_acc[i] = sens_acc[i] + 1\n",
        "                if(y.iloc[j]==1):\n",
        "                    sens_stats[0][i] =sens_stats[0][i] + 1\n",
        "                if(y.iloc[j]==1 and y_pred[j]==1):\n",
        "                    sens_stats[2][i] =sens_stats[2][i] + 1\n",
        "                if(y.iloc[j]==0):\n",
        "                    sens_stats[1][i] =sens_stats[1][i] + 1\n",
        "                if(y.iloc[j]==0 and y_pred[j]==1):\n",
        "                    sens_stats[3][i] =sens_stats[3][i] + 1    \n",
        "        sens_acc[i] = sens_acc[i]/sizes[i]\n",
        "        #print(sens_acc[i],sizes[i])\n",
        "        \n",
        "    \n",
        "    TPR = np.zeros(m,dtype=float)\n",
        "    FPR = np.zeros(m,dtype=float)\n",
        "    \n",
        "    accu = 0\n",
        "    n = len(y)\n",
        "    #flip(y_pred)\n",
        "    for i in range(n):\n",
        "        if(y.iloc[i] == y_pred[i]):\n",
        "            accu = accu+1\n",
        "    \n",
        "    accu = accu/n\n",
        "    \n",
        "    max_tpr = -1 \n",
        "    min_tpr = 2\n",
        "    max_fpr = -1\n",
        "    min_fpr = 2\n",
        "    \n",
        "    for i in range(m):\n",
        "        TPR[i] = sens_stats[2][i]/sens_stats[0][i]\n",
        "        FPR[i] = sens_stats[3][i]/sens_stats[1][i]\n",
        "        if(TPR[i] >= max_tpr):\n",
        "            max_tpr = TPR[i]\n",
        "        if(TPR[i] <= min_tpr):\n",
        "            min_tpr = TPR[i]\n",
        "        if(FPR[i] >= max_fpr):\n",
        "            max_fpr = FPR[i]\n",
        "        if(FPR[i] <= min_fpr):\n",
        "            min_fpr = FPR[i]\n",
        "    \n",
        "    for i in  range(m):\n",
        "        print(\"TPR of the \",i,\"th sensitive groups is\",TPR[i])\n",
        "        \n",
        "    print(\"=====================\")    \n",
        "    for i in  range(m):\n",
        "        print(\"FPR of the \",i,\"th sensitive groups is\",FPR[i])\n",
        "    \n",
        "    print(\"=====================\")\n",
        "    for i in  range(m):    \n",
        "        print(\"Acceptance Rate of the \",i,\"th sensitive groups is\",sens_acc[i])\n",
        "    \n",
        "    DEO = abs(max_tpr-min_tpr) + abs(max_fpr-min_fpr)\n",
        "    \n",
        "    print(\"The Difference of Equalized odds of the classifier is=\",DEO)\n",
        "    print(\"Accuracy of the classifier\",accu)\n",
        "    \n",
        "#find_eo_stats_multiple(y_test,y_pred_CEO)\n",
        "#find_eo_stats_multiple(y_test,y_test_pred)\n",
        "\n",
        "\n",
        "\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIhYlkKIyq6-",
        "outputId": "c42af435-ce26-4ca5-e487-c3dc4b20b1fd"
      },
      "source": [
        "pip install pulp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pulp in /usr/local/lib/python3.7/dist-packages (2.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EB0jk_jxfe0J",
        "outputId": "288707b5-a72b-4d62-aafc-636080ad0d2d"
      },
      "source": [
        "import pulp as pl\n",
        "solver_list = pl.listSolvers(onlyAvailable=True)\n",
        "print(solver_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['PULP_CBC_CMD', 'PULP_CHOCO_CMD']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Gs7ctb6fe0W",
        "outputId": "f278dc9c-dbc1-4041-b65c-57dd8bd7e3c5"
      },
      "source": [
        "accu_all,DP_all,acceptance_rate,alpha_weight = main2(sensitive, y_test, y_test_pred,e)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sensitive attribute  1\n",
            "ACTUAL----------total ,accepted, aceeptance rate:\n",
            "9211\n",
            "2796\n",
            "0.30355010313755293\n",
            "sensitive attribute  2\n",
            "ACTUAL----------total ,accepted, aceeptance rate:\n",
            "4356\n",
            "468\n",
            "0.10743801652892562\n",
            "sensitive attribute  3\n",
            "ACTUAL----------total ,accepted, aceeptance rate:\n",
            "11678\n",
            "2946\n",
            "0.252269224182223\n",
            "sensitive attribute  4\n",
            "ACTUAL----------total ,accepted, aceeptance rate:\n",
            "1220\n",
            "162\n",
            "0.13278688524590163\n",
            "sensitive attribute  5\n",
            "ACTUAL----------total ,accepted, aceeptance rate:\n",
            "404\n",
            "126\n",
            "0.3118811881188119\n",
            "sensitive attribute  6\n",
            "ACTUAL----------total ,accepted, aceeptance rate:\n",
            "150\n",
            "15\n",
            "0.1\n",
            "sensitive attribute  7\n",
            "ACTUAL----------total ,accepted, aceeptance rate:\n",
            "115\n",
            "15\n",
            "0.13043478260869565\n",
            "data acceptance rates\n",
            "[0.30355010313755293, 0.10743801652892562, 0.252269224182223, 0.13278688524590163, 0.3118811881188119, 0.1, 0.13043478260869565]\n",
            "data DP\n",
            "0.21188118811881188\n",
            "sensitive attribute  1\n",
            "prec reca accuracy for each sens\n",
            "0.7673830594184576 0.6512875536480687 0.834219954402345\n",
            "SVM----------total , accepted, aceeptance rate:\n",
            "9211\n",
            "2373\n",
            "0.2576267506242536\n",
            "sensitive attribute  2\n",
            "prec reca accuracy for each sens\n",
            "0.7636363636363637 0.5384615384615384 0.9325068870523416\n",
            "SVM----------total , accepted, aceeptance rate:\n",
            "4356\n",
            "330\n",
            "0.07575757575757576\n",
            "sensitive attribute  3\n",
            "prec reca accuracy for each sens\n",
            "0.7656123276561233 0.6408689748811949 0.8599075184106868\n",
            "SVM----------total , accepted, aceeptance rate:\n",
            "11678\n",
            "2466\n",
            "0.2111662955985614\n",
            "sensitive attribute  4\n",
            "prec reca accuracy for each sens\n",
            "0.7981651376146789 0.5370370370370371 0.9204918032786885\n",
            "SVM----------total , accepted, aceeptance rate:\n",
            "1220\n",
            "109\n",
            "0.08934426229508197\n",
            "sensitive attribute  5\n",
            "prec reca accuracy for each sens\n",
            "0.7889908256880734 0.6825396825396826 0.844059405940594\n",
            "SVM----------total , accepted, aceeptance rate:\n",
            "404\n",
            "109\n",
            "0.2698019801980198\n",
            "sensitive attribute  6\n",
            "prec reca accuracy for each sens\n",
            "0.5333333333333333 0.5333333333333333 0.9066666666666666\n",
            "SVM----------total , accepted, aceeptance rate:\n",
            "150\n",
            "15\n",
            "0.1\n",
            "sensitive attribute  7\n",
            "prec reca accuracy for each sens\n",
            "1.0 0.26666666666666666 0.9043478260869565\n",
            "SVM----------total , accepted, aceeptance rate:\n",
            "115\n",
            "4\n",
            "0.034782608695652174\n",
            "data acceptance rates\n",
            "[0.2576267506242536, 0.07575757575757576, 0.2111662955985614, 0.08934426229508197, 0.2698019801980198, 0.1, 0.034782608695652174]\n",
            "data DP\n",
            "0.23501937150236765\n",
            "SVM accuracy--------------------------\n",
            "0.7669256381798002 0.6351102941176471 0.8657772536301319\n",
            "----------------This is for covergence at beta =  0.15  ----------------\n",
            "dimension of data\n",
            "7 13567\n",
            "[9211 4356]\n",
            "Optimal\n",
            "objective is:\n",
            "8168628.0\n",
            "discripency is:\n",
            "None\n",
            "gamma-epsilon-delta [0.175442, 0.142103, 0.166039, 0.164754, 0.153465, 0.14, 0.104348] 0.01 1\n",
            "<--------------------------------------->\n",
            "TPR of the  0 th sensitive groups is 0.765379113018598\n",
            "TPR of the  1 th sensitive groups is 0.8098290598290598\n",
            "TPR of the  2 th sensitive groups is 0.7780040733197556\n",
            "TPR of the  3 th sensitive groups is 0.6975308641975309\n",
            "TPR of the  4 th sensitive groups is 0.7619047619047619\n",
            "TPR of the  5 th sensitive groups is 0.7333333333333333\n",
            "TPR of the  6 th sensitive groups is 0.4666666666666667\n",
            "=====================\n",
            "FPR of the  0 th sensitive groups is 0.16430241621200312\n",
            "FPR of the  1 th sensitive groups is 0.10082304526748971\n",
            "FPR of the  2 th sensitive groups is 0.1500229042601924\n",
            "FPR of the  3 th sensitive groups is 0.06899810964083176\n",
            "FPR of the  4 th sensitive groups is 0.16546762589928057\n",
            "FPR of the  5 th sensitive groups is 0.11851851851851852\n",
            "FPR of the  6 th sensitive groups is 0.01\n",
            "=====================\n",
            "Acceptance Rate of the  0 th sensitive groups is 0.34675930952122463\n",
            "Acceptance Rate of the  1 th sensitive groups is 0.17699724517906337\n",
            "Acceptance Rate of the  2 th sensitive groups is 0.3084432265798938\n",
            "Acceptance Rate of the  3 th sensitive groups is 0.15245901639344261\n",
            "Acceptance Rate of the  4 th sensitive groups is 0.35148514851485146\n",
            "Acceptance Rate of the  5 th sensitive groups is 0.18\n",
            "Acceptance Rate of the  6 th sensitive groups is 0.06956521739130435\n",
            "The Difference of Equalized odds of the classifier is= 0.4986300190616737\n",
            "Accuracy of the classifier 0.8385051964325201\n",
            "sensitive attribute  1\n",
            "precision 0.6700062617407639\n",
            "recall 0.765379113018598\n",
            "FPR 0.16430241621200312\n",
            "TP,FP,TN,FN\n",
            "2140 1054 5361 656\n",
            "sensitive attribute  2\n",
            "precision 0.4915693904020752\n",
            "recall 0.8098290598290598\n",
            "FPR 0.10082304526748971\n",
            "TP,FP,TN,FN\n",
            "379 392 3496 89\n",
            "sensitive attribute  3\n",
            "precision 0.6363131593559134\n",
            "recall 0.7780040733197556\n",
            "FPR 0.1500229042601924\n",
            "TP,FP,TN,FN\n",
            "2292 1310 7422 654\n",
            "sensitive attribute  4\n",
            "precision 0.6075268817204301\n",
            "recall 0.6975308641975309\n",
            "FPR 0.06899810964083176\n",
            "TP,FP,TN,FN\n",
            "113 73 985 49\n",
            "sensitive attribute  5\n",
            "precision 0.676056338028169\n",
            "recall 0.7619047619047619\n",
            "FPR 0.16546762589928057\n",
            "TP,FP,TN,FN\n",
            "96 46 232 30\n",
            "sensitive attribute  6\n",
            "precision 0.4074074074074074\n",
            "recall 0.7333333333333333\n",
            "FPR 0.11851851851851852\n",
            "TP,FP,TN,FN\n",
            "11 16 119 4\n",
            "sensitive attribute  7\n",
            "precision 0.875\n",
            "recall 0.4666666666666667\n",
            "FPR 0.01\n",
            "TP,FP,TN,FN\n",
            "7 1 99 8\n",
            "acceptance rates\n",
            "[0.34675930952122463, 0.17699724517906337, 0.3084432265798938, 0.15245901639344261, 0.35148514851485146, 0.18, 0.06956521739130435]\n",
            "DP\n",
            "0.28191993112354713\n",
            "dimension of data\n",
            "7 13567\n",
            "[9211 4356]\n",
            "Optimal\n",
            "objective is:\n",
            "7429647.0\n",
            "discripency is:\n",
            "None\n",
            "gamma-epsilon-delta [0.175442, 0.142103, 0.166039, 0.164754, 0.153465, 0.14, 0.104348] 0.01 1\n",
            "<--------------------------------------->\n",
            "TPR of the  0 th sensitive groups is 0.7550071530758226\n",
            "TPR of the  1 th sensitive groups is 0.7905982905982906\n",
            "TPR of the  2 th sensitive groups is 0.7661235573659199\n",
            "TPR of the  3 th sensitive groups is 0.6790123456790124\n",
            "TPR of the  4 th sensitive groups is 0.7619047619047619\n",
            "TPR of the  5 th sensitive groups is 0.7333333333333333\n",
            "TPR of the  6 th sensitive groups is 0.4666666666666667\n",
            "=====================\n",
            "FPR of the  0 th sensitive groups is 0.15183164458300857\n",
            "FPR of the  1 th sensitive groups is 0.08641975308641975\n",
            "FPR of the  2 th sensitive groups is 0.13605130554283096\n",
            "FPR of the  3 th sensitive groups is 0.062381852551984876\n",
            "FPR of the  4 th sensitive groups is 0.15467625899280577\n",
            "FPR of the  5 th sensitive groups is 0.08888888888888889\n",
            "FPR of the  6 th sensitive groups is 0.01\n",
            "=====================\n",
            "Acceptance Rate of the  0 th sensitive groups is 0.3349256323960482\n",
            "Acceptance Rate of the  1 th sensitive groups is 0.1620752984389348\n",
            "Acceptance Rate of the  2 th sensitive groups is 0.29499914368898783\n",
            "Acceptance Rate of the  3 th sensitive groups is 0.14426229508196722\n",
            "Acceptance Rate of the  4 th sensitive groups is 0.34405940594059403\n",
            "Acceptance Rate of the  5 th sensitive groups is 0.15333333333333332\n",
            "Acceptance Rate of the  6 th sensitive groups is 0.06956521739130435\n",
            "The Difference of Equalized odds of the classifier is= 0.4686078829244297\n",
            "Accuracy of the classifier 0.8457286061767524\n",
            "sensitive attribute  1\n",
            "precision 0.6842787682333874\n",
            "recall 0.7550071530758226\n",
            "FPR 0.15183164458300857\n",
            "TP,FP,TN,FN\n",
            "2111 974 5441 685\n",
            "sensitive attribute  2\n",
            "precision 0.5240793201133145\n",
            "recall 0.7905982905982906\n",
            "FPR 0.08641975308641975\n",
            "TP,FP,TN,FN\n",
            "370 336 3552 98\n",
            "sensitive attribute  3\n",
            "precision 0.6551523947750363\n",
            "recall 0.7661235573659199\n",
            "FPR 0.13605130554283096\n",
            "TP,FP,TN,FN\n",
            "2257 1188 7544 689\n",
            "sensitive attribute  4\n",
            "precision 0.625\n",
            "recall 0.6790123456790124\n",
            "FPR 0.062381852551984876\n",
            "TP,FP,TN,FN\n",
            "110 66 992 52\n",
            "sensitive attribute  5\n",
            "precision 0.6906474820143885\n",
            "recall 0.7619047619047619\n",
            "FPR 0.15467625899280577\n",
            "TP,FP,TN,FN\n",
            "96 43 235 30\n",
            "sensitive attribute  6\n",
            "precision 0.4782608695652174\n",
            "recall 0.7333333333333333\n",
            "FPR 0.08888888888888889\n",
            "TP,FP,TN,FN\n",
            "11 12 123 4\n",
            "sensitive attribute  7\n",
            "precision 0.875\n",
            "recall 0.4666666666666667\n",
            "FPR 0.01\n",
            "TP,FP,TN,FN\n",
            "7 1 99 8\n",
            "acceptance rates\n",
            "[0.3349256323960482, 0.1620752984389348, 0.29499914368898783, 0.14426229508196722, 0.34405940594059403, 0.15333333333333332, 0.06956521739130435]\n",
            "DP\n",
            "0.2744941885492897\n",
            "dimension of data\n",
            "7 13567\n",
            "[9211 4356]\n",
            "Optimal\n",
            "objective is:\n",
            "7413945.0\n",
            "discripency is:\n",
            "None\n",
            "gamma-epsilon-delta [0.175442, 0.142103, 0.166039, 0.164754, 0.153465, 0.14, 0.104348] 0.01 1\n",
            "<--------------------------------------->\n",
            "TPR of the  0 th sensitive groups is 0.7546494992846924\n",
            "TPR of the  1 th sensitive groups is 0.7884615384615384\n",
            "TPR of the  2 th sensitive groups is 0.7657841140529531\n",
            "TPR of the  3 th sensitive groups is 0.6728395061728395\n",
            "TPR of the  4 th sensitive groups is 0.7619047619047619\n",
            "TPR of the  5 th sensitive groups is 0.7333333333333333\n",
            "TPR of the  6 th sensitive groups is 0.4666666666666667\n",
            "=====================\n",
            "FPR of the  0 th sensitive groups is 0.15151987529228372\n",
            "FPR of the  1 th sensitive groups is 0.08641975308641975\n",
            "FPR of the  2 th sensitive groups is 0.13582226294090702\n",
            "FPR of the  3 th sensitive groups is 0.062381852551984876\n",
            "FPR of the  4 th sensitive groups is 0.15467625899280577\n",
            "FPR of the  5 th sensitive groups is 0.08888888888888889\n",
            "FPR of the  6 th sensitive groups is 0.01\n",
            "=====================\n",
            "Acceptance Rate of the  0 th sensitive groups is 0.3345999348604929\n",
            "Acceptance Rate of the  1 th sensitive groups is 0.1618457300275482\n",
            "Acceptance Rate of the  2 th sensitive groups is 0.29474225038533997\n",
            "Acceptance Rate of the  3 th sensitive groups is 0.14344262295081966\n",
            "Acceptance Rate of the  4 th sensitive groups is 0.34405940594059403\n",
            "Acceptance Rate of the  5 th sensitive groups is 0.15333333333333332\n",
            "Acceptance Rate of the  6 th sensitive groups is 0.06956521739130435\n",
            "The Difference of Equalized odds of the classifier is= 0.46647113078767755\n",
            "Accuracy of the classifier 0.8457286061767524\n",
            "sensitive attribute  1\n",
            "precision 0.6846203763789747\n",
            "recall 0.7546494992846924\n",
            "FPR 0.15151987529228372\n",
            "TP,FP,TN,FN\n",
            "2110 972 5443 686\n",
            "sensitive attribute  2\n",
            "precision 0.5234042553191489\n",
            "recall 0.7884615384615384\n",
            "FPR 0.08641975308641975\n",
            "TP,FP,TN,FN\n",
            "369 336 3552 99\n",
            "sensitive attribute  3\n",
            "precision 0.6554328878558977\n",
            "recall 0.7657841140529531\n",
            "FPR 0.13582226294090702\n",
            "TP,FP,TN,FN\n",
            "2256 1186 7546 690\n",
            "sensitive attribute  4\n",
            "precision 0.6228571428571429\n",
            "recall 0.6728395061728395\n",
            "FPR 0.062381852551984876\n",
            "TP,FP,TN,FN\n",
            "109 66 992 53\n",
            "sensitive attribute  5\n",
            "precision 0.6906474820143885\n",
            "recall 0.7619047619047619\n",
            "FPR 0.15467625899280577\n",
            "TP,FP,TN,FN\n",
            "96 43 235 30\n",
            "sensitive attribute  6\n",
            "precision 0.4782608695652174\n",
            "recall 0.7333333333333333\n",
            "FPR 0.08888888888888889\n",
            "TP,FP,TN,FN\n",
            "11 12 123 4\n",
            "sensitive attribute  7\n",
            "precision 0.875\n",
            "recall 0.4666666666666667\n",
            "FPR 0.01\n",
            "TP,FP,TN,FN\n",
            "7 1 99 8\n",
            "acceptance rates\n",
            "[0.3345999348604929, 0.1618457300275482, 0.29474225038533997, 0.14344262295081966, 0.34405940594059403, 0.15333333333333332, 0.06956521739130435]\n",
            "DP\n",
            "0.2744941885492897\n",
            "dimension of data\n",
            "7 13567\n",
            "[9211 4356]\n",
            "Optimal\n",
            "objective is:\n",
            "7372271.0\n",
            "discripency is:\n",
            "None\n",
            "gamma-epsilon-delta [0.175442, 0.142103, 0.166039, 0.164754, 0.153465, 0.14, 0.104348] 0.01 1\n",
            "<--------------------------------------->\n",
            "TPR of the  0 th sensitive groups is 0.753934191702432\n",
            "TPR of the  1 th sensitive groups is 0.7863247863247863\n",
            "TPR of the  2 th sensitive groups is 0.7647657841140529\n",
            "TPR of the  3 th sensitive groups is 0.6728395061728395\n",
            "TPR of the  4 th sensitive groups is 0.7619047619047619\n",
            "TPR of the  5 th sensitive groups is 0.7333333333333333\n",
            "TPR of the  6 th sensitive groups is 0.4666666666666667\n",
            "=====================\n",
            "FPR of the  0 th sensitive groups is 0.150896336710834\n",
            "FPR of the  1 th sensitive groups is 0.08564814814814815\n",
            "FPR of the  2 th sensitive groups is 0.13536417773705908\n",
            "FPR of the  3 th sensitive groups is 0.06049149338374291\n",
            "FPR of the  4 th sensitive groups is 0.1510791366906475\n",
            "FPR of the  5 th sensitive groups is 0.08888888888888889\n",
            "FPR of the  6 th sensitive groups is 0.01\n",
            "=====================\n",
            "Acceptance Rate of the  0 th sensitive groups is 0.33394853978938227\n",
            "Acceptance Rate of the  1 th sensitive groups is 0.16092745638200184\n",
            "Acceptance Rate of the  2 th sensitive groups is 0.2941428326768282\n",
            "Acceptance Rate of the  3 th sensitive groups is 0.1418032786885246\n",
            "Acceptance Rate of the  4 th sensitive groups is 0.3415841584158416\n",
            "Acceptance Rate of the  5 th sensitive groups is 0.15333333333333332\n",
            "Acceptance Rate of the  6 th sensitive groups is 0.06956521739130435\n",
            "The Difference of Equalized odds of the classifier is= 0.4607372563487671\n",
            "Accuracy of the classifier 0.8460234392275374\n",
            "sensitive attribute  1\n",
            "precision 0.6853055916775033\n",
            "recall 0.753934191702432\n",
            "FPR 0.150896336710834\n",
            "TP,FP,TN,FN\n",
            "2108 968 5447 688\n",
            "sensitive attribute  2\n",
            "precision 0.5249643366619116\n",
            "recall 0.7863247863247863\n",
            "FPR 0.08564814814814815\n",
            "TP,FP,TN,FN\n",
            "368 333 3555 100\n",
            "sensitive attribute  3\n",
            "precision 0.6558951965065503\n",
            "recall 0.7647657841140529\n",
            "FPR 0.13536417773705908\n",
            "TP,FP,TN,FN\n",
            "2253 1182 7550 693\n",
            "sensitive attribute  4\n",
            "precision 0.630057803468208\n",
            "recall 0.6728395061728395\n",
            "FPR 0.06049149338374291\n",
            "TP,FP,TN,FN\n",
            "109 64 994 53\n",
            "sensitive attribute  5\n",
            "precision 0.6956521739130435\n",
            "recall 0.7619047619047619\n",
            "FPR 0.1510791366906475\n",
            "TP,FP,TN,FN\n",
            "96 42 236 30\n",
            "sensitive attribute  6\n",
            "precision 0.4782608695652174\n",
            "recall 0.7333333333333333\n",
            "FPR 0.08888888888888889\n",
            "TP,FP,TN,FN\n",
            "11 12 123 4\n",
            "sensitive attribute  7\n",
            "precision 0.875\n",
            "recall 0.4666666666666667\n",
            "FPR 0.01\n",
            "TP,FP,TN,FN\n",
            "7 1 99 8\n",
            "acceptance rates\n",
            "[0.33394853978938227, 0.16092745638200184, 0.2941428326768282, 0.1418032786885246, 0.3415841584158416, 0.15333333333333332, 0.06956521739130435]\n",
            "DP\n",
            "0.2720189410245373\n",
            "dimension of data\n",
            "7 13567\n",
            "[9211 4356]\n",
            "Optimal\n",
            "objective is:\n",
            "7315123.0\n",
            "discripency is:\n",
            "None\n",
            "gamma-epsilon-delta [0.175442, 0.142103, 0.166039, 0.164754, 0.153465, 0.14, 0.104348] 0.01 1\n",
            "<--------------------------------------->\n",
            "TPR of the  0 th sensitive groups is 0.7528612303290415\n",
            "TPR of the  1 th sensitive groups is 0.7841880341880342\n",
            "TPR of the  2 th sensitive groups is 0.763408010862186\n",
            "TPR of the  3 th sensitive groups is 0.6728395061728395\n",
            "TPR of the  4 th sensitive groups is 0.7619047619047619\n",
            "TPR of the  5 th sensitive groups is 0.7333333333333333\n",
            "TPR of the  6 th sensitive groups is 0.4666666666666667\n",
            "=====================\n",
            "FPR of the  0 th sensitive groups is 0.1499610288386594\n",
            "FPR of the  1 th sensitive groups is 0.084619341563786\n",
            "FPR of the  2 th sensitive groups is 0.1342189647274393\n",
            "FPR of the  3 th sensitive groups is 0.06049149338374291\n",
            "FPR of the  4 th sensitive groups is 0.1510791366906475\n",
            "FPR of the  5 th sensitive groups is 0.08888888888888889\n",
            "FPR of the  6 th sensitive groups is 0.01\n",
            "=====================\n",
            "Acceptance Rate of the  0 th sensitive groups is 0.3329714471827163\n",
            "Acceptance Rate of the  1 th sensitive groups is 0.15977961432506887\n",
            "Acceptance Rate of the  2 th sensitive groups is 0.29294399725980474\n",
            "Acceptance Rate of the  3 th sensitive groups is 0.1418032786885246\n",
            "Acceptance Rate of the  4 th sensitive groups is 0.3415841584158416\n",
            "Acceptance Rate of the  5 th sensitive groups is 0.15333333333333332\n",
            "Acceptance Rate of the  6 th sensitive groups is 0.06956521739130435\n",
            "The Difference of Equalized odds of the classifier is= 0.458600504212015\n",
            "Accuracy of the classifier 0.8464656888037149\n",
            "sensitive attribute  1\n",
            "precision 0.6863384414737529\n",
            "recall 0.7528612303290415\n",
            "FPR 0.1499610288386594\n",
            "TP,FP,TN,FN\n",
            "2105 962 5453 691\n",
            "sensitive attribute  2\n",
            "precision 0.5272988505747126\n",
            "recall 0.7841880341880342\n",
            "FPR 0.084619341563786\n",
            "TP,FP,TN,FN\n",
            "367 329 3559 101\n",
            "sensitive attribute  3\n",
            "precision 0.6574101140017539\n",
            "recall 0.763408010862186\n",
            "FPR 0.1342189647274393\n",
            "TP,FP,TN,FN\n",
            "2249 1172 7560 697\n",
            "sensitive attribute  4\n",
            "precision 0.630057803468208\n",
            "recall 0.6728395061728395\n",
            "FPR 0.06049149338374291\n",
            "TP,FP,TN,FN\n",
            "109 64 994 53\n",
            "sensitive attribute  5\n",
            "precision 0.6956521739130435\n",
            "recall 0.7619047619047619\n",
            "FPR 0.1510791366906475\n",
            "TP,FP,TN,FN\n",
            "96 42 236 30\n",
            "sensitive attribute  6\n",
            "precision 0.4782608695652174\n",
            "recall 0.7333333333333333\n",
            "FPR 0.08888888888888889\n",
            "TP,FP,TN,FN\n",
            "11 12 123 4\n",
            "sensitive attribute  7\n",
            "precision 0.875\n",
            "recall 0.4666666666666667\n",
            "FPR 0.01\n",
            "TP,FP,TN,FN\n",
            "7 1 99 8\n",
            "acceptance rates\n",
            "[0.3329714471827163, 0.15977961432506887, 0.29294399725980474, 0.1418032786885246, 0.3415841584158416, 0.15333333333333332, 0.06956521739130435]\n",
            "DP\n",
            "0.2720189410245373\n",
            "dimension of data\n",
            "7 13567\n",
            "[9211 4356]\n",
            "Optimal\n",
            "objective is:\n",
            "5200210.0\n",
            "discripency is:\n",
            "None\n",
            "gamma-epsilon-delta [0.175442, 0.142103, 0.166039, 0.164754, 0.153465, 0.14, 0.104348] 0.01 1\n",
            "<--------------------------------------->\n",
            "TPR of the  0 th sensitive groups is 0.5053648068669528\n",
            "TPR of the  1 th sensitive groups is 0.6153846153846154\n",
            "TPR of the  2 th sensitive groups is 0.5298710115410726\n",
            "TPR of the  3 th sensitive groups is 0.4876543209876543\n",
            "TPR of the  4 th sensitive groups is 0.3888888888888889\n",
            "TPR of the  5 th sensitive groups is 0.3333333333333333\n",
            "TPR of the  6 th sensitive groups is 0.4666666666666667\n",
            "=====================\n",
            "FPR of the  0 th sensitive groups is 0.08293063133281371\n",
            "FPR of the  1 th sensitive groups is 0.08873456790123457\n",
            "FPR of the  2 th sensitive groups is 0.09012826385707742\n",
            "FPR of the  3 th sensitive groups is 0.0500945179584121\n",
            "FPR of the  4 th sensitive groups is 0.09352517985611511\n",
            "FPR of the  5 th sensitive groups is 0.07407407407407407\n",
            "FPR of the  6 th sensitive groups is 0.01\n",
            "=====================\n",
            "Acceptance Rate of the  0 th sensitive groups is 0.21116056888502877\n",
            "Acceptance Rate of the  1 th sensitive groups is 0.1453168044077135\n",
            "Acceptance Rate of the  2 th sensitive groups is 0.20106182565507794\n",
            "Acceptance Rate of the  3 th sensitive groups is 0.10819672131147541\n",
            "Acceptance Rate of the  4 th sensitive groups is 0.18564356435643564\n",
            "Acceptance Rate of the  5 th sensitive groups is 0.1\n",
            "Acceptance Rate of the  6 th sensitive groups is 0.06956521739130435\n",
            "The Difference of Equalized odds of the classifier is= 0.36557646190739723\n",
            "Accuracy of the classifier 0.8201518390211543\n",
            "sensitive attribute  1\n",
            "precision 0.7264781491002571\n",
            "recall 0.5053648068669528\n",
            "FPR 0.08293063133281371\n",
            "TP,FP,TN,FN\n",
            "1413 532 5883 1383\n",
            "sensitive attribute  2\n",
            "precision 0.4549763033175355\n",
            "recall 0.6153846153846154\n",
            "FPR 0.08873456790123457\n",
            "TP,FP,TN,FN\n",
            "288 345 3543 180\n",
            "sensitive attribute  3\n",
            "precision 0.6648211243611585\n",
            "recall 0.5298710115410726\n",
            "FPR 0.09012826385707742\n",
            "TP,FP,TN,FN\n",
            "1561 787 7945 1385\n",
            "sensitive attribute  4\n",
            "precision 0.5984848484848485\n",
            "recall 0.4876543209876543\n",
            "FPR 0.0500945179584121\n",
            "TP,FP,TN,FN\n",
            "79 53 1005 83\n",
            "sensitive attribute  5\n",
            "precision 0.6533333333333333\n",
            "recall 0.3888888888888889\n",
            "FPR 0.09352517985611511\n",
            "TP,FP,TN,FN\n",
            "49 26 252 77\n",
            "sensitive attribute  6\n",
            "precision 0.3333333333333333\n",
            "recall 0.3333333333333333\n",
            "FPR 0.07407407407407407\n",
            "TP,FP,TN,FN\n",
            "5 10 125 10\n",
            "sensitive attribute  7\n",
            "precision 0.875\n",
            "recall 0.4666666666666667\n",
            "FPR 0.01\n",
            "TP,FP,TN,FN\n",
            "7 1 99 8\n",
            "acceptance rates\n",
            "[0.21116056888502877, 0.1453168044077135, 0.20106182565507794, 0.10819672131147541, 0.18564356435643564, 0.1, 0.06956521739130435]\n",
            "DP\n",
            "0.14159535149372443\n",
            "----------------This is for covergence at beta =  0.2  ----------------\n",
            "dimension of data\n",
            "7 13567\n",
            "[9211 4356]\n",
            "Optimal\n",
            "objective is:\n",
            "8168628.0\n",
            "discripency is:\n",
            "None\n",
            "gamma-epsilon-delta [0.175442, 0.142103, 0.166039, 0.164754, 0.153465, 0.14, 0.104348] 0.01 1\n",
            "<--------------------------------------->\n",
            "TPR of the  0 th sensitive groups is 0.765379113018598\n",
            "TPR of the  1 th sensitive groups is 0.8098290598290598\n",
            "TPR of the  2 th sensitive groups is 0.7780040733197556\n",
            "TPR of the  3 th sensitive groups is 0.6975308641975309\n",
            "TPR of the  4 th sensitive groups is 0.7619047619047619\n",
            "TPR of the  5 th sensitive groups is 0.7333333333333333\n",
            "TPR of the  6 th sensitive groups is 0.4666666666666667\n",
            "=====================\n",
            "FPR of the  0 th sensitive groups is 0.16430241621200312\n",
            "FPR of the  1 th sensitive groups is 0.10082304526748971\n",
            "FPR of the  2 th sensitive groups is 0.1500229042601924\n",
            "FPR of the  3 th sensitive groups is 0.06899810964083176\n",
            "FPR of the  4 th sensitive groups is 0.16546762589928057\n",
            "FPR of the  5 th sensitive groups is 0.11851851851851852\n",
            "FPR of the  6 th sensitive groups is 0.01\n",
            "=====================\n",
            "Acceptance Rate of the  0 th sensitive groups is 0.34675930952122463\n",
            "Acceptance Rate of the  1 th sensitive groups is 0.17699724517906337\n",
            "Acceptance Rate of the  2 th sensitive groups is 0.3084432265798938\n",
            "Acceptance Rate of the  3 th sensitive groups is 0.15245901639344261\n",
            "Acceptance Rate of the  4 th sensitive groups is 0.35148514851485146\n",
            "Acceptance Rate of the  5 th sensitive groups is 0.18\n",
            "Acceptance Rate of the  6 th sensitive groups is 0.06956521739130435\n",
            "The Difference of Equalized odds of the classifier is= 0.4986300190616737\n",
            "Accuracy of the classifier 0.8385051964325201\n",
            "sensitive attribute  1\n",
            "precision 0.6700062617407639\n",
            "recall 0.765379113018598\n",
            "FPR 0.16430241621200312\n",
            "TP,FP,TN,FN\n",
            "2140 1054 5361 656\n",
            "sensitive attribute  2\n",
            "precision 0.4915693904020752\n",
            "recall 0.8098290598290598\n",
            "FPR 0.10082304526748971\n",
            "TP,FP,TN,FN\n",
            "379 392 3496 89\n",
            "sensitive attribute  3\n",
            "precision 0.6363131593559134\n",
            "recall 0.7780040733197556\n",
            "FPR 0.1500229042601924\n",
            "TP,FP,TN,FN\n",
            "2292 1310 7422 654\n",
            "sensitive attribute  4\n",
            "precision 0.6075268817204301\n",
            "recall 0.6975308641975309\n",
            "FPR 0.06899810964083176\n",
            "TP,FP,TN,FN\n",
            "113 73 985 49\n",
            "sensitive attribute  5\n",
            "precision 0.676056338028169\n",
            "recall 0.7619047619047619\n",
            "FPR 0.16546762589928057\n",
            "TP,FP,TN,FN\n",
            "96 46 232 30\n",
            "sensitive attribute  6\n",
            "precision 0.4074074074074074\n",
            "recall 0.7333333333333333\n",
            "FPR 0.11851851851851852\n",
            "TP,FP,TN,FN\n",
            "11 16 119 4\n",
            "sensitive attribute  7\n",
            "precision 0.875\n",
            "recall 0.4666666666666667\n",
            "FPR 0.01\n",
            "TP,FP,TN,FN\n",
            "7 1 99 8\n",
            "acceptance rates\n",
            "[0.34675930952122463, 0.17699724517906337, 0.3084432265798938, 0.15245901639344261, 0.35148514851485146, 0.18, 0.06956521739130435]\n",
            "DP\n",
            "0.28191993112354713\n",
            "dimension of data\n",
            "7 13567\n",
            "[9211 4356]\n",
            "Optimal\n",
            "objective is:\n",
            "7617754.0\n",
            "discripency is:\n",
            "None\n",
            "gamma-epsilon-delta [0.175442, 0.142103, 0.166039, 0.164754, 0.153465, 0.14, 0.104348] 0.01 1\n",
            "<--------------------------------------->\n",
            "TPR of the  0 th sensitive groups is 0.7575107296137339\n",
            "TPR of the  1 th sensitive groups is 0.7948717948717948\n",
            "TPR of the  2 th sensitive groups is 0.7691785471826205\n",
            "TPR of the  3 th sensitive groups is 0.6790123456790124\n",
            "TPR of the  4 th sensitive groups is 0.7619047619047619\n",
            "TPR of the  5 th sensitive groups is 0.7333333333333333\n",
            "TPR of the  6 th sensitive groups is 0.4666666666666667\n",
            "=====================\n",
            "FPR of the  0 th sensitive groups is 0.15510522213561964\n",
            "FPR of the  1 th sensitive groups is 0.09027777777777778\n",
            "FPR of the  2 th sensitive groups is 0.13960146587265232\n",
            "FPR of the  3 th sensitive groups is 0.06427221172022685\n",
            "FPR of the  4 th sensitive groups is 0.1618705035971223\n",
            "FPR of the  5 th sensitive groups is 0.0962962962962963\n",
            "FPR of the  6 th sensitive groups is 0.01\n",
            "=====================\n",
            "Acceptance Rate of the  0 th sensitive groups is 0.3379654760612311\n",
            "Acceptance Rate of the  1 th sensitive groups is 0.1659779614325069\n",
            "Acceptance Rate of the  2 th sensitive groups is 0.2984243877376263\n",
            "Acceptance Rate of the  3 th sensitive groups is 0.14590163934426228\n",
            "Acceptance Rate of the  4 th sensitive groups is 0.349009900990099\n",
            "Acceptance Rate of the  5 th sensitive groups is 0.16\n",
            "Acceptance Rate of the  6 th sensitive groups is 0.06956521739130435\n",
            "The Difference of Equalized odds of the classifier is= 0.48007563180225044\n",
            "Accuracy of the classifier 0.8437384830839537\n",
            "sensitive attribute  1\n",
            "precision 0.6803726309026662\n",
            "recall 0.7575107296137339\n",
            "FPR 0.15510522213561964\n",
            "TP,FP,TN,FN\n",
            "2118 995 5420 678\n",
            "sensitive attribute  2\n",
            "precision 0.5145228215767634\n",
            "recall 0.7948717948717948\n",
            "FPR 0.09027777777777778\n",
            "TP,FP,TN,FN\n",
            "372 351 3537 96\n",
            "sensitive attribute  3\n",
            "precision 0.6502152080344333\n",
            "recall 0.7691785471826205\n",
            "FPR 0.13960146587265232\n",
            "TP,FP,TN,FN\n",
            "2266 1219 7513 680\n",
            "sensitive attribute  4\n",
            "precision 0.6179775280898876\n",
            "recall 0.6790123456790124\n",
            "FPR 0.06427221172022685\n",
            "TP,FP,TN,FN\n",
            "110 68 990 52\n",
            "sensitive attribute  5\n",
            "precision 0.6808510638297872\n",
            "recall 0.7619047619047619\n",
            "FPR 0.1618705035971223\n",
            "TP,FP,TN,FN\n",
            "96 45 233 30\n",
            "sensitive attribute  6\n",
            "precision 0.4583333333333333\n",
            "recall 0.7333333333333333\n",
            "FPR 0.0962962962962963\n",
            "TP,FP,TN,FN\n",
            "11 13 122 4\n",
            "sensitive attribute  7\n",
            "precision 0.875\n",
            "recall 0.4666666666666667\n",
            "FPR 0.01\n",
            "TP,FP,TN,FN\n",
            "7 1 99 8\n",
            "acceptance rates\n",
            "[0.3379654760612311, 0.1659779614325069, 0.2984243877376263, 0.14590163934426228, 0.349009900990099, 0.16, 0.06956521739130435]\n",
            "DP\n",
            "0.27944468359879465\n",
            "dimension of data\n",
            "7 13567\n",
            "[9211 4356]\n",
            "Optimal\n",
            "objective is:\n",
            "7596530.0\n",
            "discripency is:\n",
            "None\n",
            "gamma-epsilon-delta [0.175442, 0.142103, 0.166039, 0.164754, 0.153465, 0.14, 0.104348] 0.01 1\n",
            "<--------------------------------------->\n",
            "TPR of the  0 th sensitive groups is 0.7571530758226037\n",
            "TPR of the  1 th sensitive groups is 0.7948717948717948\n",
            "TPR of the  2 th sensitive groups is 0.7688391038696538\n",
            "TPR of the  3 th sensitive groups is 0.6790123456790124\n",
            "TPR of the  4 th sensitive groups is 0.7619047619047619\n",
            "TPR of the  5 th sensitive groups is 0.7333333333333333\n",
            "TPR of the  6 th sensitive groups is 0.4666666666666667\n",
            "=====================\n",
            "FPR of the  0 th sensitive groups is 0.1547934528448948\n",
            "FPR of the  1 th sensitive groups is 0.08976337448559671\n",
            "FPR of the  2 th sensitive groups is 0.13914338066880438\n",
            "FPR of the  3 th sensitive groups is 0.06427221172022685\n",
            "FPR of the  4 th sensitive groups is 0.1618705035971223\n",
            "FPR of the  5 th sensitive groups is 0.0962962962962963\n",
            "FPR of the  6 th sensitive groups is 0.01\n",
            "=====================\n",
            "Acceptance Rate of the  0 th sensitive groups is 0.33763977852567584\n",
            "Acceptance Rate of the  1 th sensitive groups is 0.1655188246097337\n",
            "Acceptance Rate of the  2 th sensitive groups is 0.2979962322315465\n",
            "Acceptance Rate of the  3 th sensitive groups is 0.14590163934426228\n",
            "Acceptance Rate of the  4 th sensitive groups is 0.349009900990099\n",
            "Acceptance Rate of the  5 th sensitive groups is 0.16\n",
            "Acceptance Rate of the  6 th sensitive groups is 0.06956521739130435\n",
            "The Difference of Equalized odds of the classifier is= 0.48007563180225044\n",
            "Accuracy of the classifier 0.8439596078720425\n",
            "sensitive attribute  1\n",
            "precision 0.6807073954983923\n",
            "recall 0.7571530758226037\n",
            "FPR 0.1547934528448948\n",
            "TP,FP,TN,FN\n",
            "2117 993 5422 679\n",
            "sensitive attribute  2\n",
            "precision 0.5159500693481276\n",
            "recall 0.7948717948717948\n",
            "FPR 0.08976337448559671\n",
            "TP,FP,TN,FN\n",
            "372 349 3539 96\n",
            "sensitive attribute  3\n",
            "precision 0.6508620689655172\n",
            "recall 0.7688391038696538\n",
            "FPR 0.13914338066880438\n",
            "TP,FP,TN,FN\n",
            "2265 1215 7517 681\n",
            "sensitive attribute  4\n",
            "precision 0.6179775280898876\n",
            "recall 0.6790123456790124\n",
            "FPR 0.06427221172022685\n",
            "TP,FP,TN,FN\n",
            "110 68 990 52\n",
            "sensitive attribute  5\n",
            "precision 0.6808510638297872\n",
            "recall 0.7619047619047619\n",
            "FPR 0.1618705035971223\n",
            "TP,FP,TN,FN\n",
            "96 45 233 30\n",
            "sensitive attribute  6\n",
            "precision 0.4583333333333333\n",
            "recall 0.7333333333333333\n",
            "FPR 0.0962962962962963\n",
            "TP,FP,TN,FN\n",
            "11 13 122 4\n",
            "sensitive attribute  7\n",
            "precision 0.875\n",
            "recall 0.4666666666666667\n",
            "FPR 0.01\n",
            "TP,FP,TN,FN\n",
            "7 1 99 8\n",
            "acceptance rates\n",
            "[0.33763977852567584, 0.1655188246097337, 0.2979962322315465, 0.14590163934426228, 0.349009900990099, 0.16, 0.06956521739130435]\n",
            "DP\n",
            "0.27944468359879465\n",
            "dimension of data\n",
            "7 13567\n",
            "[9211 4356]\n",
            "Optimal\n",
            "objective is:\n",
            "7571807.0\n",
            "discripency is:\n",
            "None\n",
            "gamma-epsilon-delta [0.175442, 0.142103, 0.166039, 0.164754, 0.153465, 0.14, 0.104348] 0.01 1\n",
            "<--------------------------------------->\n",
            "TPR of the  0 th sensitive groups is 0.7564377682403434\n",
            "TPR of the  1 th sensitive groups is 0.7948717948717948\n",
            "TPR of the  2 th sensitive groups is 0.7681602172437203\n",
            "TPR of the  3 th sensitive groups is 0.6790123456790124\n",
            "TPR of the  4 th sensitive groups is 0.7619047619047619\n",
            "TPR of the  5 th sensitive groups is 0.7333333333333333\n",
            "TPR of the  6 th sensitive groups is 0.4666666666666667\n",
            "=====================\n",
            "FPR of the  0 th sensitive groups is 0.1544816835541699\n",
            "FPR of the  1 th sensitive groups is 0.08924897119341564\n",
            "FPR of the  2 th sensitive groups is 0.13868529546495648\n",
            "FPR of the  3 th sensitive groups is 0.06427221172022685\n",
            "FPR of the  4 th sensitive groups is 0.1618705035971223\n",
            "FPR of the  5 th sensitive groups is 0.0962962962962963\n",
            "FPR of the  6 th sensitive groups is 0.01\n",
            "=====================\n",
            "Acceptance Rate of the  0 th sensitive groups is 0.3372055151449354\n",
            "Acceptance Rate of the  1 th sensitive groups is 0.1650596877869605\n",
            "Acceptance Rate of the  2 th sensitive groups is 0.29748244562425075\n",
            "Acceptance Rate of the  3 th sensitive groups is 0.14590163934426228\n",
            "Acceptance Rate of the  4 th sensitive groups is 0.349009900990099\n",
            "Acceptance Rate of the  5 th sensitive groups is 0.16\n",
            "Acceptance Rate of the  6 th sensitive groups is 0.06956521739130435\n",
            "The Difference of Equalized odds of the classifier is= 0.48007563180225044\n",
            "Accuracy of the classifier 0.8441070243974349\n",
            "sensitive attribute  1\n",
            "precision 0.6809401159047006\n",
            "recall 0.7564377682403434\n",
            "FPR 0.1544816835541699\n",
            "TP,FP,TN,FN\n",
            "2115 991 5424 681\n",
            "sensitive attribute  2\n",
            "precision 0.5173852573018081\n",
            "recall 0.7948717948717948\n",
            "FPR 0.08924897119341564\n",
            "TP,FP,TN,FN\n",
            "372 347 3541 96\n",
            "sensitive attribute  3\n",
            "precision 0.6514104778353483\n",
            "recall 0.7681602172437203\n",
            "FPR 0.13868529546495648\n",
            "TP,FP,TN,FN\n",
            "2263 1211 7521 683\n",
            "sensitive attribute  4\n",
            "precision 0.6179775280898876\n",
            "recall 0.6790123456790124\n",
            "FPR 0.06427221172022685\n",
            "TP,FP,TN,FN\n",
            "110 68 990 52\n",
            "sensitive attribute  5\n",
            "precision 0.6808510638297872\n",
            "recall 0.7619047619047619\n",
            "FPR 0.1618705035971223\n",
            "TP,FP,TN,FN\n",
            "96 45 233 30\n",
            "sensitive attribute  6\n",
            "precision 0.4583333333333333\n",
            "recall 0.7333333333333333\n",
            "FPR 0.0962962962962963\n",
            "TP,FP,TN,FN\n",
            "11 13 122 4\n",
            "sensitive attribute  7\n",
            "precision 0.875\n",
            "recall 0.4666666666666667\n",
            "FPR 0.01\n",
            "TP,FP,TN,FN\n",
            "7 1 99 8\n",
            "acceptance rates\n",
            "[0.3372055151449354, 0.1650596877869605, 0.29748244562425075, 0.14590163934426228, 0.349009900990099, 0.16, 0.06956521739130435]\n",
            "DP\n",
            "0.27944468359879465\n",
            "dimension of data\n",
            "7 13567\n",
            "[9211 4356]\n",
            "Optimal\n",
            "objective is:\n",
            "7529512.0\n",
            "discripency is:\n",
            "None\n",
            "gamma-epsilon-delta [0.175442, 0.142103, 0.166039, 0.164754, 0.153465, 0.14, 0.104348] 0.01 1\n",
            "<--------------------------------------->\n",
            "TPR of the  0 th sensitive groups is 0.7560801144492132\n",
            "TPR of the  1 th sensitive groups is 0.7948717948717948\n",
            "TPR of the  2 th sensitive groups is 0.7678207739307535\n",
            "TPR of the  3 th sensitive groups is 0.6790123456790124\n",
            "TPR of the  4 th sensitive groups is 0.7619047619047619\n",
            "TPR of the  5 th sensitive groups is 0.7333333333333333\n",
            "TPR of the  6 th sensitive groups is 0.4666666666666667\n",
            "=====================\n",
            "FPR of the  0 th sensitive groups is 0.15370226032735776\n",
            "FPR of the  1 th sensitive groups is 0.08822016460905349\n",
            "FPR of the  2 th sensitive groups is 0.13788364635822262\n",
            "FPR of the  3 th sensitive groups is 0.06427221172022685\n",
            "FPR of the  4 th sensitive groups is 0.15827338129496402\n",
            "FPR of the  5 th sensitive groups is 0.08888888888888889\n",
            "FPR of the  6 th sensitive groups is 0.01\n",
            "=====================\n",
            "Acceptance Rate of the  0 th sensitive groups is 0.3365541200738248\n",
            "Acceptance Rate of the  1 th sensitive groups is 0.16414141414141414\n",
            "Acceptance Rate of the  2 th sensitive groups is 0.29679739681452305\n",
            "Acceptance Rate of the  3 th sensitive groups is 0.14590163934426228\n",
            "Acceptance Rate of the  4 th sensitive groups is 0.3465346534653465\n",
            "Acceptance Rate of the  5 th sensitive groups is 0.15333333333333332\n",
            "Acceptance Rate of the  6 th sensitive groups is 0.06956521739130435\n",
            "The Difference of Equalized odds of the classifier is= 0.4764785095000922\n",
            "Accuracy of the classifier 0.844696690499005\n",
            "sensitive attribute  1\n",
            "precision 0.6819354838709677\n",
            "recall 0.7560801144492132\n",
            "FPR 0.15370226032735776\n",
            "TP,FP,TN,FN\n",
            "2114 986 5429 682\n",
            "sensitive attribute  2\n",
            "precision 0.5202797202797202\n",
            "recall 0.7948717948717948\n",
            "FPR 0.08822016460905349\n",
            "TP,FP,TN,FN\n",
            "372 343 3545 96\n",
            "sensitive attribute  3\n",
            "precision 0.6526255049047894\n",
            "recall 0.7678207739307535\n",
            "FPR 0.13788364635822262\n",
            "TP,FP,TN,FN\n",
            "2262 1204 7528 684\n",
            "sensitive attribute  4\n",
            "precision 0.6179775280898876\n",
            "recall 0.6790123456790124\n",
            "FPR 0.06427221172022685\n",
            "TP,FP,TN,FN\n",
            "110 68 990 52\n",
            "sensitive attribute  5\n",
            "precision 0.6857142857142857\n",
            "recall 0.7619047619047619\n",
            "FPR 0.15827338129496402\n",
            "TP,FP,TN,FN\n",
            "96 44 234 30\n",
            "sensitive attribute  6\n",
            "precision 0.4782608695652174\n",
            "recall 0.7333333333333333\n",
            "FPR 0.08888888888888889\n",
            "TP,FP,TN,FN\n",
            "11 12 123 4\n",
            "sensitive attribute  7\n",
            "precision 0.875\n",
            "recall 0.4666666666666667\n",
            "FPR 0.01\n",
            "TP,FP,TN,FN\n",
            "7 1 99 8\n",
            "acceptance rates\n",
            "[0.3365541200738248, 0.16414141414141414, 0.29679739681452305, 0.14590163934426228, 0.3465346534653465, 0.15333333333333332, 0.06956521739130435]\n",
            "DP\n",
            "0.2769694360740422\n",
            "dimension of data\n",
            "7 13567\n",
            "[9211 4356]\n",
            "Optimal\n",
            "objective is:\n",
            "7335341.0\n",
            "discripency is:\n",
            "None\n",
            "gamma-epsilon-delta [0.175442, 0.142103, 0.166039, 0.164754, 0.153465, 0.14, 0.104348] 0.01 1\n",
            "<--------------------------------------->\n",
            "TPR of the  0 th sensitive groups is 0.5701001430615165\n",
            "TPR of the  1 th sensitive groups is 0.6901709401709402\n",
            "TPR of the  2 th sensitive groups is 0.5953835709436525\n",
            "TPR of the  3 th sensitive groups is 0.5617283950617284\n",
            "TPR of the  4 th sensitive groups is 0.4603174603174603\n",
            "TPR of the  5 th sensitive groups is 0.4\n",
            "TPR of the  6 th sensitive groups is 0.5333333333333333\n",
            "=====================\n",
            "FPR of the  0 th sensitive groups is 0.11660171473109898\n",
            "FPR of the  1 th sensitive groups is 0.12474279835390946\n",
            "FPR of the  2 th sensitive groups is 0.1262024736601008\n",
            "FPR of the  3 th sensitive groups is 0.07088846880907372\n",
            "FPR of the  4 th sensitive groups is 0.1223021582733813\n",
            "FPR of the  5 th sensitive groups is 0.1259259259259259\n",
            "FPR of the  6 th sensitive groups is 0.05\n",
            "=====================\n",
            "Acceptance Rate of the  0 th sensitive groups is 0.2542612094235154\n",
            "Acceptance Rate of the  1 th sensitive groups is 0.1854912764003673\n",
            "Acceptance Rate of the  2 th sensitive groups is 0.24456242507278644\n",
            "Acceptance Rate of the  3 th sensitive groups is 0.1360655737704918\n",
            "Acceptance Rate of the  4 th sensitive groups is 0.22772277227722773\n",
            "Acceptance Rate of the  5 th sensitive groups is 0.15333333333333332\n",
            "Acceptance Rate of the  6 th sensitive groups is 0.11304347826086956\n",
            "The Difference of Equalized odds of the classifier is= 0.36637341383104094\n",
            "Accuracy of the classifier 0.8098326822436795\n",
            "sensitive attribute  1\n",
            "precision 0.6806148590947908\n",
            "recall 0.5701001430615165\n",
            "FPR 0.11660171473109898\n",
            "TP,FP,TN,FN\n",
            "1594 748 5667 1202\n",
            "sensitive attribute  2\n",
            "precision 0.3997524752475248\n",
            "recall 0.6901709401709402\n",
            "FPR 0.12474279835390946\n",
            "TP,FP,TN,FN\n",
            "323 485 3403 145\n",
            "sensitive attribute  3\n",
            "precision 0.6141456582633054\n",
            "recall 0.5953835709436525\n",
            "FPR 0.1262024736601008\n",
            "TP,FP,TN,FN\n",
            "1754 1102 7630 1192\n",
            "sensitive attribute  4\n",
            "precision 0.5481927710843374\n",
            "recall 0.5617283950617284\n",
            "FPR 0.07088846880907372\n",
            "TP,FP,TN,FN\n",
            "91 75 983 71\n",
            "sensitive attribute  5\n",
            "precision 0.6304347826086957\n",
            "recall 0.4603174603174603\n",
            "FPR 0.1223021582733813\n",
            "TP,FP,TN,FN\n",
            "58 34 244 68\n",
            "sensitive attribute  6\n",
            "precision 0.2608695652173913\n",
            "recall 0.4\n",
            "FPR 0.1259259259259259\n",
            "TP,FP,TN,FN\n",
            "6 17 118 9\n",
            "sensitive attribute  7\n",
            "precision 0.6153846153846154\n",
            "recall 0.5333333333333333\n",
            "FPR 0.05\n",
            "TP,FP,TN,FN\n",
            "8 5 95 7\n",
            "acceptance rates\n",
            "[0.2542612094235154, 0.1854912764003673, 0.24456242507278644, 0.1360655737704918, 0.22772277227722773, 0.15333333333333332, 0.11304347826086956]\n",
            "DP\n",
            "0.1412177311626458\n",
            "precision 0.6085714285714285\n",
            "recall 0.5873161764705882\n",
            "TP,FP,TN,FN\n",
            "1917 1233 9070 1347\n",
            "<--------------------------------------->\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vB6s_2z5oWBT",
        "outputId": "af1dc371-41fb-42a5-f70f-5ebfed6ad934"
      },
      "source": [
        "pip install pulp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pulp\n",
            "  Downloading PuLP-2.6.0-py3-none-any.whl (14.2 MB)\n",
            "\u001b[K     || 14.2 MB 5.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: pulp\n",
            "Successfully installed pulp-2.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUAbOcIufe0d"
      },
      "source": [
        "# bilal - am_ind asian black other white female male(actual precision sequence)\n",
        "# 6         5          4          2                1              0                  3\n",
        "\n",
        "# ours -s_male, s_female  r_white, r_black, r_asian-pac-islander','r_amer-indian-eskimo','r_other\n",
        "           \n",
        "# beta=[6         5          4          2                1              0                  3]\n",
        "\n",
        "# beta=[beta[6], beta[5],beta[4],beta[2],beta[1],beta[0],beta[3]]\n",
        "\n",
        "\n",
        "#bilal -female male  am_ind asian  black other white (actual acceptance rate sequence)\n",
        "#          0      1     2       3     4      5    6   \n",
        "# ours -s_male, s_female  r_white, r_black, r_asian-pac-islander','r_amer-indian-eskimo', 'r_other\n",
        "           \n",
        "# beta=[1       0        6          4            3                   2                    5]\n",
        "\n",
        "\n",
        "#NG\n",
        "import time\n",
        "#import gurobipy as gp\n",
        "#from gurobipy import GRB\n",
        "import pulp as p \n",
        "def lp_equalized_odds(data1,eps,y_test_pred,e,beta_avg,alpha):\n",
        "    import pulp as p \n",
        "    import math\n",
        "    \n",
        "    \n",
        "    m=data1.shape[0]\n",
        "    n=data1.shape[1]\n",
        "    print('dimension of data')\n",
        "    print(m,n)\n",
        "    \n",
        "    \n",
        "\n",
        "    ############### #  SORTED for ACCURACY ONLY ####\n",
        "    m=7\n",
        "    h1=[]\n",
        "    key1=[]\n",
        "    cost=np.zeros(n,dtype=int)\n",
        "    data2=np.zeros((m,n),dtype=int)\n",
        "    \n",
        "    for i in range(n):\n",
        "            h1.append(e[i][1])\n",
        "            key1.append(i)\n",
        "\n",
        "        \n",
        "#print(hc)\n",
        "#     print(key1)\n",
        "    \n",
        "    for i in range(1,len(h1)):\n",
        "        for j in range(i,0,-1):\n",
        "            var=0\n",
        "            var2=0\n",
        "            if h1[j-1]<h1[j]:\n",
        "                index=j\n",
        "                var=h1[j]\n",
        "                h1[j]=h1[j-1]\n",
        "                h1[j-1]=var\n",
        "\n",
        "                var2=key1[j]\n",
        "                key1[j]=key1[j-1]\n",
        "                key1[j-1]=var2\n",
        "            else:\n",
        "                break\n",
        "    \n",
        "\n",
        "    \n",
        "    \n",
        "    for j in range(len(key1)):    \n",
        "         data2[0][key1[j]]=j+1\n",
        "    \n",
        "    for j in range(n):\n",
        "        summ=0\n",
        "        summ=summ+data2[0][j] \n",
        "        cost[j]=summ\n",
        "\n",
        "    Lp_prob = p.LpProblem('Problem', p.LpMinimize)  \n",
        "    solver = p.getSolver('PULP_CBC_CMD', timeLimit=20)\n",
        "   \n",
        "    \n",
        "#     X=np.zeros(n+1,dtype=p.LpVariable)\n",
        "    X=np.zeros(n+m+1,dtype=p.LpVariable)\n",
        "    Y=np.zeros(m,dtype=p.LpVariable)\n",
        "    \n",
        "    sizes=np.zeros(m,dtype=int)\n",
        "#     report_index(index,data1,e):  \n",
        "    max_size=0\n",
        "    for i in range(m):\n",
        "        count=0\n",
        "        for j in range(n):\n",
        "            if data1[i][j]==1:\n",
        "                count=count+1 \n",
        "        if count>max_size:\n",
        "            max_size=count\n",
        "        sizes[i]=count\n",
        "    print(sizes)    \n",
        "    #############################33\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    ###############################\n",
        "    beta_actual = [0.30355010313755293, 0.10743801652892562, 0.252269224182223, 0.13278688524590163, 0.3118811881188119, 0.1, 0.13043478260869565]\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "    select_sizes=np.zeros(m,dtype=int)\n",
        "   \n",
        "    size_final=np.zeros(m,dtype=int)\n",
        "\n",
        "    for i in range(m):\n",
        "        var1 = str(n+100+i)\n",
        "        Y[i]=p.LpVariable(var1,lowBound=0,upBound=1,cat='Continuous')\n",
        "    \n",
        "    for i in range(n):\n",
        "        var1=str(i)       \n",
        "        X[i]=p.LpVariable(var1,lowBound=0,upBound=1,cat='Integer')\n",
        "   \n",
        "    X[n]=p.LpVariable(str(n),lowBound=0,upBound=1,cat='Continuous')  \n",
        "\n",
        "    tpr = p.LpVariable(str(n+200),lowBound=0,upBound=1,cat='Continuous')  \n",
        "    fpr = p.LpVariable(str(n+201),lowBound=0,upBound=1,cat='Continuous')  \n",
        "\n",
        "#     for i in range(m):\n",
        "#         k=n+i+1\n",
        "#         var1=str(k)     \n",
        "#         alpha=(((sizes[i])*(sizes[i]+1))/2)\n",
        "#         X[i]=p.LpVariable(var1,lowBound=(((beta*sizes[i])*(beta*sizes[i]+1))/2),upBound=alpha,cat='Continuous')\n",
        "    \n",
        "        \n",
        "#     X[n]=  p.LpVariable(\"z1\",lowBound=0)\n",
        "    #X[n+1]=  p.LpVariable(\"z2\",lowBound=0)\n",
        "  \n",
        "\n",
        "    #########objective function#####################\n",
        "    \n",
        "#     Lp_prob += 2*X[n+1]+10*X[n+2]+9*X[n+3]+3*X[n+4]\n",
        "    #alpha=0.8\n",
        "    #beta_avg = 0.10\n",
        "    Lp_prob+= p.lpSum([(X[j])*cost[j] for j in range(n)]) \n",
        "    #Lp_prob+=1  \n",
        "    \n",
        "    #Lp_prob += Y[0]*sizes[0] + Y[1]*sizes[1] >= p.lpSum([Y[j]*sizes[j] for j in np.arange(2,6)])\n",
        "    #Lp_prob += Y[0]*sizes[0] + Y[1]*sizes[1] <= p.lpSum([Y[j]*sizes[j] for j in np.arange(2,6)])\n",
        "    \n",
        "    ##############constraint#################\n",
        "    #first select the  the number of make female in test data\n",
        "    #then apply the equalized odd constraints assuming \n",
        "    #look at all males which have been predicted positve/and all the females predicted negative\n",
        "    F_test = 0\n",
        "    M_test = 0\n",
        "        \n",
        "    #for i in range(len(y_test)):\n",
        "    #    if(data1[0][i]==1 and y_test.iloc[i]==1):\n",
        "    #        M_test= M_test+1\n",
        "    #    elif(data1[1][i]==1 and y_test.iloc[i]==1):\n",
        "    #        F_test= F_test+1\n",
        "    test_count = np.zeros(m, dtype = int)\n",
        "\n",
        "    for i in range(len(y_test)):\n",
        "      for j in range(m): \n",
        "        if(data1[j][i]==1 and y_test_pred[i]==1):\n",
        "            test_count[j] = test_count[j] +1\n",
        "                \n",
        "    \n",
        "    #Lp_prob += (p.lpSum([(X[j])*(data1[0][j])*y_test_pred[j] for j in range(n) if y_test_pred[j]==1])/M_test) <= (p.lpSum([(X[j])*(data1[1][j])*y_test_pred[j] for j in range(n) if y_test_pred[j]==1])/F_test) + 0.0009\n",
        "    #Lp_prob += (p.lpSum([(X[j])*(data1[0][j])*(1-y_test_pred[j]) for j in range(n) if y_test_pred[j]==0])/(sizes[0]-M_test)) <= (p.lpSum([(X[j])*(data1[1][j])*(1-y_test_pred[j]) for j in range(n) if y_test_pred[j]==0])/(sizes[1]-F_test))+ 0.0009\n",
        "    for i in range(m):   #TPR constraints\n",
        "      Lp_prob += (1/test_count[i])*p.lpSum([(X[j])*(data1[i][j])*y_test_pred[j] for j in range(n) if (y_test_pred[j]==1) ]) >= tpr \n",
        "      Lp_prob += (1/test_count[i])*p.lpSum([(X[j])*(data1[i][j])*y_test_pred[j] for j in range(n) if (y_test_pred[j]==1 )]) <= tpr + 0.01\n",
        "    for i in range(m):    #FPR constraints\n",
        "      Lp_prob += (1/(sizes[i]-test_count[i]))*p.lpSum([(X[j])*(data1[i][j])*(1-y_test_pred[j]) for j in range(n) if (y_test_pred[j]==0)]) >= fpr\n",
        "      Lp_prob += (1/(sizes[i]-test_count[i]))*p.lpSum([(X[j])*(data1[i][j])*(1-y_test_pred[j]) for j in range(n) if (y_test_pred[j]==0)]) <= fpr + 0.01\n",
        "\n",
        "    #Lp_prob += F_test*p.lpSum([(X[j])*(data1[0][j])*y_test_pred[j] for j in range(n) if (y_test_pred[j]==1) ]) <= M_test*p.lpSum([(X[j])*(data1[1][j])*y_test_pred[j] for j in range(n) if (y_test_pred[j]==1 )]) + 0.004\n",
        "    #Lp_prob += (sizes[1]-F_test)*p.lpSum([(X[j])*(data1[0][j])*(1-y_test_pred[j]) for j in range(n) if (y_test_pred[j]==0)]) <= (sizes[0]-M_test)*p.lpSum([(X[j])*(data1[1][j])*(1-y_test_pred[j]) for j in range(n) if (y_test_pred[j]==0 )]) + 0.004\n",
        "    \n",
        "\n",
        "\n",
        "    #Lp_prob += F_test*p.lpSum([(X[j])*(data1[0][j])*y_test_pred[j] for j in range(n) if (y_test_pred[j]==1 and y_test.iloc[j]==1) ]) <= M_test*p.lpSum([(X[j])*(data1[1][j])*y_test_pred[j] for j in range(n) if (y_test_pred[j]==1 and y_test.iloc[j]==1)]) + 0.01\n",
        "    #Lp_prob += (sizes[1]-F_test)*p.lpSum([(X[j])*(data1[0][j])*(1-y_test_pred[j]) for j in range(n) if (y_test_pred[j]==0 and y_test.iloc[j]==0)]) <= (sizes[0]-M_test)*p.lpSum([(X[j])*(data1[1][j])*(1-y_test_pred[j]) for j in range(n) if (y_test_pred[j]==0 and y_test.iloc[j]==0)]) + 0.01\n",
        "    \n",
        "    #Lp_prob += p.lpSum([(X[j])*(data1[0][j])*y_test.iloc[j] for j in range(n) if y_test.iloc[j]==1])/M_test <= p.lpSum([(X[j])*(data1[1][j])*y_test.iloc[j] for j in range(n) if y_test.iloc[j]==1])/F_test + 0.004\n",
        "    #Lp_prob += (sizes[1]-F_test)*p.lpSum([(X[j])*(data1[0][j])*(1-y_test.iloc[j]) for j in range(n) if y_test.iloc[j]==0])/M_test <= (sizes[0]-M_test)*p.lpSum([(X[j])*(data1[1][j])*(1-y_test.iloc[j]) for j in range(n) if y_test.iloc[j]==0])/F_test + 0.004\n",
        "    \n",
        "    for i in range(m):\n",
        "      #  if i<m:\n",
        "            Lp_prob += p.lpSum([(X[j])*(data1[i][j]) for j in range(n)]) >= Y[i]*sizes[i]\n",
        "            Lp_prob += p.lpSum([(X[j])*(data1[i][j]) for j in range(n)]) <= (Y[i]+0.07)*sizes[i]\n",
        "    \n",
        "    for i in range(m):\n",
        "        if beta_actual[i] >= beta_avg:\n",
        "            Lp_prob += Y[i] >= (1-alpha)*beta_actual[i] + alpha*beta_avg\n",
        "            Lp_prob += Y[i] <= beta_actual[i]\n",
        "        else:\n",
        "            Lp_prob += Y[i] >= (1-alpha)*beta_actual[i] + alpha*beta_avg\n",
        "            Lp_prob += Y[i] <= beta_avg \n",
        "    \n",
        "           \n",
        "    #Lp_prob+= p.lpSum([(X[j])*cost[j] for j in range(n)])>=100\n",
        "        \n",
        "    #####################################\n",
        "    #solver = p.CPLEX_PY()\n",
        "    #solver.buildSolverModel(Lp_prob)\n",
        "    #Lp_prob.solverModel.parameters.timelimit.set(60)\n",
        "    #solver.callSolver(P)\n",
        "    #status = solver.findSolutionValues(Lp_prob)\n",
        "    #################################################################\n",
        "    status = Lp_prob.solve(solver)   # Solver \n",
        "    print(p.LpStatus[status]) \n",
        "    print(\"objective is:\")        \n",
        "    print(p.value(Lp_prob.objective))\n",
        "    print(\"discripency is:\") \n",
        "    print(p.value(X[n]))\n",
        "    x=np.zeros(n,dtype=float)\n",
        "\n",
        "   # The solution status \n",
        "    Synth1={}\n",
        "    Synth2={}\n",
        "    # # Printing the final solution \n",
        "    for i in range(n):\n",
        "        if(p.value(X[i])==1):\n",
        "            Synth1[i]=1 \n",
        "            Synth2[i]=-1\n",
        "#             if(data1[2][i]==1):\n",
        "#                 print(\"no\")\n",
        "        else:\n",
        "            Synth1[i]=-1\n",
        "            Synth2[i]=1\n",
        "    Synthu1=Synth1  \n",
        "    Synthu2=Synth2  \n",
        "    \n",
        "              \n",
        "    return Synthu1,Synthu2   \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1Twwdewfe0y",
        "outputId": "c4a137f9-0dbd-4ed2-93b7-eb8d54146c99"
      },
      "source": [
        "pip install pulp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pulp\n",
            "  Downloading PuLP-2.5.1-py3-none-any.whl (41.2 MB)\n",
            "\u001b[K     || 41.2 MB 76 kB/s \n",
            "\u001b[?25hInstalling collected packages: pulp\n",
            "Successfully installed pulp-2.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spvW20mEfe2L"
      },
      "source": [
        "#without accuracy ---> 2\n",
        "def main2(datax, y_test, y_test_pred,e): \n",
        "        \n",
        "    n=datax.shape[1]\n",
        "    s=datax.shape[0]    \n",
        "    data = np.zeros((s, n), dtype = int)\n",
        "    \n",
        "    r = np.zeros(n, dtype = int) \n",
        "    \n",
        "    for i in range(n):\n",
        "        if int(y_test.iloc[i])==1 :\n",
        "            r[i]=1\n",
        "        else :\n",
        "            r[i]= -1  \n",
        "    \n",
        "    r2 = np.zeros(n, dtype = int) \n",
        "    for i in range(n):\n",
        "        if int(y_test_pred[i])==1 :\n",
        "            r2[i]=1\n",
        "        else :\n",
        "            r2[i]= -1          \n",
        "    ar=[]\n",
        "    \n",
        "    for j in range(s):\n",
        "        print(\"sensitive attribute \",(j+1)) \n",
        "        a=0\n",
        "        b=0\n",
        "        acc1=0\n",
        "        acc2=0\n",
        "        for i in range(n):\n",
        "                data[j][i]= datax.iloc[j,i]\n",
        "                if data[j][i]== 1 :\n",
        "                    a=a+1\n",
        "                    if r[i]==1:\n",
        "                         acc1=acc1+1 \n",
        "\n",
        "        print(\"ACTUAL----------total ,accepted, aceeptance rate:\")             \n",
        "        a1=float(acc1/a)\n",
        "        print(a)\n",
        "        \n",
        "        print(acc1)\n",
        "        print(a1)\n",
        "        ar.append(a1)\n",
        "        \n",
        "    maxi= max(ar)\n",
        "    mini= min(ar)\n",
        "    DP=float(maxi-mini)\n",
        "    print(\"data acceptance rates\")\n",
        "    print(ar)\n",
        "    print(\"data DP\")\n",
        "    print(DP)\n",
        "    \n",
        "    ar=[]\n",
        "    \n",
        "    for j in range(s):\n",
        "        print(\"sensitive attribute \",(j+1)) \n",
        "        a=0\n",
        "        b=0\n",
        "        acc1=0\n",
        "        acc2=0\n",
        "        prec=0\n",
        "        reca=0\n",
        "        accur=0\n",
        "        FP=0\n",
        "        FN=0\n",
        "        TP=0\n",
        "        TN=0\n",
        "        for i in range(n):\n",
        "             if data[j][i]== 1 :\n",
        "                    a=a+1\n",
        "                    if r2[i]==1:\n",
        "                        acc1=acc1+1 \n",
        "                        if r[i]==1:\n",
        "                            TP=TP+1\n",
        "                        else:\n",
        "                             FP=FP+1                \n",
        "                    else:\n",
        "                        if r[i]==1:\n",
        "                            FN=FN+1\n",
        "                        else:\n",
        "                            TN=TN+1    \n",
        "        \n",
        "        print(\"prec reca accuracy for each sens\") \n",
        "        prec= float(TP/(TP+FP))\n",
        "        reca= float(TP/(TP+FN))\n",
        "        accur= float((TP+TN)/a)\n",
        "        print(prec,reca,accur)\n",
        "        \n",
        "        print(\"SVM----------total , accepted, aceeptance rate:\")             \n",
        "        \n",
        "        a1=float(acc1/a)\n",
        "        print(a)\n",
        "        \n",
        "        print(acc1)\n",
        "        print(a1)\n",
        "        ar.append(a1)\n",
        "        \n",
        "    maxi= max(ar)\n",
        "    mini= min(ar)\n",
        "    DP=float(maxi-mini)\n",
        "    print(\"data acceptance rates\")\n",
        "    print(ar)\n",
        "    print(\"data DP\")\n",
        "    print(DP) \n",
        "    \n",
        "    print(\"SVM accuracy--------------------------\")\n",
        "    prec=0\n",
        "    reca=0\n",
        "    accur=0\n",
        "    FP=0\n",
        "    FN=0\n",
        "    TP=0\n",
        "    TN=0\n",
        "    for i in range(n):\n",
        "            if r2[i]==1:\n",
        "                acc1=acc1+1 \n",
        "                if r[i]==1:\n",
        "                    TP=TP+1\n",
        "                else:\n",
        "                     FP=FP+1                \n",
        "            else:\n",
        "                if r[i]==1:\n",
        "                     FN=FN+1\n",
        "                else:\n",
        "                     TN=TN+1    \n",
        "\n",
        "        \n",
        "    prec= float(TP/(TP+FP))\n",
        "    reca= float(TP/(TP+FN))\n",
        "    accur= float((TP+TN)/n)\n",
        "    print(prec,reca,accur)\n",
        "    \n",
        "    \n",
        "#     delta1=[.70,.75,.80,.85,.90,.95]\n",
        "    #gamma=.05,.06,.07\n",
        "    #delta1=[.80,.85,.90,.95]\n",
        "# (for reproducibility)  \n",
        "\n",
        "# delta1=[.8], gama=[.1], epsilon=[.05]  \n",
        "# delta1=[.8], gama=[.15], epsilon=[.01]\n",
        " \n",
        "#     delta1=np.arange(1,.79,-.01)\n",
        "    delta=1\n",
        "#     gama=[.05,.1,.15,.2,.25]\n",
        "#     epsilon=[.01,.02,.05,.1,.15,.20,.25,.30,.35,.40,.50]\n",
        "\n",
        "#ADULT ZAFAR =? epsilon=[0.088 ,0.1656, 0.168,  0.211, 0.251 ] \n",
        " \n",
        "#agarwal=> epsilon=[ 0.071, 0.1271, 0.2437, 0.27 ]\n",
        " \n",
        "\n",
        "    #gama=[0.0869, 0.0521,0.0782, 0.0608,0.0434, 0.1,0.069,0.0434,0.034]\n",
        "    epsilon=[.01]\n",
        "    beta_converge = [0.15,0.20]\n",
        "    alpha = [0,0.06,0.062,0.065,0.07,0.8]\n",
        "    \n",
        "    zero_one = np.zeros(n, dtype = int) \n",
        "    \n",
        "    fi= np.zeros(n,dtype=int) \n",
        "#     for delta in delta1:\n",
        "    #4 gamma=[0.175442,    0.142103, 0.166039,    0.164754,  0.153465,    0.14,  0.104348   ]\n",
        "    #lp_equalized_odds_no_beta(data1,eps,y_test,e,beta_avg,alpha)\n",
        "    #1 gamma=[0.259147,   0.0730028, 0.210139, 0.0893443, 0.306931, 0.0933333,  0.0347826]\n",
        "    #gamma=[0.196178,0.126722,   0.179654, 0.140164,     0.153465,   0.133333,  0.0695652]\n",
        "\n",
        "  \n",
        "    gamma = [0.175442,    0.142103, 0.166039,    0.164754,  0.153465,    0.14,  0.104348 ]\n",
        "    for eps in epsilon:\n",
        "        for beta_avg in beta_converge:\n",
        "            print(\"----------------This is for covergence at beta = \",beta_avg, \" ----------------\")\n",
        "            for a in alpha:\n",
        "                u1,u2=lp_equalized_odds(data,eps,y_test_pred,e,beta_avg,a)\n",
        "                #######################Disp_impact#######################  \n",
        "                print(\"gamma-epsilon-delta\",gamma,eps,delta)\n",
        "                accu_all=[]\n",
        "                DP_all=[]\n",
        "                precision_all=[]\n",
        "                recall_all=[]\n",
        "                acceptance_rate=np.zeros((7,28),dtype=float)\n",
        "                count=0\n",
        "                print(\"<--------------------------------------->\")\n",
        "            #        print(\"iteration t\",t)\n",
        "            #                 for alpha in np.arange(0,1.05,0.05):\n",
        "            #                     print(\"alpha: \",alpha)\n",
        "            #                     for i in range(n):\n",
        "\n",
        "            #                         z=random()\n",
        "            #                         if z < alpha:\n",
        "            #                                fi[i]= u1[i] \n",
        "\n",
        "            #                         else:\n",
        "            #                                fi[i]= r2[i]\n",
        "                \n",
        "                for i in range(n):\n",
        "                    fi[i] = u1[i]\n",
        "                    if (fi[i]==1):\n",
        "                        zero_one[i] = 1\n",
        "                    else:\n",
        "                        zero_one[i] = 0\n",
        "                ar=[]\n",
        "                #find_eo_stats(y_test,zero_one)\n",
        "                find_eo_stats_multiple(y_test,zero_one)\n",
        "\n",
        "\n",
        "                for j in range(s):\n",
        "                    print(\"sensitive attribute \",(j+1)) \n",
        "\n",
        "                    TP=0\n",
        "                    FP=0\n",
        "                    FN=0\n",
        "                    TN=0\n",
        "                    precision=0\n",
        "                    recall=0\n",
        "                    for i in range(n):\n",
        "                         if data[j][i]== 1 :                        \n",
        "                            if fi[i]==1 and r[i]==1:\n",
        "                                TP=TP+1\n",
        "                            if fi[i]==1 and r[i]==-1:\n",
        "                                FP=FP+1 \n",
        "                            if fi[i]==-1 and r[i]==1:\n",
        "                                FN=FN+1\n",
        "                            if fi[i]==-1 and r[i]==-1:\n",
        "                                TN=TN+1    \n",
        "                    if TP+FP !=0:\n",
        "                        precision=float(TP/(TP+FP))\n",
        "                    print(\"precision\",precision)\n",
        "                    if TP+FN !=0:    \n",
        "                        recall=float(TP/(TP+FN))\n",
        "                    print(\"recall\",recall)\n",
        "                    if FP+TN !=0:\n",
        "                        fpr = float(FP/(FP+TN))\n",
        "                    print(\"FPR\", fpr)    \n",
        "                    print(\"TP,FP,TN,FN\")\n",
        "                    print(TP,FP,TN,FN)\n",
        "                    a=0\n",
        "                    b=0\n",
        "                    acc1=0\n",
        "                    acc2=0\n",
        "                    for i in range(n):\n",
        "                            if data[j][i]== 1 :\n",
        "                                a=a+1\n",
        "                                if fi[i]==1:\n",
        "                                     acc1=acc1+1 \n",
        "\n",
        "            #                         print(\"total ,fair accepted, aceeptance rate:\")             \n",
        "                    a1=float(acc1/a)\n",
        "\n",
        "\n",
        "\n",
        "            #                         print(a)\n",
        "            #                         print(acc1)\n",
        "            #                         print(a1)\n",
        "                    ar.append(a1)\n",
        "\n",
        "                count = count+1\n",
        "                maxi=max(ar)\n",
        "                mini= min(ar)\n",
        "                DP=float(maxi-mini)\n",
        "                print(\"acceptance rates\")\n",
        "                print(ar)\n",
        "                print(\"DP\")\n",
        "                print(DP)\n",
        "                f_acc=0\n",
        "                for i in range(n):\n",
        "                     if fi[i] == r[i]:\n",
        "                            f_acc=f_acc+1\n",
        "                f_acc_l=float((f_acc*100)/n) \n",
        "\n",
        "#######################################################################33   \n",
        "\n",
        "#                         print(\"sensitive attribute \",(j+1)) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        TP=0\n",
        "        FP=0\n",
        "        FN=0\n",
        "        TN=0\n",
        "        precision=0\n",
        "        recall=0\n",
        "        for i in range(n):\n",
        "                if fi[i]==1 and r[i]==1:\n",
        "                    TP=TP+1\n",
        "                if fi[i]==1 and r[i]==-1:\n",
        "                    FP=FP+1 \n",
        "                if fi[i]==-1 and r[i]==1:\n",
        "                    FN=FN+1\n",
        "                if fi[i]==-1 and r[i]==-1:\n",
        "                    TN=TN+1    \n",
        "\n",
        "        if TP+FP!=0:\n",
        "            precision=float(TP/(TP+FP))\n",
        "        print(\"precision\",precision)\n",
        "        if TP+FN!=0:\n",
        "            recall=float(TP/(TP+FN))    \n",
        "\n",
        "        print(\"recall\",recall)\n",
        "        \n",
        "        accu = float((TP + TN)/n)\n",
        "        print(\"TP,FP,TN,FN\")\n",
        "        print(TP,FP,TN,FN)\n",
        "#       print(\"total ,fair accepted, aceeptance rate:\")             \n",
        "        a1=float(acc1/a)\n",
        "\n",
        "\n",
        "    print(\"<--------------------------------------->\")\n",
        "    alpha_weight=np.arange(0,1.05,.05)        \n",
        "    return accu_all,DP_all,acceptance_rate,alpha_weight"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}