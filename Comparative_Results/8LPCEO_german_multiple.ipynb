{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "8LPCEO_german_multiple.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tIEkDw7fexa"
      },
      "source": [
        "# Import libraries necessary for this project\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "import seaborn as sns\n",
        "sns.set(style=\"darkgrid\")\n",
        "from time import time\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Import 'GridSearchCV', 'make_scorer', and any other necessary libraries\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import fbeta_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "# Import the three supervised learning models from sklearn\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA    \n",
        "\n",
        "# Pretty display for notebooks\n",
        "%matplotlib inline\n",
        "from random import shuffle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4A2tdgUfeye",
        "outputId": "34c15076-9392-42d6-8382-2b8bca07e08c"
      },
      "source": [
        "\n",
        "import time\n",
        "# import pulp as p \n",
        "# from random import *\n",
        "from sklearn import preprocessing\n",
        "# Add column names to data set\n",
        "from sklearn.model_selection import train_test_split\n",
        "data= pd.read_csv('data/german.csv' , skipinitialspace=True)\n",
        "print(data.head())\n",
        "print(data.shape[0],data.shape[1])\n",
        "\n",
        "#sensitive columns name '12'='age','8'='gender/personal_status'  '19'- foreign workers 20'=1(good)/2(bad))\n",
        "\n",
        "# print(sens)\n",
        "r=data[['20']]\n",
        "# print(r)\n",
        "p=data.shape[0]\n",
        "for i in range(0,p):  \n",
        "    if data.loc[i,\"12\"]>25 :\n",
        "               data.loc[i,\"12\"] = 1 \n",
        "    else :\n",
        "               data.loc[i,\"12\"] = 2\n",
        "    if r.loc[i,'20'] == 1 :\n",
        "               r.loc[i,\"20\"] = 1 \n",
        "    else: \n",
        "               r.loc[i,\"20\"] = 0  \n",
        "            \n",
        "print(data['20'].value_counts())            \n",
        "\n",
        "print(data['8'].value_counts())\n",
        "print(data['12'].value_counts())\n",
        "print(data['19'].value_counts())\n",
        "\n",
        "####################################################################################\n",
        "\n",
        "\n",
        "# Initialize a scaler, then apply it to the features\n",
        "'''\n",
        "scaler = MinMaxScaler() # default=(0, 1)\n",
        "num_col = dat.dtypes[dat.dtypes != 'object'].index\n",
        "features_log_minmax_transform = pd.DataFrame(data = dat)\n",
        "features_log_minmax_transform[num_col] = scaler.fit_transform(features_log_minmax_transform[num_col])\n",
        "\n",
        "\n",
        "display(features_log_minmax_transform.head())\n",
        "'''\n",
        "\n",
        "# sens=DATA[['sex','race']]\n",
        "#Data_c = pd.get_dummies(features_log_minmax_transform, columns=['sex','race','workclass','education','marital-status','occupation','relationship','native-country'], prefix =['s','r','work','edu','ms','occ','rls','nc'])\n",
        "m=data.shape[1]\n",
        "data_c1=data.iloc[:,0:m-1]\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "data_c2 = min_max_scaler.fit_transform(data_c1)\n",
        "data_c = pd.DataFrame(data_c2,columns=data_c1.columns)\n",
        "print(data_c)\n",
        "\n",
        "\n",
        "\n",
        "#X_test,Y_test_pred,Y_test,e = german_rf(data_c , r)\n",
        "#print(X_test.iloc[:,:])\n",
        "X_train, X_test, y_train, y_test = train_test_split(data_c , r, test_size = 0.3, random_state=0, shuffle=True)\n",
        "X_test.reset_index(drop=True, inplace=True)\n",
        "# Y_test_pred.reset_index()\n",
        "y_test.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# print(X_test)\n",
        "# print(Y_test_pred)\n",
        "# print(Y_test)\n",
        "sens=X_test[['8', '12' ]]\n",
        "# print(sens)\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "     \n",
        "# for i in range(0,p):  \n",
        "#     if r.loc[i,'y'] == 1 :\n",
        "#                r.loc[i,\"y\"] = 1 \n",
        "#     else: \n",
        "#                r.loc[i,\"y\"] = 0 \n",
        "print(sens['8'].value_counts())            \n",
        "print(sens['12'].value_counts())\n",
        "#print(sens['19'].value_counts())\n",
        "sens1=pd.get_dummies(sens, columns=['8','12'], prefix =['8','12'])\n",
        "sensitive=sens1.T\n",
        "print(sensitive) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   0   1  2  3     4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20\n",
            "0  0   6  4  4  1169  4  4  4  0  0   4   0  67   2   1   2   2   1   1   0   1\n",
            "1  1  48  2  4  5951  0  2  2  1  0   2   0  22   2   1   1   2   1   0   0   2\n",
            "2  3  12  4  7  2096  0  3  2  0  0   3   0  49   2   1   1   1   2   0   0   1\n",
            "3  0  42  2  3  7882  0  3  2  0  2   4   1  45   2   2   1   2   2   0   0   1\n",
            "4  0  24  3  0  4870  0  2  3  0  0   4   3  53   2   2   2   2   2   0   0   2\n",
            "1000 21\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  iloc._setitem_with_indexer(indexer, value)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1    700\n",
            "2    300\n",
            "Name: 20, dtype: int64\n",
            "0    690\n",
            "1    310\n",
            "Name: 8, dtype: int64\n",
            "1    810\n",
            "2    190\n",
            "Name: 12, dtype: int64\n",
            "0    963\n",
            "1     37\n",
            "Name: 19, dtype: int64\n",
            "            0         1     2         3  ...        16   17   18   19\n",
            "0    0.000000  0.029412  1.00  0.444444  ...  0.666667  0.0  1.0  0.0\n",
            "1    0.333333  0.647059  0.50  0.444444  ...  0.666667  0.0  0.0  0.0\n",
            "2    1.000000  0.117647  1.00  0.777778  ...  0.333333  1.0  0.0  0.0\n",
            "3    0.000000  0.558824  0.50  0.333333  ...  0.666667  1.0  0.0  0.0\n",
            "4    0.000000  0.294118  0.75  0.000000  ...  0.666667  1.0  0.0  0.0\n",
            "..        ...       ...   ...       ...  ...       ...  ...  ...  ...\n",
            "995  1.000000  0.117647  0.50  0.333333  ...  0.333333  0.0  0.0  0.0\n",
            "996  0.000000  0.382353  0.50  0.111111  ...  1.000000  0.0  1.0  0.0\n",
            "997  1.000000  0.117647  0.50  0.444444  ...  0.666667  0.0  0.0  0.0\n",
            "998  0.000000  0.602941  0.50  0.444444  ...  0.666667  0.0  1.0  0.0\n",
            "999  0.333333  0.602941  1.00  0.111111  ...  0.666667  0.0  0.0  0.0\n",
            "\n",
            "[1000 rows x 20 columns]\n",
            "0.0    196\n",
            "1.0    104\n",
            "Name: 8, dtype: int64\n",
            "0.0    241\n",
            "1.0     59\n",
            "Name: 12, dtype: int64\n",
            "        0    1    2    3    4    5    6    ...  293  294  295  296  297  298  299\n",
            "8_0.0     1    1    1    1    1    1    0  ...    1    0    0    1    1    0    0\n",
            "8_1.0     0    0    0    0    0    0    1  ...    0    1    1    0    0    1    1\n",
            "12_0.0    1    1    1    1    1    1    1  ...    1    1    0    0    1    1    0\n",
            "12_1.0    0    0    0    0    0    0    0  ...    0    0    1    1    0    0    1\n",
            "\n",
            "[4 rows x 300 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z94Un5T_fezp",
        "outputId": "1be7acb9-ada6-4aaf-dc5f-8ae625c345c0"
      },
      "source": [
        "from sklearn.model_selection import cross_val_score  \n",
        "from sklearn.svm import SVC\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=99, max_depth=None, min_samples_split=15, random_state=0)\n",
        "\n",
        "#svm = SVC(kernel='rbf', random_state=0, gamma=.1, C=10.0,probability=True)\n",
        "rf.fit(X_train, y_train)\n",
        "print('The accuracy of the Logistic_Regression classifier on training data is {:.2f}'.format(rf.score(X_train, y_train)))\n",
        "print('The accuracy of the Logistic_Regression classifier on test data is {:.2f}'.format(rf.score(X_test, y_test)))\n",
        "print('####Train prediction Label###############################################')\n",
        "y_train_pred=rf.predict(X_train)\n",
        "#print(y_1)\n",
        "y_test_pred=rf.predict(X_test)\n",
        "e=rf.predict_proba(X_test)\n",
        "print(e)\n",
        "print(y_test_pred)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  import sys\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuracy of the Logistic_Regression classifier on training data is 0.90\n",
            "The accuracy of the Logistic_Regression classifier on test data is 0.76\n",
            "####Train prediction Label###############################################\n",
            "[[0.58473416 0.41526584]\n",
            " [0.10940278 0.89059722]\n",
            " [0.07387113 0.92612887]\n",
            " [0.24397644 0.75602356]\n",
            " [0.32147506 0.67852494]\n",
            " [0.20857757 0.79142243]\n",
            " [0.37187644 0.62812356]\n",
            " [0.14807527 0.85192473]\n",
            " [0.17940454 0.82059546]\n",
            " [0.58553193 0.41446807]\n",
            " [0.65658032 0.34341968]\n",
            " [0.62614566 0.37385434]\n",
            " [0.27316001 0.72683999]\n",
            " [0.32222267 0.67777733]\n",
            " [0.44761898 0.55238102]\n",
            " [0.34042877 0.65957123]\n",
            " [0.30972699 0.69027301]\n",
            " [0.30517761 0.69482239]\n",
            " [0.58861635 0.41138365]\n",
            " [0.26959707 0.73040293]\n",
            " [0.51453073 0.48546927]\n",
            " [0.55583589 0.44416411]\n",
            " [0.39764598 0.60235402]\n",
            " [0.20974686 0.79025314]\n",
            " [0.20219262 0.79780738]\n",
            " [0.6980013  0.3019987 ]\n",
            " [0.16624621 0.83375379]\n",
            " [0.29064584 0.70935416]\n",
            " [0.07469689 0.92530311]\n",
            " [0.58123858 0.41876142]\n",
            " [0.19837915 0.80162085]\n",
            " [0.29227268 0.70772732]\n",
            " [0.17826489 0.82173511]\n",
            " [0.33924515 0.66075485]\n",
            " [0.0981486  0.9018514 ]\n",
            " [0.59359258 0.40640742]\n",
            " [0.17765909 0.82234091]\n",
            " [0.12313685 0.87686315]\n",
            " [0.38908386 0.61091614]\n",
            " [0.23180674 0.76819326]\n",
            " [0.24496241 0.75503759]\n",
            " [0.4942587  0.5057413 ]\n",
            " [0.2501821  0.7498179 ]\n",
            " [0.16777448 0.83222552]\n",
            " [0.10096247 0.89903753]\n",
            " [0.62512124 0.37487876]\n",
            " [0.49969488 0.50030512]\n",
            " [0.55576144 0.44423856]\n",
            " [0.35849707 0.64150293]\n",
            " [0.29536969 0.70463031]\n",
            " [0.11688818 0.88311182]\n",
            " [0.70081525 0.29918475]\n",
            " [0.35981111 0.64018889]\n",
            " [0.2243093  0.7756907 ]\n",
            " [0.44155335 0.55844665]\n",
            " [0.15695488 0.84304512]\n",
            " [0.63858342 0.36141658]\n",
            " [0.13165944 0.86834056]\n",
            " [0.11848831 0.88151169]\n",
            " [0.27960087 0.72039913]\n",
            " [0.73050399 0.26949601]\n",
            " [0.11882483 0.88117517]\n",
            " [0.54941008 0.45058992]\n",
            " [0.2443694  0.7556306 ]\n",
            " [0.20362466 0.79637534]\n",
            " [0.09412485 0.90587515]\n",
            " [0.10229582 0.89770418]\n",
            " [0.32202931 0.67797069]\n",
            " [0.52980288 0.47019712]\n",
            " [0.18659804 0.81340196]\n",
            " [0.18246596 0.81753404]\n",
            " [0.1589121  0.8410879 ]\n",
            " [0.68030078 0.31969922]\n",
            " [0.41762624 0.58237376]\n",
            " [0.56765981 0.43234019]\n",
            " [0.23837263 0.76162737]\n",
            " [0.16698478 0.83301522]\n",
            " [0.25358763 0.74641237]\n",
            " [0.37525335 0.62474665]\n",
            " [0.02269162 0.97730838]\n",
            " [0.47127877 0.52872123]\n",
            " [0.23836439 0.76163561]\n",
            " [0.1780407  0.8219593 ]\n",
            " [0.29701159 0.70298841]\n",
            " [0.20937496 0.79062504]\n",
            " [0.14339179 0.85660821]\n",
            " [0.45377774 0.54622226]\n",
            " [0.51706443 0.48293557]\n",
            " [0.04081579 0.95918421]\n",
            " [0.36740488 0.63259512]\n",
            " [0.33711644 0.66288356]\n",
            " [0.24890689 0.75109311]\n",
            " [0.35996357 0.64003643]\n",
            " [0.194396   0.805604  ]\n",
            " [0.19727578 0.80272422]\n",
            " [0.24302904 0.75697096]\n",
            " [0.31520088 0.68479912]\n",
            " [0.37247361 0.62752639]\n",
            " [0.44216334 0.55783666]\n",
            " [0.29202092 0.70797908]\n",
            " [0.32034247 0.67965753]\n",
            " [0.19056463 0.80943537]\n",
            " [0.22538344 0.77461656]\n",
            " [0.16590573 0.83409427]\n",
            " [0.24532047 0.75467953]\n",
            " [0.10898316 0.89101684]\n",
            " [0.47290133 0.52709867]\n",
            " [0.5938101  0.4061899 ]\n",
            " [0.59191358 0.40808642]\n",
            " [0.0593565  0.9406435 ]\n",
            " [0.34537885 0.65462115]\n",
            " [0.58372317 0.41627683]\n",
            " [0.51468237 0.48531763]\n",
            " [0.14598839 0.85401161]\n",
            " [0.33079401 0.66920599]\n",
            " [0.17188729 0.82811271]\n",
            " [0.37517193 0.62482807]\n",
            " [0.21195619 0.78804381]\n",
            " [0.20771033 0.79228967]\n",
            " [0.29099052 0.70900948]\n",
            " [0.37010522 0.62989478]\n",
            " [0.13272746 0.86727254]\n",
            " [0.12713494 0.87286506]\n",
            " [0.58239832 0.41760168]\n",
            " [0.26610021 0.73389979]\n",
            " [0.322416   0.677584  ]\n",
            " [0.18373674 0.81626326]\n",
            " [0.02237077 0.97762923]\n",
            " [0.17828405 0.82171595]\n",
            " [0.21305711 0.78694289]\n",
            " [0.32301657 0.67698343]\n",
            " [0.47847204 0.52152796]\n",
            " [0.65287332 0.34712668]\n",
            " [0.23572938 0.76427062]\n",
            " [0.18095524 0.81904476]\n",
            " [0.42834523 0.57165477]\n",
            " [0.16616292 0.83383708]\n",
            " [0.04587982 0.95412018]\n",
            " [0.2243652  0.7756348 ]\n",
            " [0.61054239 0.38945761]\n",
            " [0.58338942 0.41661058]\n",
            " [0.56219487 0.43780513]\n",
            " [0.13204986 0.86795014]\n",
            " [0.27910443 0.72089557]\n",
            " [0.16576628 0.83423372]\n",
            " [0.15259463 0.84740537]\n",
            " [0.3009261  0.6990739 ]\n",
            " [0.67067769 0.32932231]\n",
            " [0.50462312 0.49537688]\n",
            " [0.25260015 0.74739985]\n",
            " [0.04450015 0.95549985]\n",
            " [0.29913098 0.70086902]\n",
            " [0.18802983 0.81197017]\n",
            " [0.08488051 0.91511949]\n",
            " [0.20866901 0.79133099]\n",
            " [0.15403601 0.84596399]\n",
            " [0.41188098 0.58811902]\n",
            " [0.20115398 0.79884602]\n",
            " [0.0694471  0.9305529 ]\n",
            " [0.07900547 0.92099453]\n",
            " [0.323916   0.676084  ]\n",
            " [0.56966326 0.43033674]\n",
            " [0.11577129 0.88422871]\n",
            " [0.17583902 0.82416098]\n",
            " [0.60633666 0.39366334]\n",
            " [0.68594859 0.31405141]\n",
            " [0.47879083 0.52120917]\n",
            " [0.18340555 0.81659445]\n",
            " [0.28402307 0.71597693]\n",
            " [0.29771424 0.70228576]\n",
            " [0.59061673 0.40938327]\n",
            " [0.17622643 0.82377357]\n",
            " [0.72390888 0.27609112]\n",
            " [0.38293105 0.61706895]\n",
            " [0.0584226  0.9415774 ]\n",
            " [0.55078176 0.44921824]\n",
            " [0.57362623 0.42637377]\n",
            " [0.45142893 0.54857107]\n",
            " [0.24989227 0.75010773]\n",
            " [0.17624596 0.82375404]\n",
            " [0.09000041 0.90999959]\n",
            " [0.51949184 0.48050816]\n",
            " [0.28663798 0.71336202]\n",
            " [0.70895214 0.29104786]\n",
            " [0.36831218 0.63168782]\n",
            " [0.4467274  0.5532726 ]\n",
            " [0.21450019 0.78549981]\n",
            " [0.40656435 0.59343565]\n",
            " [0.24484494 0.75515506]\n",
            " [0.12145667 0.87854333]\n",
            " [0.17256892 0.82743108]\n",
            " [0.83025888 0.16974112]\n",
            " [0.18465611 0.81534389]\n",
            " [0.30100893 0.69899107]\n",
            " [0.03702951 0.96297049]\n",
            " [0.19507442 0.80492558]\n",
            " [0.46836737 0.53163263]\n",
            " [0.15945526 0.84054474]\n",
            " [0.3631515  0.6368485 ]\n",
            " [0.23993856 0.76006144]\n",
            " [0.47186494 0.52813506]\n",
            " [0.5416308  0.4583692 ]\n",
            " [0.67246477 0.32753523]\n",
            " [0.7332897  0.2667103 ]\n",
            " [0.15240466 0.84759534]\n",
            " [0.29456564 0.70543436]\n",
            " [0.68137534 0.31862466]\n",
            " [0.15665116 0.84334884]\n",
            " [0.29017651 0.70982349]\n",
            " [0.14364376 0.85635624]\n",
            " [0.32064625 0.67935375]\n",
            " [0.46650138 0.53349862]\n",
            " [0.21872387 0.78127613]\n",
            " [0.04486053 0.95513947]\n",
            " [0.19085151 0.80914849]\n",
            " [0.32622591 0.67377409]\n",
            " [0.45478547 0.54521453]\n",
            " [0.48236825 0.51763175]\n",
            " [0.27067645 0.72932355]\n",
            " [0.44963291 0.55036709]\n",
            " [0.58511665 0.41488335]\n",
            " [0.24957918 0.75042082]\n",
            " [0.3393253  0.6606747 ]\n",
            " [0.14207729 0.85792271]\n",
            " [0.04744868 0.95255132]\n",
            " [0.35233628 0.64766372]\n",
            " [0.24843024 0.75156976]\n",
            " [0.84132404 0.15867596]\n",
            " [0.63810899 0.36189101]\n",
            " [0.34989392 0.65010608]\n",
            " [0.05927375 0.94072625]\n",
            " [0.17384153 0.82615847]\n",
            " [0.31329944 0.68670056]\n",
            " [0.12361088 0.87638912]\n",
            " [0.67764831 0.32235169]\n",
            " [0.53805758 0.46194242]\n",
            " [0.19409948 0.80590052]\n",
            " [0.04553108 0.95446892]\n",
            " [0.38647709 0.61352291]\n",
            " [0.40797731 0.59202269]\n",
            " [0.25174978 0.74825022]\n",
            " [0.08930444 0.91069556]\n",
            " [0.0937802  0.9062198 ]\n",
            " [0.49646476 0.50353524]\n",
            " [0.36044834 0.63955166]\n",
            " [0.33707518 0.66292482]\n",
            " [0.05550646 0.94449354]\n",
            " [0.09275779 0.90724221]\n",
            " [0.17876927 0.82123073]\n",
            " [0.09437689 0.90562311]\n",
            " [0.1680005  0.8319995 ]\n",
            " [0.37877632 0.62122368]\n",
            " [0.19166478 0.80833522]\n",
            " [0.18131867 0.81868133]\n",
            " [0.3182521  0.6817479 ]\n",
            " [0.04992521 0.95007479]\n",
            " [0.65214086 0.34785914]\n",
            " [0.37704854 0.62295146]\n",
            " [0.09291264 0.90708736]\n",
            " [0.13751454 0.86248546]\n",
            " [0.35184249 0.64815751]\n",
            " [0.31214612 0.68785388]\n",
            " [0.12473548 0.87526452]\n",
            " [0.17291305 0.82708695]\n",
            " [0.7342245  0.2657755 ]\n",
            " [0.63586062 0.36413938]\n",
            " [0.32654714 0.67345286]\n",
            " [0.42443715 0.57556285]\n",
            " [0.35743563 0.64256437]\n",
            " [0.32965921 0.67034079]\n",
            " [0.65247677 0.34752323]\n",
            " [0.36185427 0.63814573]\n",
            " [0.30333763 0.69666237]\n",
            " [0.08356727 0.91643273]\n",
            " [0.16631159 0.83368841]\n",
            " [0.08979289 0.91020711]\n",
            " [0.11684382 0.88315618]\n",
            " [0.16403633 0.83596367]\n",
            " [0.31817959 0.68182041]\n",
            " [0.36539948 0.63460052]\n",
            " [0.21048716 0.78951284]\n",
            " [0.30557626 0.69442374]\n",
            " [0.13003633 0.86996367]\n",
            " [0.10245816 0.89754184]\n",
            " [0.47618331 0.52381669]\n",
            " [0.2706681  0.7293319 ]\n",
            " [0.07630251 0.92369749]\n",
            " [0.28231986 0.71768014]\n",
            " [0.18350687 0.81649313]\n",
            " [0.2235747  0.7764253 ]\n",
            " [0.39418679 0.60581321]\n",
            " [0.05845253 0.94154747]\n",
            " [0.38398731 0.61601269]\n",
            " [0.05936739 0.94063261]\n",
            " [0.18333342 0.81666658]\n",
            " [0.2272502  0.7727498 ]\n",
            " [0.67385473 0.32614527]\n",
            " [0.10136419 0.89863581]\n",
            " [0.66355548 0.33644452]\n",
            " [0.62586549 0.37413451]]\n",
            "[0 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 0 1 0 0 1 1 1 0 1 1 1 0 1 1 1 1 1 0 1\n",
            " 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 1\n",
            " 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1\n",
            " 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 0\n",
            " 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1\n",
            " 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1\n",
            " 1 1 1 1 1 0 0 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1\n",
            " 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 0 1 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sv90ZmMLo_CQ",
        "outputId": "c49821ca-7dcb-413f-f810-987b9348a0b4"
      },
      "source": [
        "sens=X_test[[ '8','12']]\n",
        "sens_train_race= X_train[['12']]\n",
        "sens_train = X_train[['8', '12']]\n",
        "sens_train_sex = X_train[['8']]\n",
        "\n",
        "sens_test_race = X_test[['12']]\n",
        "sens_test_sex = X_test[['8']]\n",
        "\n",
        "print(sens)\n",
        "sens1=pd.get_dummies(sens, columns=['8','12'], prefix =['8','12'])\n",
        "sensitive=sens1.T\n",
        "print(sensitive) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       8   12\n",
            "0    0.0  0.0\n",
            "1    0.0  0.0\n",
            "2    0.0  0.0\n",
            "3    0.0  0.0\n",
            "4    0.0  0.0\n",
            "..   ...  ...\n",
            "295  1.0  1.0\n",
            "296  0.0  1.0\n",
            "297  0.0  0.0\n",
            "298  1.0  0.0\n",
            "299  1.0  1.0\n",
            "\n",
            "[300 rows x 2 columns]\n",
            "        0    1    2    3    4    5    6    ...  293  294  295  296  297  298  299\n",
            "8_0.0     1    1    1    1    1    1    0  ...    1    0    0    1    1    0    0\n",
            "8_1.0     0    0    0    0    0    0    1  ...    0    1    1    0    0    1    1\n",
            "12_0.0    1    1    1    1    1    1    1  ...    1    1    0    0    1    1    0\n",
            "12_1.0    0    0    0    0    0    0    0  ...    0    0    1    1    0    0    1\n",
            "\n",
            "[4 rows x 300 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14RjNrNUfe0A",
        "outputId": "34970134-68a2-474a-a3a9-c4275a7c3994"
      },
      "source": [
        "#----------------------------------FPR, TPR computation--------------------------------------------\n",
        "print(y_test.iloc[10])\n",
        "from random import *\n",
        "\n",
        "def find_eo_stats_multiple(y,y_pred):\n",
        "    m = 4\n",
        "    sens_stats = np.zeros((4,m), dtype = int)\n",
        "    sens_acc = np.zeros(m, dtype = float)\n",
        "    sizes = np.zeros(m, dtype = int)\n",
        "    #first row positives, second row negatives, third row true positive, fourth row false positive\n",
        "    \n",
        "    \n",
        "    for i in range(m):\n",
        "        for j in range(len(y)):\n",
        "            if(sensitive.iloc[i,j] == 1):\n",
        "                sizes[i] = sizes[i]+1\n",
        "                if(y_pred[j]==1):\n",
        "                    sens_acc[i] = sens_acc[i] + 1\n",
        "                if(y.iloc[j,0]==1):\n",
        "                    sens_stats[0][i] =sens_stats[0][i] + 1\n",
        "                if(y.iloc[j,0]==1 and y_pred[j]==1):\n",
        "                    sens_stats[2][i] =sens_stats[2][i] + 1\n",
        "                if(y.iloc[j,0]==0):\n",
        "                    sens_stats[1][i] =sens_stats[1][i] + 1\n",
        "                if(y.iloc[j,0]==0 and y_pred[j]==1):\n",
        "                    sens_stats[3][i] =sens_stats[3][i] + 1    \n",
        "        sens_acc[i] = sens_acc[i]/sizes[i]\n",
        "        #print(sens_acc[i],sizes[i])\n",
        "        \n",
        "    \n",
        "    TPR = np.zeros(m,dtype=float)\n",
        "    FPR = np.zeros(m,dtype=float)\n",
        "    \n",
        "    accu = 0\n",
        "    n = len(y)\n",
        "    for i in range(n):\n",
        "        if(y.iloc[i,0] == y_pred[i]):\n",
        "            accu = accu+1\n",
        "    \n",
        "    accu = accu/n\n",
        "    \n",
        "    max_tpr = -1 \n",
        "    min_tpr = 2\n",
        "    max_fpr = -1\n",
        "    min_fpr = 2\n",
        "    \n",
        "    for i in range(m):\n",
        "        TPR[i] = sens_stats[2][i]/sens_stats[0][i]\n",
        "        FPR[i] = sens_stats[3][i]/sens_stats[1][i]\n",
        "        if(TPR[i] >= max_tpr):\n",
        "            max_tpr = TPR[i]\n",
        "        if(TPR[i] <= min_tpr):\n",
        "            min_tpr = TPR[i]\n",
        "        if(FPR[i] >= max_fpr):\n",
        "            max_fpr = FPR[i]\n",
        "        if(FPR[i] <= min_fpr):\n",
        "            min_fpr = FPR[i]\n",
        "    \n",
        "    for i in  range(m):\n",
        "        print(\"TPR of the \",i,\"th sensitive groups is\",TPR[i])\n",
        "        \n",
        "    print(\"=====================\")    \n",
        "    for i in  range(m):\n",
        "        print(\"FPR of the \",i,\"th sensitive groups is\",FPR[i])\n",
        "    \n",
        "    print(\"=====================\")\n",
        "    for i in  range(m):    \n",
        "        print(\"Acceptance Rate of the \",i,\"th sensitive groups is\",sens_acc[i])\n",
        "    \n",
        "    DEO = abs(max_tpr-min_tpr) + abs(max_fpr-min_fpr)\n",
        "    \n",
        "    print(\"The Difference of Equalized odds of the classifier is=\",DEO)\n",
        "    print(\"Accuracy of the classifier\",accu)\n",
        "    \n",
        "#find_eo_stats_multiple(y_test,y_pred_CEO)\n",
        "#find_eo_stats_multiple(y_test,y_test_pred)\n",
        "\n",
        "\n",
        "\n",
        "        "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20    0\n",
            "Name: 10, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Ww_3L86fe0G",
        "outputId": "f393124e-4404-43d0-d2e9-1a88de644c87"
      },
      "source": [
        "sens=X_test[[5, 6 ,7, 8, 9,10,11]]\n",
        "print(sens)\n",
        "sensitive_final = np.zeros((len(sens.columns),len(y_test)), dtype=int)\n",
        "for i in range(len(y_test)):\n",
        "    for j in range(len(sens.columns)):\n",
        "        if(sens.iloc[i,j]>0):\n",
        "            sens.iloc[i,j] = 1\n",
        "        else:\n",
        "            sens.iloc[i,j] = 0\n",
        "        sensitive_final[j][i] = sens.iloc[i,j]    \n",
        "print(sens)            \n",
        "sensitive = sens.T\n",
        "print(sensitive_final)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              5         6         7         8         9         10        11\n",
            "s_male                                                                      \n",
            "1      -0.691448  0.691448 -0.096514 -0.171863 -0.320598 -0.086521  0.401044\n",
            "1      -0.691448  0.691448 -0.096514 -0.171863 -0.320598 -0.086521  0.401044\n",
            "1      -0.691448  0.691448 -0.096514 -0.171863 -0.320598 -0.086521  0.401044\n",
            "1      -0.691448  0.691448 -0.096514 -0.171863 -0.320598 -0.086521  0.401044\n",
            "1      -0.691448  0.691448 -0.096514 -0.171863 -0.320598 -0.086521  0.401044\n",
            "...          ...       ...       ...       ...       ...       ...       ...\n",
            "1      -0.691448  0.691448 -0.096514 -0.171863  3.119166 -0.086521 -2.493490\n",
            "1      -0.691448  0.691448 -0.096514 -0.171863 -0.320598 -0.086521  0.401044\n",
            "1      -0.691448  0.691448 -0.096514 -0.171863  3.119166 -0.086521 -2.493490\n",
            "1      -0.691448  0.691448 -0.096514 -0.171863 -0.320598 -0.086521  0.401044\n",
            "1      -0.691448  0.691448 -0.096514 -0.171863 -0.320598 -0.086521  0.401044\n",
            "\n",
            "[11306 rows x 7 columns]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         5    6    7    8    9    10   11\n",
            "s_male                                   \n",
            "1       0.0  1.0  0.0  0.0  0.0  0.0  1.0\n",
            "1       0.0  1.0  0.0  0.0  0.0  0.0  1.0\n",
            "1       0.0  1.0  0.0  0.0  0.0  0.0  1.0\n",
            "1       0.0  1.0  0.0  0.0  0.0  0.0  1.0\n",
            "1       0.0  1.0  0.0  0.0  0.0  0.0  1.0\n",
            "...     ...  ...  ...  ...  ...  ...  ...\n",
            "1       0.0  1.0  0.0  0.0  1.0  0.0  0.0\n",
            "1       0.0  1.0  0.0  0.0  0.0  0.0  1.0\n",
            "1       0.0  1.0  0.0  0.0  1.0  0.0  0.0\n",
            "1       0.0  1.0  0.0  0.0  0.0  0.0  1.0\n",
            "1       0.0  1.0  0.0  0.0  0.0  0.0  1.0\n",
            "\n",
            "[11306 rows x 7 columns]\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 1 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [1 1 1 ... 0 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIhYlkKIyq6-",
        "outputId": "c89dc5f6-b861-4f85-bbd1-4c8caa1e4bcd"
      },
      "source": [
        "pip install pulp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pulp\n",
            "  Downloading PuLP-2.6.0-py3-none-any.whl (14.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.2 MB 5.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: pulp\n",
            "Successfully installed pulp-2.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EB0jk_jxfe0J",
        "outputId": "288707b5-a72b-4d62-aafc-636080ad0d2d"
      },
      "source": [
        "import pulp as pl\n",
        "solver_list = pl.listSolvers(onlyAvailable=True)\n",
        "print(solver_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['PULP_CBC_CMD', 'PULP_CHOCO_CMD']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Gs7ctb6fe0W",
        "outputId": "d4bdb91b-ac41-41b8-f308-9ee79d60401d"
      },
      "source": [
        "accu_all,DP_all,acceptance_rate,alpha_weight = main2(sensitive, y_test, y_test_pred,e)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sensitive attribute  1\n",
            "ACTUAL----------total ,accepted, aceeptance rate:\n",
            "196\n",
            "149\n",
            "0.7602040816326531\n",
            "sensitive attribute  2\n",
            "ACTUAL----------total ,accepted, aceeptance rate:\n",
            "104\n",
            "65\n",
            "0.625\n",
            "sensitive attribute  3\n",
            "ACTUAL----------total ,accepted, aceeptance rate:\n",
            "241\n",
            "183\n",
            "0.7593360995850622\n",
            "sensitive attribute  4\n",
            "ACTUAL----------total ,accepted, aceeptance rate:\n",
            "59\n",
            "31\n",
            "0.5254237288135594\n",
            "data acceptance rates\n",
            "[0.7602040816326531, 0.625, 0.7593360995850622, 0.5254237288135594]\n",
            "data DP\n",
            "0.2347803528190937\n",
            "sensitive attribute  1\n",
            "prec reca accuracy for each sens\n",
            "0.8260869565217391 0.8926174496644296 0.7755102040816326\n",
            "RanomForest----------total , accepted, aceeptance rate:\n",
            "196\n",
            "161\n",
            "0.8214285714285714\n",
            "sensitive attribute  2\n",
            "prec reca accuracy for each sens\n",
            "0.7317073170731707 0.9230769230769231 0.7403846153846154\n",
            "RanomForest----------total , accepted, aceeptance rate:\n",
            "104\n",
            "82\n",
            "0.7884615384615384\n",
            "sensitive attribute  3\n",
            "prec reca accuracy for each sens\n",
            "0.8291457286432161 0.9016393442622951 0.7842323651452282\n",
            "RanomForest----------total , accepted, aceeptance rate:\n",
            "241\n",
            "199\n",
            "0.8257261410788381\n",
            "sensitive attribute  4\n",
            "prec reca accuracy for each sens\n",
            "0.6363636363636364 0.9032258064516129 0.6779661016949152\n",
            "RanomForest----------total , accepted, aceeptance rate:\n",
            "59\n",
            "44\n",
            "0.7457627118644068\n",
            "data acceptance rates\n",
            "[0.8214285714285714, 0.7884615384615384, 0.8257261410788381, 0.7457627118644068]\n",
            "data DP\n",
            "0.07996342921443134\n",
            "SVM accuracy--------------------------\n",
            "0.7942386831275721 0.9018691588785047 0.7633333333333333\n",
            "----------------This is for covergence at beta =  0.15  ----------------\n",
            "dimension of data\n",
            "4 300\n",
            "[196 104]\n",
            "Optimal\n",
            "objective is:\n",
            "23769.0\n",
            "discripency is:\n",
            "None\n",
            "gamma-epsilon-delta [0.175442, 0.142103, 0.166039, 0.164754, 0.153465, 0.14, 0.104348] 0.01 1\n",
            "<--------------------------------------->\n",
            "TPR of the  0 th sensitive groups is 0.8657718120805369\n",
            "TPR of the  1 th sensitive groups is 0.8\n",
            "TPR of the  2 th sensitive groups is 0.8469945355191257\n",
            "TPR of the  3 th sensitive groups is 0.8387096774193549\n",
            "=====================\n",
            "FPR of the  0 th sensitive groups is 0.44680851063829785\n",
            "FPR of the  1 th sensitive groups is 0.38461538461538464\n",
            "FPR of the  2 th sensitive groups is 0.4482758620689655\n",
            "FPR of the  3 th sensitive groups is 0.35714285714285715\n",
            "=====================\n",
            "Acceptance Rate of the  0 th sensitive groups is 0.7653061224489796\n",
            "Acceptance Rate of the  1 th sensitive groups is 0.6442307692307693\n",
            "Acceptance Rate of the  2 th sensitive groups is 0.7510373443983402\n",
            "Acceptance Rate of the  3 th sensitive groups is 0.6101694915254238\n",
            "The Difference of Equalized odds of the classifier is= 0.15690481700664521\n",
            "Accuracy of the classifier 0.77\n",
            "sensitive attribute  1\n",
            "precision 0.86\n",
            "recall 0.8657718120805369\n",
            "FPR 0.44680851063829785\n",
            "TP,FP,TN,FN\n",
            "129 21 26 20\n",
            "sensitive attribute  2\n",
            "precision 0.7761194029850746\n",
            "recall 0.8\n",
            "FPR 0.38461538461538464\n",
            "TP,FP,TN,FN\n",
            "52 15 24 13\n",
            "sensitive attribute  3\n",
            "precision 0.856353591160221\n",
            "recall 0.8469945355191257\n",
            "FPR 0.4482758620689655\n",
            "TP,FP,TN,FN\n",
            "155 26 32 28\n",
            "sensitive attribute  4\n",
            "precision 0.7222222222222222\n",
            "recall 0.8387096774193549\n",
            "FPR 0.35714285714285715\n",
            "TP,FP,TN,FN\n",
            "26 10 18 5\n",
            "acceptance rates\n",
            "[0.7653061224489796, 0.6442307692307693, 0.7510373443983402, 0.6101694915254238]\n",
            "DP\n",
            "0.15513663092355578\n",
            "dimension of data\n",
            "4 300\n",
            "[196 104]\n",
            "Optimal\n",
            "objective is:\n",
            "21610.0\n",
            "discripency is:\n",
            "None\n",
            "gamma-epsilon-delta [0.175442, 0.142103, 0.166039, 0.164754, 0.153465, 0.14, 0.104348] 0.01 1\n",
            "<--------------------------------------->\n",
            "TPR of the  0 th sensitive groups is 0.8389261744966443\n",
            "TPR of the  1 th sensitive groups is 0.7692307692307693\n",
            "TPR of the  2 th sensitive groups is 0.8142076502732241\n",
            "TPR of the  3 th sensitive groups is 0.8387096774193549\n",
            "=====================\n",
            "FPR of the  0 th sensitive groups is 0.3829787234042553\n",
            "FPR of the  1 th sensitive groups is 0.358974358974359\n",
            "FPR of the  2 th sensitive groups is 0.3793103448275862\n",
            "FPR of the  3 th sensitive groups is 0.35714285714285715\n",
            "=====================\n",
            "Acceptance Rate of the  0 th sensitive groups is 0.7295918367346939\n",
            "Acceptance Rate of the  1 th sensitive groups is 0.6153846153846154\n",
            "Acceptance Rate of the  2 th sensitive groups is 0.7095435684647303\n",
            "Acceptance Rate of the  3 th sensitive groups is 0.6101694915254238\n",
            "The Difference of Equalized odds of the classifier is= 0.0955312715272732\n",
            "Accuracy of the classifier 0.7633333333333333\n",
            "sensitive attribute  1\n",
            "precision 0.8741258741258742\n",
            "recall 0.8389261744966443\n",
            "FPR 0.3829787234042553\n",
            "TP,FP,TN,FN\n",
            "125 18 29 24\n",
            "sensitive attribute  2\n",
            "precision 0.78125\n",
            "recall 0.7692307692307693\n",
            "FPR 0.358974358974359\n",
            "TP,FP,TN,FN\n",
            "50 14 25 15\n",
            "sensitive attribute  3\n",
            "precision 0.8713450292397661\n",
            "recall 0.8142076502732241\n",
            "FPR 0.3793103448275862\n",
            "TP,FP,TN,FN\n",
            "149 22 36 34\n",
            "sensitive attribute  4\n",
            "precision 0.7222222222222222\n",
            "recall 0.8387096774193549\n",
            "FPR 0.35714285714285715\n",
            "TP,FP,TN,FN\n",
            "26 10 18 5\n",
            "acceptance rates\n",
            "[0.7295918367346939, 0.6153846153846154, 0.7095435684647303, 0.6101694915254238]\n",
            "DP\n",
            "0.11942234520927009\n",
            "dimension of data\n",
            "4 300\n",
            "[196 104]\n",
            "Optimal\n",
            "objective is:\n",
            "21365.0\n",
            "discripency is:\n",
            "None\n",
            "gamma-epsilon-delta [0.175442, 0.142103, 0.166039, 0.164754, 0.153465, 0.14, 0.104348] 0.01 1\n",
            "<--------------------------------------->\n",
            "TPR of the  0 th sensitive groups is 0.8322147651006712\n",
            "TPR of the  1 th sensitive groups is 0.7692307692307693\n",
            "TPR of the  2 th sensitive groups is 0.8087431693989071\n",
            "TPR of the  3 th sensitive groups is 0.8387096774193549\n",
            "=====================\n",
            "FPR of the  0 th sensitive groups is 0.3829787234042553\n",
            "FPR of the  1 th sensitive groups is 0.358974358974359\n",
            "FPR of the  2 th sensitive groups is 0.3793103448275862\n",
            "FPR of the  3 th sensitive groups is 0.35714285714285715\n",
            "=====================\n",
            "Acceptance Rate of the  0 th sensitive groups is 0.7244897959183674\n",
            "Acceptance Rate of the  1 th sensitive groups is 0.6153846153846154\n",
            "Acceptance Rate of the  2 th sensitive groups is 0.7053941908713693\n",
            "Acceptance Rate of the  3 th sensitive groups is 0.6101694915254238\n",
            "The Difference of Equalized odds of the classifier is= 0.09531477444998376\n",
            "Accuracy of the classifier 0.76\n",
            "sensitive attribute  1\n",
            "precision 0.8732394366197183\n",
            "recall 0.8322147651006712\n",
            "FPR 0.3829787234042553\n",
            "TP,FP,TN,FN\n",
            "124 18 29 25\n",
            "sensitive attribute  2\n",
            "precision 0.78125\n",
            "recall 0.7692307692307693\n",
            "FPR 0.358974358974359\n",
            "TP,FP,TN,FN\n",
            "50 14 25 15\n",
            "sensitive attribute  3\n",
            "precision 0.8705882352941177\n",
            "recall 0.8087431693989071\n",
            "FPR 0.3793103448275862\n",
            "TP,FP,TN,FN\n",
            "148 22 36 35\n",
            "sensitive attribute  4\n",
            "precision 0.7222222222222222\n",
            "recall 0.8387096774193549\n",
            "FPR 0.35714285714285715\n",
            "TP,FP,TN,FN\n",
            "26 10 18 5\n",
            "acceptance rates\n",
            "[0.7244897959183674, 0.6153846153846154, 0.7053941908713693, 0.6101694915254238]\n",
            "DP\n",
            "0.1143203043929436\n",
            "dimension of data\n",
            "4 300\n",
            "[196 104]\n",
            "Optimal\n",
            "objective is:\n",
            "21365.0\n",
            "discripency is:\n",
            "None\n",
            "gamma-epsilon-delta [0.175442, 0.142103, 0.166039, 0.164754, 0.153465, 0.14, 0.104348] 0.01 1\n",
            "<--------------------------------------->\n",
            "TPR of the  0 th sensitive groups is 0.8322147651006712\n",
            "TPR of the  1 th sensitive groups is 0.7692307692307693\n",
            "TPR of the  2 th sensitive groups is 0.8087431693989071\n",
            "TPR of the  3 th sensitive groups is 0.8387096774193549\n",
            "=====================\n",
            "FPR of the  0 th sensitive groups is 0.3829787234042553\n",
            "FPR of the  1 th sensitive groups is 0.358974358974359\n",
            "FPR of the  2 th sensitive groups is 0.3793103448275862\n",
            "FPR of the  3 th sensitive groups is 0.35714285714285715\n",
            "=====================\n",
            "Acceptance Rate of the  0 th sensitive groups is 0.7244897959183674\n",
            "Acceptance Rate of the  1 th sensitive groups is 0.6153846153846154\n",
            "Acceptance Rate of the  2 th sensitive groups is 0.7053941908713693\n",
            "Acceptance Rate of the  3 th sensitive groups is 0.6101694915254238\n",
            "The Difference of Equalized odds of the classifier is= 0.09531477444998376\n",
            "Accuracy of the classifier 0.76\n",
            "sensitive attribute  1\n",
            "precision 0.8732394366197183\n",
            "recall 0.8322147651006712\n",
            "FPR 0.3829787234042553\n",
            "TP,FP,TN,FN\n",
            "124 18 29 25\n",
            "sensitive attribute  2\n",
            "precision 0.78125\n",
            "recall 0.7692307692307693\n",
            "FPR 0.358974358974359\n",
            "TP,FP,TN,FN\n",
            "50 14 25 15\n",
            "sensitive attribute  3\n",
            "precision 0.8705882352941177\n",
            "recall 0.8087431693989071\n",
            "FPR 0.3793103448275862\n",
            "TP,FP,TN,FN\n",
            "148 22 36 35\n",
            "sensitive attribute  4\n",
            "precision 0.7222222222222222\n",
            "recall 0.8387096774193549\n",
            "FPR 0.35714285714285715\n",
            "TP,FP,TN,FN\n",
            "26 10 18 5\n",
            "acceptance rates\n",
            "[0.7244897959183674, 0.6153846153846154, 0.7053941908713693, 0.6101694915254238]\n",
            "DP\n",
            "0.1143203043929436\n",
            "dimension of data\n",
            "4 300\n",
            "[196 104]\n",
            "Optimal\n",
            "objective is:\n",
            "20997.0\n",
            "discripency is:\n",
            "None\n",
            "gamma-epsilon-delta [0.175442, 0.142103, 0.166039, 0.164754, 0.153465, 0.14, 0.104348] 0.01 1\n",
            "<--------------------------------------->\n",
            "TPR of the  0 th sensitive groups is 0.8389261744966443\n",
            "TPR of the  1 th sensitive groups is 0.7692307692307693\n",
            "TPR of the  2 th sensitive groups is 0.8142076502732241\n",
            "TPR of the  3 th sensitive groups is 0.8387096774193549\n",
            "=====================\n",
            "FPR of the  0 th sensitive groups is 0.3404255319148936\n",
            "FPR of the  1 th sensitive groups is 0.3333333333333333\n",
            "FPR of the  2 th sensitive groups is 0.3448275862068966\n",
            "FPR of the  3 th sensitive groups is 0.32142857142857145\n",
            "=====================\n",
            "Acceptance Rate of the  0 th sensitive groups is 0.7193877551020408\n",
            "Acceptance Rate of the  1 th sensitive groups is 0.6057692307692307\n",
            "Acceptance Rate of the  2 th sensitive groups is 0.7012448132780082\n",
            "Acceptance Rate of the  3 th sensitive groups is 0.5932203389830508\n",
            "The Difference of Equalized odds of the classifier is= 0.09309442004420015\n",
            "Accuracy of the classifier 0.7733333333333333\n",
            "sensitive attribute  1\n",
            "precision 0.8865248226950354\n",
            "recall 0.8389261744966443\n",
            "FPR 0.3404255319148936\n",
            "TP,FP,TN,FN\n",
            "125 16 31 24\n",
            "sensitive attribute  2\n",
            "precision 0.7936507936507936\n",
            "recall 0.7692307692307693\n",
            "FPR 0.3333333333333333\n",
            "TP,FP,TN,FN\n",
            "50 13 26 15\n",
            "sensitive attribute  3\n",
            "precision 0.8816568047337278\n",
            "recall 0.8142076502732241\n",
            "FPR 0.3448275862068966\n",
            "TP,FP,TN,FN\n",
            "149 20 38 34\n",
            "sensitive attribute  4\n",
            "precision 0.7428571428571429\n",
            "recall 0.8387096774193549\n",
            "FPR 0.32142857142857145\n",
            "TP,FP,TN,FN\n",
            "26 9 19 5\n",
            "acceptance rates\n",
            "[0.7193877551020408, 0.6057692307692307, 0.7012448132780082, 0.5932203389830508]\n",
            "DP\n",
            "0.12616741611898996\n",
            "dimension of data\n",
            "4 300\n",
            "[196 104]\n",
            "Optimal\n",
            "objective is:\n",
            "3240.0\n",
            "discripency is:\n",
            "None\n",
            "gamma-epsilon-delta [0.175442, 0.142103, 0.166039, 0.164754, 0.153465, 0.14, 0.104348] 0.01 1\n",
            "<--------------------------------------->\n",
            "TPR of the  0 th sensitive groups is 0.348993288590604\n",
            "TPR of the  1 th sensitive groups is 0.3230769230769231\n",
            "TPR of the  2 th sensitive groups is 0.36065573770491804\n",
            "TPR of the  3 th sensitive groups is 0.22580645161290322\n",
            "=====================\n",
            "FPR of the  0 th sensitive groups is 0.0425531914893617\n",
            "FPR of the  1 th sensitive groups is 0.1282051282051282\n",
            "FPR of the  2 th sensitive groups is 0.05172413793103448\n",
            "FPR of the  3 th sensitive groups is 0.14285714285714285\n",
            "=====================\n",
            "Acceptance Rate of the  0 th sensitive groups is 0.2755102040816326\n",
            "Acceptance Rate of the  1 th sensitive groups is 0.25\n",
            "Acceptance Rate of the  2 th sensitive groups is 0.2863070539419087\n",
            "Acceptance Rate of the  3 th sensitive groups is 0.1864406779661017\n",
            "The Difference of Equalized odds of the classifier is= 0.23515323745979597\n",
            "Accuracy of the classifier 0.5066666666666667\n",
            "sensitive attribute  1\n",
            "precision 0.9629629629629629\n",
            "recall 0.348993288590604\n",
            "FPR 0.0425531914893617\n",
            "TP,FP,TN,FN\n",
            "52 2 45 97\n",
            "sensitive attribute  2\n",
            "precision 0.8076923076923077\n",
            "recall 0.3230769230769231\n",
            "FPR 0.1282051282051282\n",
            "TP,FP,TN,FN\n",
            "21 5 34 44\n",
            "sensitive attribute  3\n",
            "precision 0.9565217391304348\n",
            "recall 0.36065573770491804\n",
            "FPR 0.05172413793103448\n",
            "TP,FP,TN,FN\n",
            "66 3 55 117\n",
            "sensitive attribute  4\n",
            "precision 0.6363636363636364\n",
            "recall 0.22580645161290322\n",
            "FPR 0.14285714285714285\n",
            "TP,FP,TN,FN\n",
            "7 4 24 24\n",
            "acceptance rates\n",
            "[0.2755102040816326, 0.25, 0.2863070539419087, 0.1864406779661017]\n",
            "DP\n",
            "0.099866375975807\n",
            "----------------This is for covergence at beta =  0.2  ----------------\n",
            "dimension of data\n",
            "4 300\n",
            "[196 104]\n",
            "Optimal\n",
            "objective is:\n",
            "23769.0\n",
            "discripency is:\n",
            "None\n",
            "gamma-epsilon-delta [0.175442, 0.142103, 0.166039, 0.164754, 0.153465, 0.14, 0.104348] 0.01 1\n",
            "<--------------------------------------->\n",
            "TPR of the  0 th sensitive groups is 0.8657718120805369\n",
            "TPR of the  1 th sensitive groups is 0.8\n",
            "TPR of the  2 th sensitive groups is 0.8469945355191257\n",
            "TPR of the  3 th sensitive groups is 0.8387096774193549\n",
            "=====================\n",
            "FPR of the  0 th sensitive groups is 0.44680851063829785\n",
            "FPR of the  1 th sensitive groups is 0.38461538461538464\n",
            "FPR of the  2 th sensitive groups is 0.4482758620689655\n",
            "FPR of the  3 th sensitive groups is 0.35714285714285715\n",
            "=====================\n",
            "Acceptance Rate of the  0 th sensitive groups is 0.7653061224489796\n",
            "Acceptance Rate of the  1 th sensitive groups is 0.6442307692307693\n",
            "Acceptance Rate of the  2 th sensitive groups is 0.7510373443983402\n",
            "Acceptance Rate of the  3 th sensitive groups is 0.6101694915254238\n",
            "The Difference of Equalized odds of the classifier is= 0.15690481700664521\n",
            "Accuracy of the classifier 0.77\n",
            "sensitive attribute  1\n",
            "precision 0.86\n",
            "recall 0.8657718120805369\n",
            "FPR 0.44680851063829785\n",
            "TP,FP,TN,FN\n",
            "129 21 26 20\n",
            "sensitive attribute  2\n",
            "precision 0.7761194029850746\n",
            "recall 0.8\n",
            "FPR 0.38461538461538464\n",
            "TP,FP,TN,FN\n",
            "52 15 24 13\n",
            "sensitive attribute  3\n",
            "precision 0.856353591160221\n",
            "recall 0.8469945355191257\n",
            "FPR 0.4482758620689655\n",
            "TP,FP,TN,FN\n",
            "155 26 32 28\n",
            "sensitive attribute  4\n",
            "precision 0.7222222222222222\n",
            "recall 0.8387096774193549\n",
            "FPR 0.35714285714285715\n",
            "TP,FP,TN,FN\n",
            "26 10 18 5\n",
            "acceptance rates\n",
            "[0.7653061224489796, 0.6442307692307693, 0.7510373443983402, 0.6101694915254238]\n",
            "DP\n",
            "0.15513663092355578\n",
            "dimension of data\n",
            "4 300\n",
            "[196 104]\n",
            "Optimal\n",
            "objective is:\n",
            "21610.0\n",
            "discripency is:\n",
            "None\n",
            "gamma-epsilon-delta [0.175442, 0.142103, 0.166039, 0.164754, 0.153465, 0.14, 0.104348] 0.01 1\n",
            "<--------------------------------------->\n",
            "TPR of the  0 th sensitive groups is 0.8389261744966443\n",
            "TPR of the  1 th sensitive groups is 0.7692307692307693\n",
            "TPR of the  2 th sensitive groups is 0.8142076502732241\n",
            "TPR of the  3 th sensitive groups is 0.8387096774193549\n",
            "=====================\n",
            "FPR of the  0 th sensitive groups is 0.3829787234042553\n",
            "FPR of the  1 th sensitive groups is 0.358974358974359\n",
            "FPR of the  2 th sensitive groups is 0.3793103448275862\n",
            "FPR of the  3 th sensitive groups is 0.35714285714285715\n",
            "=====================\n",
            "Acceptance Rate of the  0 th sensitive groups is 0.7295918367346939\n",
            "Acceptance Rate of the  1 th sensitive groups is 0.6153846153846154\n",
            "Acceptance Rate of the  2 th sensitive groups is 0.7095435684647303\n",
            "Acceptance Rate of the  3 th sensitive groups is 0.6101694915254238\n",
            "The Difference of Equalized odds of the classifier is= 0.0955312715272732\n",
            "Accuracy of the classifier 0.7633333333333333\n",
            "sensitive attribute  1\n",
            "precision 0.8741258741258742\n",
            "recall 0.8389261744966443\n",
            "FPR 0.3829787234042553\n",
            "TP,FP,TN,FN\n",
            "125 18 29 24\n",
            "sensitive attribute  2\n",
            "precision 0.78125\n",
            "recall 0.7692307692307693\n",
            "FPR 0.358974358974359\n",
            "TP,FP,TN,FN\n",
            "50 14 25 15\n",
            "sensitive attribute  3\n",
            "precision 0.8713450292397661\n",
            "recall 0.8142076502732241\n",
            "FPR 0.3793103448275862\n",
            "TP,FP,TN,FN\n",
            "149 22 36 34\n",
            "sensitive attribute  4\n",
            "precision 0.7222222222222222\n",
            "recall 0.8387096774193549\n",
            "FPR 0.35714285714285715\n",
            "TP,FP,TN,FN\n",
            "26 10 18 5\n",
            "acceptance rates\n",
            "[0.7295918367346939, 0.6153846153846154, 0.7095435684647303, 0.6101694915254238]\n",
            "DP\n",
            "0.11942234520927009\n",
            "dimension of data\n",
            "4 300\n",
            "[196 104]\n",
            "Optimal\n",
            "objective is:\n",
            "21610.0\n",
            "discripency is:\n",
            "None\n",
            "gamma-epsilon-delta [0.175442, 0.142103, 0.166039, 0.164754, 0.153465, 0.14, 0.104348] 0.01 1\n",
            "<--------------------------------------->\n",
            "TPR of the  0 th sensitive groups is 0.8389261744966443\n",
            "TPR of the  1 th sensitive groups is 0.7692307692307693\n",
            "TPR of the  2 th sensitive groups is 0.8142076502732241\n",
            "TPR of the  3 th sensitive groups is 0.8387096774193549\n",
            "=====================\n",
            "FPR of the  0 th sensitive groups is 0.3829787234042553\n",
            "FPR of the  1 th sensitive groups is 0.358974358974359\n",
            "FPR of the  2 th sensitive groups is 0.3793103448275862\n",
            "FPR of the  3 th sensitive groups is 0.35714285714285715\n",
            "=====================\n",
            "Acceptance Rate of the  0 th sensitive groups is 0.7295918367346939\n",
            "Acceptance Rate of the  1 th sensitive groups is 0.6153846153846154\n",
            "Acceptance Rate of the  2 th sensitive groups is 0.7095435684647303\n",
            "Acceptance Rate of the  3 th sensitive groups is 0.6101694915254238\n",
            "The Difference of Equalized odds of the classifier is= 0.0955312715272732\n",
            "Accuracy of the classifier 0.7633333333333333\n",
            "sensitive attribute  1\n",
            "precision 0.8741258741258742\n",
            "recall 0.8389261744966443\n",
            "FPR 0.3829787234042553\n",
            "TP,FP,TN,FN\n",
            "125 18 29 24\n",
            "sensitive attribute  2\n",
            "precision 0.78125\n",
            "recall 0.7692307692307693\n",
            "FPR 0.358974358974359\n",
            "TP,FP,TN,FN\n",
            "50 14 25 15\n",
            "sensitive attribute  3\n",
            "precision 0.8713450292397661\n",
            "recall 0.8142076502732241\n",
            "FPR 0.3793103448275862\n",
            "TP,FP,TN,FN\n",
            "149 22 36 34\n",
            "sensitive attribute  4\n",
            "precision 0.7222222222222222\n",
            "recall 0.8387096774193549\n",
            "FPR 0.35714285714285715\n",
            "TP,FP,TN,FN\n",
            "26 10 18 5\n",
            "acceptance rates\n",
            "[0.7295918367346939, 0.6153846153846154, 0.7095435684647303, 0.6101694915254238]\n",
            "DP\n",
            "0.11942234520927009\n",
            "dimension of data\n",
            "4 300\n",
            "[196 104]\n",
            "Optimal\n",
            "objective is:\n",
            "21610.0\n",
            "discripency is:\n",
            "None\n",
            "gamma-epsilon-delta [0.175442, 0.142103, 0.166039, 0.164754, 0.153465, 0.14, 0.104348] 0.01 1\n",
            "<--------------------------------------->\n",
            "TPR of the  0 th sensitive groups is 0.8389261744966443\n",
            "TPR of the  1 th sensitive groups is 0.7692307692307693\n",
            "TPR of the  2 th sensitive groups is 0.8142076502732241\n",
            "TPR of the  3 th sensitive groups is 0.8387096774193549\n",
            "=====================\n",
            "FPR of the  0 th sensitive groups is 0.3829787234042553\n",
            "FPR of the  1 th sensitive groups is 0.358974358974359\n",
            "FPR of the  2 th sensitive groups is 0.3793103448275862\n",
            "FPR of the  3 th sensitive groups is 0.35714285714285715\n",
            "=====================\n",
            "Acceptance Rate of the  0 th sensitive groups is 0.7295918367346939\n",
            "Acceptance Rate of the  1 th sensitive groups is 0.6153846153846154\n",
            "Acceptance Rate of the  2 th sensitive groups is 0.7095435684647303\n",
            "Acceptance Rate of the  3 th sensitive groups is 0.6101694915254238\n",
            "The Difference of Equalized odds of the classifier is= 0.0955312715272732\n",
            "Accuracy of the classifier 0.7633333333333333\n",
            "sensitive attribute  1\n",
            "precision 0.8741258741258742\n",
            "recall 0.8389261744966443\n",
            "FPR 0.3829787234042553\n",
            "TP,FP,TN,FN\n",
            "125 18 29 24\n",
            "sensitive attribute  2\n",
            "precision 0.78125\n",
            "recall 0.7692307692307693\n",
            "FPR 0.358974358974359\n",
            "TP,FP,TN,FN\n",
            "50 14 25 15\n",
            "sensitive attribute  3\n",
            "precision 0.8713450292397661\n",
            "recall 0.8142076502732241\n",
            "FPR 0.3793103448275862\n",
            "TP,FP,TN,FN\n",
            "149 22 36 34\n",
            "sensitive attribute  4\n",
            "precision 0.7222222222222222\n",
            "recall 0.8387096774193549\n",
            "FPR 0.35714285714285715\n",
            "TP,FP,TN,FN\n",
            "26 10 18 5\n",
            "acceptance rates\n",
            "[0.7295918367346939, 0.6153846153846154, 0.7095435684647303, 0.6101694915254238]\n",
            "DP\n",
            "0.11942234520927009\n",
            "dimension of data\n",
            "4 300\n",
            "[196 104]\n",
            "Optimal\n",
            "objective is:\n",
            "21365.0\n",
            "discripency is:\n",
            "None\n",
            "gamma-epsilon-delta [0.175442, 0.142103, 0.166039, 0.164754, 0.153465, 0.14, 0.104348] 0.01 1\n",
            "<--------------------------------------->\n",
            "TPR of the  0 th sensitive groups is 0.8322147651006712\n",
            "TPR of the  1 th sensitive groups is 0.7692307692307693\n",
            "TPR of the  2 th sensitive groups is 0.8087431693989071\n",
            "TPR of the  3 th sensitive groups is 0.8387096774193549\n",
            "=====================\n",
            "FPR of the  0 th sensitive groups is 0.3829787234042553\n",
            "FPR of the  1 th sensitive groups is 0.358974358974359\n",
            "FPR of the  2 th sensitive groups is 0.3793103448275862\n",
            "FPR of the  3 th sensitive groups is 0.35714285714285715\n",
            "=====================\n",
            "Acceptance Rate of the  0 th sensitive groups is 0.7244897959183674\n",
            "Acceptance Rate of the  1 th sensitive groups is 0.6153846153846154\n",
            "Acceptance Rate of the  2 th sensitive groups is 0.7053941908713693\n",
            "Acceptance Rate of the  3 th sensitive groups is 0.6101694915254238\n",
            "The Difference of Equalized odds of the classifier is= 0.09531477444998376\n",
            "Accuracy of the classifier 0.76\n",
            "sensitive attribute  1\n",
            "precision 0.8732394366197183\n",
            "recall 0.8322147651006712\n",
            "FPR 0.3829787234042553\n",
            "TP,FP,TN,FN\n",
            "124 18 29 25\n",
            "sensitive attribute  2\n",
            "precision 0.78125\n",
            "recall 0.7692307692307693\n",
            "FPR 0.358974358974359\n",
            "TP,FP,TN,FN\n",
            "50 14 25 15\n",
            "sensitive attribute  3\n",
            "precision 0.8705882352941177\n",
            "recall 0.8087431693989071\n",
            "FPR 0.3793103448275862\n",
            "TP,FP,TN,FN\n",
            "148 22 36 35\n",
            "sensitive attribute  4\n",
            "precision 0.7222222222222222\n",
            "recall 0.8387096774193549\n",
            "FPR 0.35714285714285715\n",
            "TP,FP,TN,FN\n",
            "26 10 18 5\n",
            "acceptance rates\n",
            "[0.7244897959183674, 0.6153846153846154, 0.7053941908713693, 0.6101694915254238]\n",
            "DP\n",
            "0.1143203043929436\n",
            "dimension of data\n",
            "4 300\n",
            "[196 104]\n",
            "Optimal\n",
            "objective is:\n",
            "4287.0\n",
            "discripency is:\n",
            "None\n",
            "gamma-epsilon-delta [0.175442, 0.142103, 0.166039, 0.164754, 0.153465, 0.14, 0.104348] 0.01 1\n",
            "<--------------------------------------->\n",
            "TPR of the  0 th sensitive groups is 0.3959731543624161\n",
            "TPR of the  1 th sensitive groups is 0.36923076923076925\n",
            "TPR of the  2 th sensitive groups is 0.3989071038251366\n",
            "TPR of the  3 th sensitive groups is 0.3225806451612903\n",
            "=====================\n",
            "FPR of the  0 th sensitive groups is 0.06382978723404255\n",
            "FPR of the  1 th sensitive groups is 0.15384615384615385\n",
            "FPR of the  2 th sensitive groups is 0.05172413793103448\n",
            "FPR of the  3 th sensitive groups is 0.21428571428571427\n",
            "=====================\n",
            "Acceptance Rate of the  0 th sensitive groups is 0.3163265306122449\n",
            "Acceptance Rate of the  1 th sensitive groups is 0.28846153846153844\n",
            "Acceptance Rate of the  2 th sensitive groups is 0.3153526970954357\n",
            "Acceptance Rate of the  3 th sensitive groups is 0.2711864406779661\n",
            "The Difference of Equalized odds of the classifier is= 0.2388880350185261\n",
            "Accuracy of the classifier 0.5333333333333333\n",
            "sensitive attribute  1\n",
            "precision 0.9516129032258065\n",
            "recall 0.3959731543624161\n",
            "FPR 0.06382978723404255\n",
            "TP,FP,TN,FN\n",
            "59 3 44 90\n",
            "sensitive attribute  2\n",
            "precision 0.8\n",
            "recall 0.36923076923076925\n",
            "FPR 0.15384615384615385\n",
            "TP,FP,TN,FN\n",
            "24 6 33 41\n",
            "sensitive attribute  3\n",
            "precision 0.9605263157894737\n",
            "recall 0.3989071038251366\n",
            "FPR 0.05172413793103448\n",
            "TP,FP,TN,FN\n",
            "73 3 55 110\n",
            "sensitive attribute  4\n",
            "precision 0.625\n",
            "recall 0.3225806451612903\n",
            "FPR 0.21428571428571427\n",
            "TP,FP,TN,FN\n",
            "10 6 22 21\n",
            "acceptance rates\n",
            "[0.3163265306122449, 0.28846153846153844, 0.3153526970954357, 0.2711864406779661]\n",
            "DP\n",
            "0.045140089934278815\n",
            "precision 0.9021739130434783\n",
            "recall 0.3878504672897196\n",
            "TP,FP,TN,FN\n",
            "83 9 77 131\n",
            "<--------------------------------------->\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNmoFsh18-87",
        "outputId": "880d77a2-0f9c-4f11-ad72-be7e270bcd0d"
      },
      "source": [
        "pip install pulp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pulp\n",
            "  Downloading PuLP-2.6.0-py3-none-any.whl (14.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.2 MB 4.6 MB/s \n",
            "\u001b[?25hInstalling collected packages: pulp\n",
            "Successfully installed pulp-2.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUAbOcIufe0d"
      },
      "source": [
        "# bilal - am_ind asian black other white female male(actual precision sequence)\n",
        "# 6         5          4          2                1              0                  3\n",
        "\n",
        "# ours -s_male, s_female  r_white, r_black, r_asian-pac-islander','r_amer-indian-eskimo','r_other\n",
        "           \n",
        "# beta=[6         5          4          2                1              0                  3]\n",
        "\n",
        "# beta=[beta[6], beta[5],beta[4],beta[2],beta[1],beta[0],beta[3]]\n",
        "\n",
        "\n",
        "#bilal -female male  am_ind asian  black other white (actual acceptance rate sequence)\n",
        "#          0      1     2       3     4      5    6   \n",
        "# ours -s_male, s_female  r_white, r_black, r_asian-pac-islander','r_amer-indian-eskimo', 'r_other\n",
        "           \n",
        "# beta=[1       0        6          4            3                   2                    5]\n",
        "\n",
        "\n",
        "#NG\n",
        "import time\n",
        "#import gurobipy as gp\n",
        "#from gurobipy import GRB\n",
        "import pulp as p \n",
        "def lp_equalized_odds(data1,eps,y_test_pred,e,beta_avg,alpha):\n",
        "    import pulp as p \n",
        "    import math\n",
        "    \n",
        "    \n",
        "    m=data1.shape[0]\n",
        "    n=data1.shape[1]\n",
        "    print('dimension of data')\n",
        "    print(m,n)\n",
        "    \n",
        "    \n",
        "    ############### #  SORTED for ACCURACY ONLY ####\n",
        "    m=2\n",
        "    h1=[]\n",
        "    key1=[]\n",
        "    cost=np.zeros(n,dtype=int)\n",
        "    data2=np.zeros((m,n),dtype=int)\n",
        "    \n",
        "    for i in range(n):\n",
        "            h1.append(e[i][1])\n",
        "            key1.append(i)\n",
        "\n",
        "        \n",
        "#print(hc)\n",
        "#     print(key1)\n",
        "    \n",
        "    for i in range(1,len(h1)):\n",
        "        for j in range(i,0,-1):\n",
        "            var=0\n",
        "            var2=0\n",
        "            if h1[j-1]<h1[j]:\n",
        "                index=j\n",
        "                var=h1[j]\n",
        "                h1[j]=h1[j-1]\n",
        "                h1[j-1]=var\n",
        "\n",
        "                var2=key1[j]\n",
        "                key1[j]=key1[j-1]\n",
        "                key1[j-1]=var2\n",
        "            else:\n",
        "                break\n",
        "    \n",
        "\n",
        "    \n",
        "    \n",
        "    for j in range(len(key1)):    \n",
        "         data2[0][key1[j]]=j+1\n",
        "    \n",
        "    for j in range(n):\n",
        "        summ=0\n",
        "        summ=summ+data2[0][j] \n",
        "        cost[j]=summ\n",
        "\n",
        "    Lp_prob = p.LpProblem('Problem', p.LpMinimize)  \n",
        "    solver = p.getSolver('PULP_CBC_CMD', timeLimit=20)\n",
        "   \n",
        "    \n",
        "#     X=np.zeros(n+1,dtype=p.LpVariable)\n",
        "    X=np.zeros(n+m+1,dtype=p.LpVariable)\n",
        "    Y=np.zeros(m,dtype=p.LpVariable)\n",
        "    \n",
        "    sizes=np.zeros(m,dtype=int)\n",
        "#     report_index(index,data1,e):  \n",
        "    max_size=0\n",
        "    for i in range(m):\n",
        "        count=0\n",
        "        for j in range(n):\n",
        "            if data1[i][j]==1:\n",
        "                count=count+1 \n",
        "        if count>max_size:\n",
        "            max_size=count\n",
        "        sizes[i]=count\n",
        "    print(sizes)    \n",
        "    #############################33\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    ###############################\n",
        "    beta_actual = [0.7621359223300971, 0.6382978723404256]\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "    select_sizes=np.zeros(m,dtype=int)\n",
        "   \n",
        "    size_final=np.zeros(m,dtype=int)\n",
        "\n",
        "    for i in range(m):\n",
        "        var1 = str(n+100+i)\n",
        "        Y[i]=p.LpVariable(var1,lowBound=0,upBound=1,cat='Continuous')\n",
        "    \n",
        "    for i in range(n):\n",
        "        var1=str(i)       \n",
        "        X[i]=p.LpVariable(var1,lowBound=0,upBound=1,cat='Integer')\n",
        "   \n",
        "    X[n]=p.LpVariable(str(n),lowBound=0,upBound=1,cat='Continuous')  \n",
        "\n",
        "    tpr = p.LpVariable(str(n+200),lowBound=0,upBound=1,cat='Continuous')  \n",
        "    fpr = p.LpVariable(str(n+201),lowBound=0,upBound=1,cat='Continuous')  \n",
        "\n",
        "#     for i in range(m):\n",
        "#         k=n+i+1\n",
        "#         var1=str(k)     \n",
        "#         alpha=(((sizes[i])*(sizes[i]+1))/2)\n",
        "#         X[i]=p.LpVariable(var1,lowBound=(((beta*sizes[i])*(beta*sizes[i]+1))/2),upBound=alpha,cat='Continuous')\n",
        "    \n",
        "        \n",
        "#     X[n]=  p.LpVariable(\"z1\",lowBound=0)\n",
        "    #X[n+1]=  p.LpVariable(\"z2\",lowBound=0)\n",
        "  \n",
        "\n",
        "    #########objective function#####################\n",
        "    \n",
        "#     Lp_prob += 2*X[n+1]+10*X[n+2]+9*X[n+3]+3*X[n+4]\n",
        "    #alpha=0.8\n",
        "    #beta_avg = 0.10\n",
        "    Lp_prob+= p.lpSum([(X[j])*cost[j] for j in range(n)]) \n",
        "    #Lp_prob+=1  \n",
        "    \n",
        "    #Lp_prob += Y[0]*sizes[0] + Y[1]*sizes[1] >= p.lpSum([Y[j]*sizes[j] for j in np.arange(2,6)])\n",
        "    #Lp_prob += Y[0]*sizes[0] + Y[1]*sizes[1] <= p.lpSum([Y[j]*sizes[j] for j in np.arange(2,6)])\n",
        "    \n",
        "    ##############constraint#################\n",
        "    #first select the  the number of make female in test data\n",
        "    #then apply the equalized odd constraints assuming \n",
        "    #look at all males which have been predicted positve/and all the females predicted negative\n",
        "    F_test = 0\n",
        "    M_test = 0\n",
        "        \n",
        "    #for i in range(len(y_test)):\n",
        "    #    if(data1[0][i]==1 and y_test.iloc[i]==1):\n",
        "    #        M_test= M_test+1\n",
        "    #    elif(data1[1][i]==1 and y_test.iloc[i]==1):\n",
        "    #        F_test= F_test+1\n",
        "    test_count = np.zeros(m, dtype = int)\n",
        "\n",
        "    for i in range(len(y_test)):\n",
        "      for j in range(m): \n",
        "        if(data1[j][i]==1 and y_test_pred[i]==1):\n",
        "            test_count[j] = test_count[j] +1\n",
        "                \n",
        "    \n",
        "    #Lp_prob += (p.lpSum([(X[j])*(data1[0][j])*y_test_pred[j] for j in range(n) if y_test_pred[j]==1])/M_test) <= (p.lpSum([(X[j])*(data1[1][j])*y_test_pred[j] for j in range(n) if y_test_pred[j]==1])/F_test) + 0.0009\n",
        "    #Lp_prob += (p.lpSum([(X[j])*(data1[0][j])*(1-y_test_pred[j]) for j in range(n) if y_test_pred[j]==0])/(sizes[0]-M_test)) <= (p.lpSum([(X[j])*(data1[1][j])*(1-y_test_pred[j]) for j in range(n) if y_test_pred[j]==0])/(sizes[1]-F_test))+ 0.0009\n",
        "    for i in range(m):   #TPR constraints\n",
        "      Lp_prob += (1/test_count[i])*p.lpSum([(X[j])*(data1[i][j])*y_test_pred[j] for j in range(n) if (y_test_pred[j]==1) ]) >= tpr \n",
        "      Lp_prob += (1/test_count[i])*p.lpSum([(X[j])*(data1[i][j])*y_test_pred[j] for j in range(n) if (y_test_pred[j]==1 )]) <= tpr + 0.1\n",
        "    for i in range(m):    #FPR constraints\n",
        "      Lp_prob += (1/(sizes[i]-test_count[i]))*p.lpSum([(X[j])*(data1[i][j])*(1-y_test_pred[j]) for j in range(n) if (y_test_pred[j]==0)]) >= fpr\n",
        "      Lp_prob += (1/(sizes[i]-test_count[i]))*p.lpSum([(X[j])*(data1[i][j])*(1-y_test_pred[j]) for j in range(n) if (y_test_pred[j]==0)]) <= fpr + 0.1\n",
        "\n",
        "    \n",
        "    for i in range(m):\n",
        "      #  if i<m:\n",
        "            Lp_prob += p.lpSum([(X[j])*(data1[i][j]) for j in range(n)]) >= Y[i]*sizes[i]\n",
        "            Lp_prob += p.lpSum([(X[j])*(data1[i][j]) for j in range(n)]) <= (Y[i]+0.1)*sizes[i]\n",
        "    \n",
        "    for i in range(m):\n",
        "        if beta_actual[i] >= beta_avg:\n",
        "            Lp_prob += Y[i] >= (1-alpha)*beta_actual[i] + alpha*beta_avg\n",
        "            Lp_prob += Y[i] <= beta_actual[i]\n",
        "        else:\n",
        "            Lp_prob += Y[i] >= (1-alpha)*beta_actual[i] + alpha*beta_avg\n",
        "            Lp_prob += Y[i] <= beta_avg \n",
        "    \n",
        "           \n",
        "    #Lp_prob+= p.lpSum([(X[j])*cost[j] for j in range(n)])>=100\n",
        "        \n",
        "    #####################################\n",
        "    #solver = p.CPLEX_PY()\n",
        "    #solver.buildSolverModel(Lp_prob)\n",
        "    #Lp_prob.solverModel.parameters.timelimit.set(60)\n",
        "    #solver.callSolver(P)\n",
        "    #status = solver.findSolutionValues(Lp_prob)\n",
        "    #################################################################\n",
        "    status = Lp_prob.solve(solver)   # Solver \n",
        "    print(p.LpStatus[status]) \n",
        "    print(\"objective is:\")        \n",
        "    print(p.value(Lp_prob.objective))\n",
        "    print(\"discripency is:\") \n",
        "    print(p.value(X[n]))\n",
        "    x=np.zeros(n,dtype=float)\n",
        "\n",
        "   # The solution status \n",
        "    Synth1={}\n",
        "    Synth2={}\n",
        "    # # Printing the final solution \n",
        "    for i in range(n):\n",
        "        if(p.value(X[i])==1):\n",
        "            Synth1[i]=1 \n",
        "            Synth2[i]=-1\n",
        "#             if(data1[2][i]==1):\n",
        "#                 print(\"no\")\n",
        "        else:\n",
        "            Synth1[i]=-1\n",
        "            Synth2[i]=1\n",
        "    Synthu1=Synth1  \n",
        "    Synthu2=Synth2  \n",
        "    \n",
        "              \n",
        "    return Synthu1,Synthu2   \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7OUBvqe8ZAm-",
        "outputId": "03f86785-f848-4336-b4f2-84c4feaafb52"
      },
      "source": [
        "pip install pulp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pulp\n",
            "  Downloading PuLP-2.5.1-py3-none-any.whl (41.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 41.2 MB 75 kB/s \n",
            "\u001b[?25hInstalling collected packages: pulp\n",
            "Successfully installed pulp-2.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1Twwdewfe0y",
        "outputId": "c4a137f9-0dbd-4ed2-93b7-eb8d54146c99"
      },
      "source": [
        "pip install pulp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pulp\n",
            "  Downloading PuLP-2.5.1-py3-none-any.whl (41.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 41.2 MB 76 kB/s \n",
            "\u001b[?25hInstalling collected packages: pulp\n",
            "Successfully installed pulp-2.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1ryCyPhfe1L"
      },
      "source": [
        "pip install pulp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spvW20mEfe2L"
      },
      "source": [
        "#without accuracy ---> 2\n",
        "def main2(datax, y_test, y_test_pred,e): \n",
        "        \n",
        "    n=datax.shape[1]\n",
        "    s=datax.shape[0]    \n",
        "    data = np.zeros((s, n), dtype = int)\n",
        "    \n",
        "    r = np.zeros(n, dtype = int) \n",
        "    \n",
        "    for i in range(n):\n",
        "        if int(y_test.iloc[i])==1 :\n",
        "            r[i]=1\n",
        "        else :\n",
        "            r[i]= -1  \n",
        "    \n",
        "    r2 = np.zeros(n, dtype = int) \n",
        "    for i in range(n):\n",
        "        if int(y_test_pred[i])==1 :\n",
        "            r2[i]=1\n",
        "        else :\n",
        "            r2[i]= -1          \n",
        "    ar=[]\n",
        "    \n",
        "    for j in range(s):\n",
        "        print(\"sensitive attribute \",(j+1)) \n",
        "        a=0\n",
        "        b=0\n",
        "        acc1=0\n",
        "        acc2=0\n",
        "        for i in range(n):\n",
        "                data[j][i]= datax.iloc[j,i]\n",
        "                if data[j][i]== 1 :\n",
        "                    a=a+1\n",
        "                    if r[i]==1:\n",
        "                         acc1=acc1+1 \n",
        "\n",
        "        print(\"ACTUAL----------total ,accepted, aceeptance rate:\")             \n",
        "        a1=float(acc1/a)\n",
        "        print(a)\n",
        "        \n",
        "        print(acc1)\n",
        "        print(a1)\n",
        "        ar.append(a1)\n",
        "        \n",
        "    maxi= max(ar)\n",
        "    mini= min(ar)\n",
        "    DP=float(maxi-mini)\n",
        "    print(\"data acceptance rates\")\n",
        "    print(ar)\n",
        "    print(\"data DP\")\n",
        "    print(DP)\n",
        "    \n",
        "    ar=[]\n",
        "    \n",
        "    for j in range(s):\n",
        "        print(\"sensitive attribute \",(j+1)) \n",
        "        a=0\n",
        "        b=0\n",
        "        acc1=0\n",
        "        acc2=0\n",
        "        prec=0\n",
        "        reca=0\n",
        "        accur=0\n",
        "        FP=0\n",
        "        FN=0\n",
        "        TP=0\n",
        "        TN=0\n",
        "        for i in range(n):\n",
        "             if data[j][i]== 1 :\n",
        "                    a=a+1\n",
        "                    if r2[i]==1:\n",
        "                        acc1=acc1+1 \n",
        "                        if r[i]==1:\n",
        "                            TP=TP+1\n",
        "                        else:\n",
        "                             FP=FP+1                \n",
        "                    else:\n",
        "                        if r[i]==1:\n",
        "                            FN=FN+1\n",
        "                        else:\n",
        "                            TN=TN+1    \n",
        "        \n",
        "        print(\"prec reca accuracy for each sens\") \n",
        "        prec= float(TP/(TP+FP))\n",
        "        reca= float(TP/(TP+FN))\n",
        "        accur= float((TP+TN)/a)\n",
        "        print(prec,reca,accur)\n",
        "        \n",
        "        print(\"RanomForest----------total , accepted, aceeptance rate:\")             \n",
        "        \n",
        "        a1=float(acc1/a)\n",
        "        print(a)\n",
        "        \n",
        "        print(acc1)\n",
        "        print(a1)\n",
        "        ar.append(a1)\n",
        "        \n",
        "    maxi= max(ar)\n",
        "    mini= min(ar)\n",
        "    DP=float(maxi-mini)\n",
        "    print(\"data acceptance rates\")\n",
        "    print(ar)\n",
        "    print(\"data DP\")\n",
        "    print(DP) \n",
        "    \n",
        "    print(\"SVM accuracy--------------------------\")\n",
        "    prec=0\n",
        "    reca=0\n",
        "    accur=0\n",
        "    FP=0\n",
        "    FN=0\n",
        "    TP=0\n",
        "    TN=0\n",
        "    for i in range(n):\n",
        "            if r2[i]==1:\n",
        "                acc1=acc1+1 \n",
        "                if r[i]==1:\n",
        "                    TP=TP+1\n",
        "                else:\n",
        "                     FP=FP+1                \n",
        "            else:\n",
        "                if r[i]==1:\n",
        "                     FN=FN+1\n",
        "                else:\n",
        "                     TN=TN+1    \n",
        "\n",
        "        \n",
        "    prec= float(TP/(TP+FP))\n",
        "    reca= float(TP/(TP+FN))\n",
        "    accur= float((TP+TN)/n)\n",
        "    print(prec,reca,accur)\n",
        "    \n",
        "    \n",
        "#     delta1=[.70,.75,.80,.85,.90,.95]\n",
        "    #gamma=.05,.06,.07\n",
        "    #delta1=[.80,.85,.90,.95]\n",
        "# (for reproducibility)  \n",
        "\n",
        "# delta1=[.8], gama=[.1], epsilon=[.05]  \n",
        "# delta1=[.8], gama=[.15], epsilon=[.01]\n",
        " \n",
        "#     delta1=np.arange(1,.79,-.01)\n",
        "    delta=1\n",
        "#     gama=[.05,.1,.15,.2,.25]\n",
        "#     epsilon=[.01,.02,.05,.1,.15,.20,.25,.30,.35,.40,.50]\n",
        "\n",
        "#ADULT ZAFAR =? epsilon=[0.088 ,0.1656, 0.168,  0.211, 0.251 ] \n",
        " \n",
        "#agarwal=> epsilon=[ 0.071, 0.1271, 0.2437, 0.27 ]\n",
        " \n",
        "\n",
        "    #gama=[0.0869, 0.0521,0.0782, 0.0608,0.0434, 0.1,0.069,0.0434,0.034]\n",
        "    epsilon=[.01]\n",
        "    beta_converge = [0.15,0.20]\n",
        "    alpha = [0,0.06,0.062,0.065,0.07,0.8]\n",
        "    \n",
        "    zero_one = np.zeros(n, dtype = int) \n",
        "    \n",
        "    fi= np.zeros(n,dtype=int) \n",
        "#     for delta in delta1:\n",
        "    #4 gamma=[0.175442,    0.142103, 0.166039,    0.164754,  0.153465,    0.14,  0.104348   ]\n",
        "    #lp_equalized_odds_no_beta(data1,eps,y_test,e,beta_avg,alpha)\n",
        "    #1 gamma=[0.259147,   0.0730028, 0.210139, 0.0893443, 0.306931, 0.0933333,  0.0347826]\n",
        "    #gamma=[0.196178,0.126722,   0.179654, 0.140164,     0.153465,   0.133333,  0.0695652]\n",
        "\n",
        "  \n",
        "    gamma = [0.175442,    0.142103, 0.166039,    0.164754,  0.153465,    0.14,  0.104348 ]\n",
        "    for eps in epsilon:\n",
        "        for beta_avg in beta_converge:\n",
        "            print(\"----------------This is for covergence at beta = \",beta_avg, \" ----------------\")\n",
        "            for a in alpha:\n",
        "                u1,u2=lp_equalized_odds(data,eps,y_test_pred,e,beta_avg,a)\n",
        "                #######################Disp_impact#######################  \n",
        "                print(\"gamma-epsilon-delta\",gamma,eps,delta)\n",
        "                accu_all=[]\n",
        "                DP_all=[]\n",
        "                precision_all=[]\n",
        "                recall_all=[]\n",
        "                acceptance_rate=np.zeros((7,28),dtype=float)\n",
        "                count=0\n",
        "                print(\"<--------------------------------------->\")\n",
        "            #        print(\"iteration t\",t)\n",
        "            #                 for alpha in np.arange(0,1.05,0.05):\n",
        "            #                     print(\"alpha: \",alpha)\n",
        "            #                     for i in range(n):\n",
        "\n",
        "            #                         z=random()\n",
        "            #                         if z < alpha:\n",
        "            #                                fi[i]= u1[i] \n",
        "\n",
        "            #                         else:\n",
        "            #                                fi[i]= r2[i]\n",
        "                \n",
        "                for i in range(n):\n",
        "                    fi[i] = u1[i]\n",
        "                    if (fi[i]==1):\n",
        "                        zero_one[i] = 1\n",
        "                    else:\n",
        "                        zero_one[i] = 0\n",
        "                ar=[]\n",
        "                #find_eo_stats(y_test,zero_one)\n",
        "                find_eo_stats_multiple(y_test,zero_one)\n",
        "\n",
        "\n",
        "                for j in range(s):\n",
        "                    print(\"sensitive attribute \",(j+1)) \n",
        "\n",
        "                    TP=0\n",
        "                    FP=0\n",
        "                    FN=0\n",
        "                    TN=0\n",
        "                    precision=0\n",
        "                    recall=0\n",
        "                    for i in range(n):\n",
        "                         if data[j][i]== 1 :                        \n",
        "                            if fi[i]==1 and r[i]==1:\n",
        "                                TP=TP+1\n",
        "                            if fi[i]==1 and r[i]==-1:\n",
        "                                FP=FP+1 \n",
        "                            if fi[i]==-1 and r[i]==1:\n",
        "                                FN=FN+1\n",
        "                            if fi[i]==-1 and r[i]==-1:\n",
        "                                TN=TN+1    \n",
        "                    if TP+FP !=0:\n",
        "                        precision=float(TP/(TP+FP))\n",
        "                    print(\"precision\",precision)\n",
        "                    if TP+FN !=0:    \n",
        "                        recall=float(TP/(TP+FN))\n",
        "                    print(\"recall\",recall)\n",
        "                    if FP+TN !=0:\n",
        "                        fpr = float(FP/(FP+TN))\n",
        "                    print(\"FPR\", fpr)    \n",
        "                    print(\"TP,FP,TN,FN\")\n",
        "                    print(TP,FP,TN,FN)\n",
        "                    a=0\n",
        "                    b=0\n",
        "                    acc1=0\n",
        "                    acc2=0\n",
        "                    for i in range(n):\n",
        "                            if data[j][i]== 1 :\n",
        "                                a=a+1\n",
        "                                if fi[i]==1:\n",
        "                                     acc1=acc1+1 \n",
        "\n",
        "            #                         print(\"total ,fair accepted, aceeptance rate:\")             \n",
        "                    a1=float(acc1/a)\n",
        "\n",
        "\n",
        "\n",
        "            #                         print(a)\n",
        "            #                         print(acc1)\n",
        "            #                         print(a1)\n",
        "                    ar.append(a1)\n",
        "\n",
        "                count = count+1\n",
        "                maxi=max(ar)\n",
        "                mini= min(ar)\n",
        "                DP=float(maxi-mini)\n",
        "                print(\"acceptance rates\")\n",
        "                print(ar)\n",
        "                print(\"DP\")\n",
        "                print(DP)\n",
        "                f_acc=0\n",
        "                for i in range(n):\n",
        "                     if fi[i] == r[i]:\n",
        "                            f_acc=f_acc+1\n",
        "                f_acc_l=float((f_acc*100)/n) \n",
        "\n",
        "#######################################################################33   \n",
        "\n",
        "#                         print(\"sensitive attribute \",(j+1)) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        TP=0\n",
        "        FP=0\n",
        "        FN=0\n",
        "        TN=0\n",
        "        precision=0\n",
        "        recall=0\n",
        "        for i in range(n):\n",
        "                if fi[i]==1 and r[i]==1:\n",
        "                    TP=TP+1\n",
        "                if fi[i]==1 and r[i]==-1:\n",
        "                    FP=FP+1 \n",
        "                if fi[i]==-1 and r[i]==1:\n",
        "                    FN=FN+1\n",
        "                if fi[i]==-1 and r[i]==-1:\n",
        "                    TN=TN+1    \n",
        "\n",
        "        if TP+FP!=0:\n",
        "            precision=float(TP/(TP+FP))\n",
        "        print(\"precision\",precision)\n",
        "        if TP+FN!=0:\n",
        "            recall=float(TP/(TP+FN))    \n",
        "\n",
        "        print(\"recall\",recall)\n",
        "        \n",
        "        accu = float((TP + TN)/n)\n",
        "        print(\"TP,FP,TN,FN\")\n",
        "        print(TP,FP,TN,FN)\n",
        "#       print(\"total ,fair accepted, aceeptance rate:\")             \n",
        "        a1=float(acc1/a)\n",
        "\n",
        "\n",
        "    print(\"<--------------------------------------->\")\n",
        "    alpha_weight=np.arange(0,1.05,.05)        \n",
        "    return accu_all,DP_all,acceptance_rate,alpha_weight"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}